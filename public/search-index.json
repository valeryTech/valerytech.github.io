[{"content":"","date":"2023-09-07","id":0,"permalink":"/blog/","summary":"","tags":[],"title":"Blog"},{"content":"Facebook system design interview questions Design an online collaborative editing tool Design a distributed botnet How would you use a load balancer for memcache servers? How would you architect the Facebook newsfeed? Implement a typeahead feature Google system design interview questions Design and implement statistics for a calendar Design a task scheduling feature Design a ticketing platform Design an elevator Design a boggle server Design a distributed ID generation system How would you deploy a solution for cloud computing to build in redundancy for the compute cluster? Design the server infrastructure for GMail Amazon system design interview questions\nHow would you design an electronic voting system? Design a warehouse system for Amazon Design an online poker game Design a system to interview candidates Design a search engine autocomplete Design an airport How would you design a system that reads book reviews from other sources and displays them on your online bookstore? Design a promotion mechanism which could give 10% cash back on a particular credit card How would you build software behind an amazon pick up location with lockers? Microsoft system design interview questions\nHow does buffer overflow work? How would you design an online portal to sell products? Design a new fitness wearable to measure heart rate Design a shopping cart Topics from grokking\n","date":"2025-02-22","id":1,"permalink":"/system-design/projects/_themes/","summary":"\u003ch1 id=\"facebook-system-design-interview-questions\"\u003e\u003cstrong\u003eFacebook system design interview questions\u003c/strong\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eDesign an online collaborative editing tool\u003c/li\u003e\n\u003cli\u003eDesign a distributed botnet\u003c/li\u003e\n\u003cli\u003eHow would you use a load balancer for memcache servers?\u003c/li\u003e\n\u003cli\u003eHow would you architect the Facebook newsfeed?\u003c/li\u003e\n\u003cli\u003eImplement a typeahead feature\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"google-system-design-interview-questions\"\u003e\u003cstrong\u003eGoogle system design interview questions\u003c/strong\u003e\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eDesign and implement statistics for a calendar\u003c/li\u003e\n\u003cli\u003eDesign a task scheduling feature\u003c/li\u003e\n\u003cli\u003eDesign a ticketing platform\u003c/li\u003e\n\u003cli\u003eDesign an elevator\u003c/li\u003e\n\u003cli\u003eDesign a boggle server\u003c/li\u003e\n\u003cli\u003eDesign a distributed ID generation system\u003c/li\u003e\n\u003cli\u003eHow would you deploy a solution for cloud computing to build in redundancy for the compute cluster?\u003c/li\u003e\n\u003cli\u003eDesign the server infrastructure for GMail\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eAmazon\u003c/strong\u003e \u003cstrong\u003esystem design interview questions\u003c/strong\u003e\u003c/p\u003e","tags":[],"title":" Themes"},{"content":"Actions find out info about:\nAPI Design principles, API evolution, its usage as interface of a system\nwhat is API (fringe, interface, algebra etc) #sd #todo\nprotocol vs API vs SLA vs Algebra ?? #engineering maybe add terms from hexagonal architecture style\nAPI Design https://slack.engineering/how-we-design-our-apis-at-slack/ ~ you can add to this chapter information about communitcations from SO Hard Parts\nhttps://www.martinfowler.com/tags/API%20design.html\n","date":"2025-02-22","id":2,"permalink":"/system-design/topics/api/","summary":"\u003ch1 id=\"actions\"\u003eActions\u003c/h1\u003e\n\u003cp\u003efind out info about:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eAPI Design principles, API evolution, its usage as interface of a system\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ewhat is API (fringe, interface, algebra etc) #sd #todo\u003c/p\u003e","tags":[],"title":"Api"},{"content":"important things: define core responsiblities and how to introduce it to design maybe add problem based lens\nmicrosoft: In a microservices architecture, a client might interact with more than one front-end service. Given this fact, how does a client know what endpoints to call? What happens when new services are introduced, or existing services are refactored? How do services handle SSL termination, authentication, and other concerns? An API gateway can help to address these challenges.\nCore Responsibilities Funny enough, I\u0026rsquo;ll often have candidates introduce a gateway in a system design interview and emphasize that it will do all this middleware stuff but never mention the core reason they need it \u0026ndash; request routing.\nUsage The TLDR is: use it when you have a microservices architecture and don\u0026rsquo;t use it when you have a simple client-server architecture.\nWith a microservices architecture, an API Gateway becomes almost essential. Without one, clients would need to know about and communicate with multiple services directly, leading to tighter coupling and more complex client code. The gateway provides a clean separation between your internal service architecture and your external API surface.\nHowever, it\u0026rsquo;s equally important to recognize when an API Gateway might be overkill. For simple monolithic applications or systems with a single client type, introducing an API Gateway adds unnecessary complexity.\nI\u0026rsquo;ve mentioned this throughout, but I want it to be super clear. While it\u0026rsquo;s important to understand every component you introduce into your design, the API Gateway is not the most interesting. There is a far greater chance that you are making a mistake by spending too much time on it than not enough.\nGet it down, say it will handle routing and middleware, and move on.\nSources (good one) https://www.hellointerview.com/learn/system-design/deep-dives/api-gateway\nother to sort:\nhttps://www.youtube.com/watch?v=1vjOv_f9L8I https://learn.microsoft.com/en-us/azure/architecture/microservices/design/gateway https://microservices.io/patterns/apigateway.html https://aws.amazon.com/api-gateway/resources/\nCommon API Gateway Solutions: AWS API Gateway: A fully managed service from Amazon Web Services for creating, deploying, and managing APIs at any scale. Kong: An open-source, scalable API Gateway that is often used in microservices environments. NGINX: Frequently used as a reverse proxy and API Gateway, providing features like load balancing, request routing, and rate limiting. Apigee: A Google Cloud API management platform that offers full lifecycle API management with security, monitoring, and versioning. Traefik: A modern HTTP reverse proxy and load balancer for microservices, which acts as an API Gateway with built-in service discovery, SSL, and other functionalities. ","date":"2025-02-22","id":3,"permalink":"/system-design/elements/api_gateway/","summary":"\u003cp\u003eimportant things: define core responsiblities and how to introduce it to design\nmaybe add problem based lens\u003c/p\u003e\n\u003cp\u003emicrosoft:\nIn a microservices architecture, a client might interact with more than one front-end service. Given this fact, how does a client know what endpoints to call? What happens when new services are introduced, or existing services are refactored? How do services handle SSL termination, authentication, and other concerns? An \u003cem\u003eAPI gateway\u003c/em\u003e can help to address these challenges.\u003c/p\u003e","tags":[],"title":"Api Gateway"},{"content":"","date":"2025-02-22","id":4,"permalink":"/engineering/architecture/","summary":"","tags":[],"title":"Architecture"},{"content":"What Software Architects Do That Programmers DON\u0026rsquo;T https://www.youtube.com/watch?v=IwrvE-wHm84\ndownload book \u0026ldquo;# Fundamentals of Software Architecture: A Modern Engineering Approach\u0026rdquo; Fundamentals of Software Architecture, 2nd Edition by Mark Richards, Neal Ford; Released March 2025 https://www.reddit.com/r/softwarearchitecture/comments/16usw23/megathread_software_architecture_books_resources/\nhttps://fundamentalsofsoftwarearchitecture.com/katas/list.html\nCommon dev practices https://www.youtube.com/@HealthyDev/videos\nhttps://www.youtube.com/watch?v=2ClljZaK6_A\nhttps://handbook.gitlab.com/handbook/engineering/architecture/design-documents/\n","date":"2025-02-22","id":5,"permalink":"/engineering/architecture/architecture/","summary":"\u003ch1 id=\"what-software-architects-do-that-programmers-dont\"\u003eWhat Software Architects Do That Programmers DON\u0026rsquo;T\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=IwrvE-wHm84\"\u003ehttps://www.youtube.com/watch?v=IwrvE-wHm84\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e download book \u0026ldquo;# Fundamentals of Software Architecture: A Modern Engineering Approach\u0026rdquo; Fundamentals of Software Architecture, 2nd Edition by Mark Richards, Neal Ford;  Released March 2025\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://www.reddit.com/r/softwarearchitecture/comments/16usw23/megathread_software_architecture_books_resources/\"\u003ehttps://www.reddit.com/r/softwarearchitecture/comments/16usw23/megathread_software_architecture_books_resources/\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"Architecture"},{"content":"","date":"2025-02-22","id":6,"permalink":"/system-design/elements/background/","summary":"","tags":[],"title":"Background"},{"content":"ontology Cache definitions:\nCaching is a commonly used performance optimization (\u0026lt;= really important that it is an optimization) whereby the previous result of some operation is stored so that subsequent requests can use this stored value rather than spending time and resources recalculating the value.\nIn computing, a cache is a component that stores data so that future requests for that data can be served faster; the data stored in a cache might be the result of an earlier computation or a copy of data stored elsewhere.\nStructurally, cache can be viewed as a high-speed storage layer that sits between the application and the original source of the data, such as a database, a file system, or a remote web service.\nAs locigal layer: in computing, a cache is a high-speed data storage layer which stores a subset of data, typically transient in nature, so that future requests for that data are served up faster than is possible by accessing the data\u0026rsquo;s primary storage location.\ncommon In computing, a cache is a component that stores data so that future requests for that data can be served faster; the data stored in a cache might be the result of an earlier computation or a copy of data stored elsewhere.\nCache can be viewed as a high-speed storage layer that sits between the application and the original source of the data, such as a database, a file system, or a remote web service.\nCaches can store the results of simple lookups, as in this example, but really they can store any piece of data, such as the result of a complex calculation. We can cache to help improve the performance of our system as part of helping reduce latency, to scale our application, and in some cases even to improve the robustness of our system.\nCaching as optimization\nTreat caching primarily as a performance optimization. Cache in as few places as possible to make it easier to reason about the freshness of data.\nData change frequency\nGenerally, caching is used only in cases where the external data doesn\u0026rsquo;t change often or you can replicate all the data on your systems. Also if you don\u0026rsquo;t care about data consistency then cache is really useful.\nRule of Thumb\nGenerally, caching is used for read-heavy systems. Popular read-heavy systems are Twitter and YouTube. Are most of their millions of users tweeting and posting videos? No, their most common users are reading tweets and watching videos.\nanother rule\nThe 80/20 rule: You want to store 80% of read requests in 20% of storage (or memory). Generally, these are the most popular requests.\nFor example, if there is a tweet from a very popular person on twitter you would cache it everywhere (on a device, on a CDN, on the application) so that whenever that tweet is finished you can fetch it very quickly and serve it up to your users.\nA CDN (content delivery network) is something that\u0026rsquo;s commonly used to deliver cached data.\nTo maintain coherence of the external data, you should apply a periodic refresh strategy to the cache so it doesn\u0026rsquo;t become outdated.\nUpdating cache: cache-aside, write-through, write-behind, refresh-ahead.\nTargets A cache can reduce load on system which data it holds. A primary reason to set up caching outside of your database is to reduce load within your database engine.\nCases: when you want to speed up a system =\u0026gt; When you douing a lot of network requests =\u0026gt; Very computationaly long computations =\u0026gt; Don\u0026rsquo;t want to send an identical request from multiple clients to DB (for example, star instargam profile)\nFor Performance.\nWith microservices, we are often concerned about the adverse impact of network latency and about the cost of needing to interact with multiple microservices to get some data. Fetching data from a cache can help greatly here, as we avoid the need for network calls to be made, which also has the impact of reducing load on downstream microservices. Aside from avoiding network hops, it reduces the need to create the data on each request.\nFor Scale.\nIf you can divert reads to caches, you can avoid contention on parts of your system to allow it to better scale. An example of this that we\u0026rsquo;ve already covered in this chapter is the use of database read replicas. The read traffic is served by the read replicas, reducing the load on the primary database node and allowing reads to be scaled effectively. The reads on a replica are done against data that might be stale. The read replica will eventually get updated by the replication from primary to replica node—this form of cache invalidation is handled automatically by the database technology. More broadly, caching for scale is useful in any situation in which the origin is a point of contention. Putting caches between clients and the origin can reduce the load on the origin, better allowing it to scale.\nFor Robustness.\nIf you have an entire set of data available to you in a local cache, you have the potential to operate even if the origin is unavailable—this in turn could improve the robustness of your system.\nParameters side of a cache: client/server\nWhere to Cache As we have covered multiple times, microservices give you options. And this is absolutely the case with caching. We have lots of different places where we could cache. The different cache locations I\u0026rsquo;ll outline here have different trade-offs, and what sort of optimization you\u0026rsquo;re trying to make will likely point you toward the cache location that makes the most sense for you.\nClient-side. With client-side caching, the data is cached outside the scope of the origin. In our example, this could be done as simply as holding an in-memory hashtable.\nIn general, client-side caches tend to be pretty effective, as they avoid the network call to the downstream microservice. This makes them suitable not only for caching for improved latency but also for caching for robustness.\nIt\u0026rsquo;s important to note that our client cache could decide to cache only some of the information we get from the microservice.\nClient-side caching has a few downsides, though. Firstly, you tend to be more restricted in your options around invalidation mechanisms. Secondly, when there\u0026rsquo;s a lot of client-side caching going on, you can see a degree of inconsistency between clients. This means that you could see a different view of the cached data in each of those clients at the same time. The more clients you have, the more problematic this is likely to be. Techniques such as notification-based invalidation, which we\u0026rsquo;ll look at shortly, can help reduce this problem, but they won\u0026rsquo;t eliminate it.\nAnother mitigation for this is to have a shared client-side cache, perhaps making use of a dedicated caching tool like Redis or memcached, as we see in Figure 13-11. Here, we avoid the problem of inconsistency between the different clients. This can also be more efficient in terms of resource use, as we are reducing the number of copies of this data we need to manage (caches often end up being in memory, and memory is often one of the biggest infrastructural constraints). The flip side is that our clients now need to make a round trip to the shared cache.\nAnother thing to consider here is who is responsible for this shared cache. Depending on who owns it and how it is implemented, a shared cache like this can blur the lines between client-side caching and server-side caching, which we explore next.\nServer-side. A server itself maintains a cache on behalf of his consumers. Here, the Catalog microservice has full responsibility for managing the cache. Due to the nature of how these caches are typically implemented—such as an in-memory data structure, or a local dedicated caching node—it\u0026rsquo;s easier to implement more sophisticated cache invalidation mechanisms. Write-through caches, for example (which we\u0026rsquo;ll look at shortly), would be much simpler to implement in this situation. Having a server-side cache also makes it easier to avoid the issue with different consumers seeing different cached values that can occur with client-side caching.\nThe major issue with this form of caching is that it has reduced scope for optimizing for latency, as a round trip by consumers to the microservice is still needed. This also reduces the effectiveness of this form of caching for any form of robustness.\nThis might make this form of caching seem less useful, but there is huge value to transparently improving performance for all consumers of a microservice just by making a decision to implement caching internally. A microservice that is widely used across an organization may benefit hugely by implementing some form of internal caching, helping perhaps to improve response times for a number of consumers while also allowing the microservice to scale more effectively.\nRequest cache With a request cache, we store a cached answer for the original request. So in Figure 13-13 for example, we store the actual top ten entries. Subsequent requests for the top ten best sellers result in the cached result being returned. No lookups in the Sales data needed, no round trips to Catalog—this is far and away the most effective cache in terms of optimizing for speed. The benefits here are obvious. This is super efficient, for one thing. However, we need to recognize that this form of caching is highly specific.\ncache invalidation Data eviction rules (policies). We need to know when to delete stale data or redundant data. Policies: LRU, LFU, combined, etc.\nInvalidation is the process by which we evict data from our cache. It\u0026rsquo;s an idea that is simple in concept but complex in execution, if for no other reason than there are a wealth of options in terms of how to implement it, and numerous trade-offs to consider in terms of making use of data that might be out of date. Fundamentally, though, it comes down to deciding in which situations a piece of cached data should be removed from your cache.Sometimes this happens because we are told a new version of a piece of data is available; at other times it might require us to assume our cached copy is stale and fetch a new copy from the origin.\nTTL This is one of the simplest mechanisms to use for cache invalidation. Each entry in the cache is assumed to be valid for only a certain duration in time. After that time has passed, the data is invalidated, and we fetch a new copy. But the simplicity of implementation needs to be balanced against how much tolerance you have around operating on out-of-date data. Even if you\u0026rsquo;re not using HTTP, the idea of the origin giving hints to the client as to how (and if) data should be cached is a really powerful concept. This means you don\u0026rsquo;t have to guess about these things on the client side; you can actually make an informed choice about how to handle a piece of data.\nNotification-based With notification-based invalidation, we use events to help subscribers know if their local cache entries need to be invalidated. To my mind, this is the most elegant mechanism for invalidation, though that is balanced by its relative complexity with respect to TTL-based invalidation. The main benefit of this mechanism is that it reduces the potential window wherein the cache is serving stale data. The window in which a cache might now be serving stale data is limited to the time taken for the notification to be sent and processed.\nConditional GETS\nLRU\nFor most systems 20% of the data accounts for 80% of the reads. So using LRU will result in fewer cache misses. Because of the 80/20 rule, we want to give special treatment to the most popular data! That\u0026rsquo;s why we use LRU. As a result, we can throw stuff in the cache (and not miss), which reduces latency for 80% of your requests.\nWriting to cache Write-through With a write-through cache, the cache is updated at the same time as the state in the origin. \u0026ldquo;At the same time\u0026rdquo; is where write-through caches get tricky, of course. Implementing a write-through mechanism on a server-side cache is somewhat straightforward, as you could update a database and an in-memory cache within the same transaction without too much difficulty. If the cache is elsewhere, it\u0026rsquo;s more difficult to reason about what \u0026ldquo;at the same time\u0026rdquo; means in terms of these entries being updated.\nDue to this difficulty, you\u0026rsquo;d typically see write-through caching being used in a microservice architecture on the server side. The benefits are pretty clear—the win‐ dow in which a client might see stale data could be practically eliminated. This is bal‐ anced against the fact that server-side caches may well be less generally useful, limiting the circumstances in which a write-through cache would be effective in microservices.\nWrite-behind (write-back) With a write-behind cache, the cache itself is updated first, and then the origin is updated. Conceptually, you can think of the cache as a buffer. Writing into the cache is faster than updating the origin. So we write the result into the cache, allowing faster subsequent reads, and trust that the origin will be updated afterward. The main concern around write-behind caches is going to be the potential for data loss. If the cache itself isn\u0026rsquo;t durable, we could lose the data before the data is written to the origin. Additionally, we\u0026rsquo;re now in an interesting spot—what is the origin in this context? We\u0026rsquo;d expect the origin to be the microservice where this data is sourced from—but if we update the cache first, is that really the origin?\nFreshness Versus Optimization Coming back to our example of TTL-based invalidation, I explained earlier that if we request a fresh copy of the data that has a five-minute TTL, and a second later the data at the origin changes, then our cache will be operating on out-of-date data for the remaining four minutes and 59 seconds. If this is unacceptable, one solution would be to reduce the TTL, thereby reducing the duration in which we could operate on stale data. So perhaps we reduce the TTL to one minute. This means that our window of staleness is reduced to one-fifth of what it was, but we\u0026rsquo;ve made five times as many calls to the origin, so we have to consider the associated latency and load impact.\nBalancing these forces is going to come down to understanding the requirements of the end user and of the wider system. Users will obviously always want to operate on the freshest data, but not if that means the system falls down under load. Likewise, sometimes the safest thing to do is to turn off features if a cache fails, in order to avoid an overload on the origin causing more serious issues. When it comes to finetuning what, where, and how to cache, you\u0026rsquo;ll often find yourself having to balance along a number of axes. This is just another reason to try to keep things as simple as possible—the fewer the caches, the easier it can be to reason about the system. #principle #architecture/tip\nThe Golden Rule of Caching Treat caching primarily as a performance optimization. Cache in as few places as possible to make it easier to reason about the freshness of data.\nBe careful about caching in too many places! The more caches between you and the source of fresh data, the more stale the data can be, and the harder it can be to determine the freshness of the data that a client eventually sees. It can also be more difficult to reason about where data needs to be invalidated. The trade-off around caching—balancing freshness of data against optimization of your system for load or latency—is a delicate one, and if you cannot easily reason about how fresh (or not) data might be, this becomes difficult.\nComing back to the famous quote from Knuth earlier, premature optimization can cause issues. Caching adds complexity, and we want to add as little complexity as possible. The ideal number of places to cache is zero. Anything else should be an optimization you have to make—but be aware of the complexity it can bring.\nPatterns cache-aside This is the most popular cache pattern. In this pattern, we have an application which will try to fetch data from the cache, and if the data is not found (also known as a \u0026ldquo;cache miss\u0026rdquo;) it will fetch data from the database. Or it will do an expensive computation. And then it will put that data back to the cache before returning the query back to the user.\nIn this pattern we only cache the data that we need, which is advantageous. One disadvantage of this pattern is that the data can become stale if there are lots of updates to the database. This disadvantage can be mitigated by having a \u0026ldquo;Time To Live\u0026rdquo; (or any other expiry pattern)—this is a concept we can skip for now since we\u0026rsquo;ll return to it later.\nAnother disadvantage to this pattern: If there are a lot of cache misses in our application, then the application has to do a lot more work than in the regular flow of just fetching data solely from the database. In this case, the application will first go to the cache, then there will be a cache miss, then it will go back to the database, and then write that data back to the cache before going back to the user.\nIf there are a lot of cache misses, then this cache is causing more problems than it\u0026rsquo;s worth.\nwrite-through In these patterns, the application directly writes the data to the cache. And then the cache synchronously (or asynchronously) writes the data to the database. When we write it synchronously it\u0026rsquo;s called \u0026ldquo;write-through,\u0026rdquo; and when we write it asynchronously it\u0026rsquo;s called \u0026ldquo;write-back\u0026rdquo; (or \u0026ldquo;write-behind\u0026rdquo;). In the asynchronous example, we put the data in a queue, which writes the data back to the database and improves the latency of writes on your application.\nKnow\nIf you\u0026rsquo;re experiencing slow writes, a quick fix is async writes to the database.\nIn both of these patterns, there is an obvious disadvantage. Here we are writing all the data to the cache, which might not even be read. Hence, we are overloading the cache (or cache memory) with expensive calls that might not even be required. For example, there are some accounts on Twitter that are not followed by many people. If we put every tweet in the cache from these unpopular accounts, it will take up expensive memory. Also, if the database goes down before the data is written to the database, this causes inconsistency.\nhttps://redisson.org/glossary/write-through-and-write-behind-caching.html https://www.reddit.com/r/AskProgramming/comments/16bkua0/why_use_a_writethrough_cache_in_distributed/\nin-memory type As example of this solution we can pick Redis.\nPolicies While the LFU method may seem like an intuitive approach to memory management it is not without faults. Consider an item in memory which is referenced repeatedly for a short period of time and is not accessed again for an extended period of time. Due to how rapidly it was just accessed its counter has increased drastically even though it will not be used again for a decent amount of time. This leaves other blocks which may actually be used more frequently susceptible to purging simply because they were accessed through a different method.3\nMoreover, new items that just entered the cache are subject to being removed very soon again, because they start with a low counter, even though they might be used very frequently after that. Due to major issues like these, an explicit LFU system is fairly uncommon; instead, there are hybrids that utilize LFU concepts. https://en.wikipedia.org/wiki/Least_frequently_used#cite_note-4\nProcesses when working with cache\nCache efficiency\nProblems Cache Breakdown (Thundering herd problem). A cache breakdown increases the load on the database dramatically especially when lots of hot keys expire at the same time.\nsharding (consistent hashing) Cache warming\nLinks https://levelup.gitconnected.com/3-caching-problems-every-developer-should-know-1449f07e9166 https://devblogs.microsoft.com/buckh/caching-what-could-go-wrong/ https://netflixtechblog.com/cache-warming-agility-for-a-stateful-service-2d3b1da82642\n","date":"2025-02-22","id":7,"permalink":"/system-design/topics/caching/","summary":"\u003ch1 id=\"ontology\"\u003eontology\u003c/h1\u003e\n\u003cp\u003eCache definitions:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eCaching is a commonly used \u003cem\u003eperformance optimization\u003c/em\u003e (\u0026lt;= really important that it is an optimization) whereby the previous result of some operation is stored so that subsequent requests can use this stored value rather than spending time and resources recalculating the value.\u003c/p\u003e","tags":[],"title":"Caching"},{"content":"I had a SD question to design a public chat room. For something large scale as quora, twitter, reddit, 4chan where users can pop in to a room anytime, read, send, like past/live messages and then exit the chat room, and also see a list of currently running chat rooms. Each chat room can have several 1000s of users at any time.\nAfter laying out reqts-read heavy, where async delays are acceptable, immutable msgs with append only wide columns as a persistent store sharded on chat_room_id.\nFocused mostly on the read msgs part, proposed a write through cache, holding the last ~500 msgs in the cache and then run a timer every 2s to scan the web socket ids from another cache (backed by a let\u0026rsquo;s call it socket relation table), dispatch the msgs on the sockets \u0026amp; mark the low water mark for the active clients on that socket relation table. Also TTL the cache to 2s.\nThe active clients will only see the last ~500 msgs, and if past history is required that will be fetched from the persistent store on demand. And then talked a few mins on book-keeping client entry exit, and also a few mins on ingestion flow.\nI was unable to explain what happens when more than 500 msgs comes in \u0026lt;2s. I proposed a time based instead of a count based cache. But follow up questions and me being on shaky ground. Had a reject. Any ideas?\n","date":"2025-02-22","id":8,"permalink":"/system-design/projects/chat-room/","summary":"\u003cp\u003eI had a SD question to design a public chat room. For something large scale as quora, twitter, reddit, 4chan where users can pop in to a room anytime, read, send, like past/live messages and then exit the chat room, and also see a list of currently running chat rooms. Each chat room can have several 1000s of users at any time.\u003c/p\u003e","tags":[],"title":"Chat Room"},{"content":"","date":"2025-02-22","id":9,"permalink":"/projects/common/chatbots_practice/","summary":"","tags":[],"title":"Chatbots Practice"},{"content":"Requirements Example: Design a code deployment system aimed for developers at a company. They should be able to tag a release, and our system will package it and deploy it to some servers.\nArtifact: (product name, version, commit hash)\nTrigger a release: publishes a code artifact and deploys it to all servers.\nPerformance: 1 hour from release triggered to servers.\nAvailability: Can tolerate some downtime: 99.9% availability.\nData Types Ask your interviewer about the type of these artifacts that we are building.\nCode Artifacts. Type: blobs (ZIP, TAR, bz2, etc.) API 1 2 putRelease: 3 POST release/{productId}/{commitId} 4 returns: deploymentId # Id to check the status of the deployment 5 6 getDeploymentStatus: 7 GET deployment/{deploymentId} 8 returns: status # PENDING, DEPLOYED 9\rScale Ask your interviewer about the scale of these deployments. Here are examples of some good questions to ask (and an interviewer\u0026rsquo;s possible replies):\nCandidate: What\u0026rsquo;s the average size of the artifacts that we need to package?\nInterviewer: We\u0026rsquo;ll say 1 to 10GB.\nCandidate: How many artifacts do we expect to deploy daily?\nInterviewer: In the order of thousands.\nCandidate: How many machines do we need to deploy to?\nInterviewer: Around hundreds.\n","date":"2025-02-22","id":10,"permalink":"/system-design/projects/code_deployment/","summary":"\u003ch1 id=\"requirements\"\u003eRequirements\u003c/h1\u003e\n\u003cp\u003eExample: Design a code deployment system aimed for developers at a company. They should be able to tag a release, and our system will package it and deploy it to some servers.\u003c/p\u003e","tags":[],"title":"Code Deployment"},{"content":"patterns\nmost important points\nresources\nbest practices\nproblems:\nwhere (collaborate on it). knowledge platform.\n","date":"2025-02-22","id":11,"permalink":"/projects/common/collaborative_learning_ts-and-js/","summary":"\u003cp\u003epatterns\u003c/p\u003e\n\u003cp\u003emost important points\u003c/p\u003e\n\u003cp\u003eresources\u003c/p\u003e\n\u003cp\u003ebest practices\u003c/p\u003e\n\u003cp\u003eproblems:\u003c/p\u003e\n\u003cp\u003ewhere (collaborate on it). knowledge platform.\u003c/p\u003e","tags":[],"title":"Collaborative Learning Ts And Js"},{"content":"","date":"2025-02-22","id":12,"permalink":"/projects/common/","summary":"","tags":[],"title":"Common"},{"content":"Common Getting communication between microservices right is problematic for many due in great part, I feel, to the fact that people gravitate toward a chosen technological approach without first considering the different types of communication they might want. In this chapter, I\u0026rsquo;ll try and tease apart the different styles of communication to help you understand the pros and cons of each, as well as which approach will best fit your problem space.\nThus when it comes to the bewildering array of technology available to us for communication between microservices, I think it\u0026rsquo;s important to talk first about the style of communication you want, and only then look for the right technology to imple‐ ment that style. With that in mind, let\u0026rsquo;s take a look at a model I\u0026rsquo;ve been using for several years to help distinguish between the different approaches for microservice- to-microservice communication, which in turn can help you filter the technology options you\u0026rsquo;ll want to look at.\nCommunication Polling vs Streaming sometimes we want to switch between these styles when we want to reduce latency (in what?) we can use streaming (proactively pushing a data using some long-lived open connection throught a socket for example) Collaboration Now we can distinguish at least 2 types of collaboration: request-response collaboration with event-driven collaboration.\nAPI API Gateway\nAn API gateway is an API management tool that sits between a client and a collection of backend services. API gateway is a fully managed service that supports rate limiting, SSL termination, authentication, IP whitelisting, servicing static content, etc.\nApi_gateway\n","date":"2025-02-22","id":13,"permalink":"/system-design/topics/communication/","summary":"\u003ch1 id=\"common\"\u003eCommon\u003c/h1\u003e\n\u003cp\u003eGetting communication between microservices right is problematic for many due in great part, I feel, to the fact that people gravitate toward a chosen technological approach without first considering the different types of communication they might want. In this chapter, I\u0026rsquo;ll try and tease apart the different styles of communication to help you understand the pros and cons of each, as well as which approach will best fit your problem space.\u003c/p\u003e","tags":[],"title":"Communication"},{"content":"https://www.alexdebrie.com/posts/database-consistency/\nconsistency definition, models, etc\nconsistency patterns\nsources: understanding distributes systems DDIA\nhttps://en.wikipedia.org/wiki/Consistency_model https://en.wikipedia.org/wiki/Sequential_consistency\nhttps://systemdesign.one/consistency-patterns/ https://www.designgurus.io/blog/Consistency-Patterns-Distributed-Systems\n","date":"2025-02-22","id":14,"permalink":"/system-design/nfr/consistency/","summary":"\u003cp\u003e\u003ca href=\"https://www.alexdebrie.com/posts/database-consistency/\"\u003ehttps://www.alexdebrie.com/posts/database-consistency/\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"consistency\"\u003econsistency\u003c/h1\u003e\n\u003cp\u003edefinition, models, etc\u003c/p\u003e\n\u003cp\u003econsistency patterns\u003c/p\u003e\n\u003cp\u003esources:\nunderstanding distributes systems\nDDIA\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Consistency_model\"\u003ehttps://en.wikipedia.org/wiki/Consistency_model\u003c/a\u003e\n\u003ca href=\"https://en.wikipedia.org/wiki/Sequential_consistency\"\u003ehttps://en.wikipedia.org/wiki/Sequential_consistency\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://systemdesign.one/consistency-patterns/\"\u003ehttps://systemdesign.one/consistency-patterns/\u003c/a\u003e\n\u003ca href=\"https://www.designgurus.io/blog/Consistency-Patterns-Distributed-Systems\"\u003ehttps://www.designgurus.io/blog/Consistency-Patterns-Distributed-Systems\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"Consistency"},{"content":"https://en.wikipedia.org/wiki/Critical_thinking\n#todo make a diff / comparison with Reflective_practice first assumption: reflective practice is focused on past actions whereby critical thinking is about\n","date":"2025-02-22","id":15,"permalink":"/projects/common/critical_thinking/","summary":"\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Critical_thinking\"\u003ehttps://en.wikipedia.org/wiki/Critical_thinking\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e#todo make a diff / comparison with \u003ca href=\"http://localhost:1313/projects/foundations/reflective_practice/\"\u003eReflective_practice\u003c/a\u003e\nfirst assumption: reflective practice is focused on past actions whereby critical thinking is about\u003c/p\u003e","tags":[],"title":"Critical Thinking"},{"content":"","date":"2025-02-22","id":16,"permalink":"/engineering/data/","summary":"","tags":[],"title":"Data"},{"content":"data access pattern\nTypes: SQL, Key-Value Stores, Blob Stores (S3, GCS), Timelines DB, Graph(Neo4j), Spatial (QuadTree)\nsql Pros: ACID transactions, imposing very strict structure =\u0026gt; querying capabilities, DB indexing, Data Normalization.\nk-v stores pros: caching, no-strict scheme, speed examples\nfunctional partitioning sharding Divide a data store into a set of horizontal partitions or shards. This can improve scalability when storing and accessing large volumes of data.\nThere are some questions to answer: how to split up a data? Where shards need to be placed? How to prevent hot spots and deal with them?\nShard number determination logic should be placed in RP (reversed proxy) doing this on behalf of database\u0026rsquo; shards.\nContext and problem\nwhat is it? trade-offs\ndenormalization indexing definitions.\nIndexing is a mechanism by which the underlying data is mapped for faster retrieval. For a system to process an instruction involving data access, these are the certain steps involved:\nproblems\ncons: -\u0026gt; This means that by maintaining an index, we could reduce the I/O calls to the disk substantially, from 25 calls before the index to 2 calls (one for the index and the other for the specific block).\nmultilevel index B-Trees, B+ trees\nhttps://jepsen.io/analyses\n","date":"2025-02-22","id":17,"permalink":"/engineering/data/databases/","summary":"\u003cp\u003edata access pattern\u003c/p\u003e\n\u003cp\u003eTypes: SQL, Key-Value Stores, Blob Stores (S3, GCS), Timelines DB, Graph(Neo4j), Spatial (QuadTree)\u003c/p\u003e\n\u003ch1 id=\"sql\"\u003esql\u003c/h1\u003e\n\u003cp\u003ePros: ACID transactions, imposing very strict structure =\u0026gt; querying capabilities, DB indexing, Data Normalization.\u003c/p\u003e","tags":[],"title":"Databases"},{"content":"Sources distributed: book \u0026ldquo;thinking in distributed systems\u0026rdquo; book \u0026ldquo;understanding distributed systems\u0026rdquo;\nMartin Kleppman course videos: https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB + pdf lecture notes\nDDiA by Kleppman\nhttps://martinfowler.com/articles/patterns-of-distributed-systems/\nhttps://www.the-paper-trail.org/post/2014-08-09-distributed-systems-theory-for-the-distributed-systems-engineer/\nMIT distributed systems course (quite good as it contains much practice) https://pdos.csail.mit.edu/6.824/schedule.html https://muratbuffalo.blogspot.com/2020/06/learning-about-distributed-systems.html\nbookmarks in dev folder\nbooks in SD folder\nUdi Dahan \u0026ldquo;advanced distributed system design\u0026rdquo; (coursehunter)\nhttps://aws.amazon.com/builders-library/challenges-with-distributed-systems/ https://aws.amazon.com/builders-library/avoiding-fallback-in-distributed-systems/ https://aws.amazon.com/builders-library/leader-election-in-distributed-systems https://aws.amazon.com/builders-library/instrumenting-distributed-systems-for-operational-visibility\nhttps://www.designgurus.io/blog/consistency-patterns-distributed-system https://www.designgurus.io/course-play/grokking-the-advanced-system-design-interview/doc/12-split-brain\nbooks: tanenbaum, understanding distributed systems, thinking in distributed systems,\ntheorems,\narticle from one man, about distributed systems or db?\n","date":"2025-02-22","id":18,"permalink":"/system-design/topics/distributed-systems/","summary":"\u003ch1 id=\"sources\"\u003eSources\u003c/h1\u003e\n\u003cp\u003edistributed:\nbook \u0026ldquo;thinking in distributed systems\u0026rdquo;\nbook \u0026ldquo;understanding distributed systems\u0026rdquo;\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMartin Kleppman course videos: \u003ca href=\"https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB\"\u003ehttps://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB\u003c/a\u003e + pdf lecture notes\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDDiA by Kleppman\u003c/p\u003e","tags":[],"title":"Distributed Systems"},{"content":"","date":"2025-02-22","id":19,"permalink":"/system-design/elements/","summary":"","tags":[],"title":"Elements"},{"content":"syn: element, pattern, high-level strategy\nFrom https://docs.microsoft.com/en-us/azure/architecture/patterns/ interviewing.io system-design-primer\nDistributed computing with MapReduce Consistent hashing Scatter gather stateless services and idempotent API as a key to scalability pattern description blueprint: a definition of a pattern; which problems this patterns solves; specific algorithm(s) of work and {scenario}/{case};\ntrade-offs and analogues; specific implementations / solutions { s }\nalso, you can introduce a rule of thumb practice (when to use )\ndns todo: add description and usage of DNS in a SD interview\ncdn A content delivery network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user.\nServing content from CDNs can significantly improve performance in two ways:\nUsers receive content from data centers close to them Your servers do not have to serve requests that the CDN fulfills Load Balancer Load balancing refers to efficiently distributing incoming network traffic across a group of backend servers, also known as a server farm or server pool.\nWhy do we need LB? LB is to provide redundancy, reliability, and improve performance.\nterms: LB policy (server selection strategy), backend set list (server pool).\nLB policies: purely random, RR, weighted RR, load- or timeout-based, least (+weighted) connections, ip-based (hashing), path based (functional decomposition), combined policy and model.\nOn which levels LB can be done? L4 and L7\nService Discovery.\nService discovery is the mechanism the load balancer uses to discover the pool of servers it can route requests to. A naive way to implement it is to use a static configuration file that lists the IP addresses of all the servers, which is painful to manage and keep up to date. A more flexible solution is to have a fault-tolerant coordination service, like, e.g., etcd or Zookeeper, manage the list of servers. When a new server comes online, it registers itself to the coordination service with a TTL. When the server unregisters itself, or the TTL expires because it hasn\u0026rsquo;t renewed its registration, the server is removed from the pool.\nExamples of Load Balancers: nginx, haproxy, \u0026hellip; .\nproxy Proxy runs on behalf of client where is RP is running on behalf of server. Can do many interesting things: filter requests, logging, caching, LB, changing infromation in requests and responses.\nAPI Gateway Api_gateway\ntrade-offs\npub/sub todo: resolve some problems, situations (for example, rebalancing) See \u0026rsquo;events\u0026rsquo; file for more detailed view. todo: refactor to messaging\nasynchronism message queues, task queues Queues\nsidecar pattern Segregating the functionalities of an application into a separate process can be viewed as Sidecar Pattern.\nservice mesh The evolution of service mesh architecture has been a game changer. It shifts the complexity associated with microservice architecture to a separate infrastructure layer and provides a lot of functionalities like load balancing, service discovery, traffic management, circuit breaking, telemetry, fault injection, and more.\ndistributed calculations key-value store IM data grid\nrate limiting going distributed problems and solutions: tier-base RL is more complicated\nsecurity The question of security is all about the trade off between total safety (a wall) vs total convenience (a hole in the wall).\nprocesses: authentication (figuring out who are you talking to) and authorization ()\nsources: sam newmans book, interviewing.io sd section,\nThat way you can rely on two factors to authenticate who\u0026rsquo;s at the door: looking through the peephole and asking for the password to authenticate.\nhashing (store the hashes of passwords in a db) salting, rainbow tables\nsequencer from https://www.educative.io/courses/grokking-modern-system-design-interview-for-engineers-managers/system-design-sequencer\n","date":"2025-02-22","id":20,"permalink":"/system-design/elements/elements/","summary":"\u003cp\u003esyn: element, pattern, high-level strategy\u003c/p\u003e\n\u003cp\u003eFrom\nhttps://docs.microsoft.com/en-us/azure/architecture/patterns/\ninterviewing.io\nsystem-design-primer\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDistributed computing with MapReduce\u003c/li\u003e\n\u003cli\u003eConsistent hashing\u003c/li\u003e\n\u003cli\u003eScatter gather\nstateless services and idempotent API as a key to scalability\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003epattern description blueprint:\u003c/strong\u003e\na definition of a pattern;\nwhich problems this patterns solves;\nspecific algorithm(s) of work and {scenario}/{case};\u003cbr\u003e\ntrade-offs and analogues;\nspecific implementations / solutions { s }\u003c/p\u003e","tags":[],"title":"Elements"},{"content":"","date":"2025-02-22","id":21,"permalink":"/engineering/","summary":"","tags":[],"title":"Engineering"},{"content":"Common Description In this section we are dealing with various engineering intricancises.\n","date":"2025-02-22","id":22,"permalink":"/engineering/engineering-guide/","summary":"\u003ch1 id=\"common-description\"\u003eCommon Description\u003c/h1\u003e\n\u003cp\u003eIn this section we are dealing with various engineering intricancises.\u003c/p\u003e","tags":[],"title":"Engineering Guide"},{"content":"Sources https://www.hellointerview.com/blog/mastering-estimation #todo\nHow to do it According to Jeff Dean, Google Senior Fellow, \u0026ldquo;back-of-the-envelope calculations are estimates you create using a combination of thought experiments and common performance numbers to get a good feel for which designs will meet your requirements\u0026rdquo; Commonly asked back-of-the-envelope estimations: QPS, peak QPS, storage, cache, number of servers, etc. You can practice these calculations when preparing for an interview. Practice makes perfect.\nThroughput of each layer Latency caused between each layer Overall latency justification char: rps, volumes, etc\nWhen in doubt, just guess higher—it\u0026rsquo;s called margin of safety. For our Twitter example we can go for these numbers: Reads/minute: 100k Writes/minute: 1k\nhttps://www.reddit.com/r/ExperiencedDevs/comments/19e19jn/what_is_the_point_of_back_of_the_envelope/\nDAU (daily active users), QPS (query per seconds) Strategy?? First, we need to recognize a limited resource in our system, then approximate the actual usage. For example, our servers are capped by 2GHz CPUs, and we would like to know if we can serve all user requests using a single server. So how to approximate the actual usage? We need to break down the usage to its constituting factors, make a rough estimate of those factors (if needed, further breaking them down), and combining them. For example, we might expect to have 1K active users, each issuing 15 requests per day. That\u0026rsquo;s 15K requests per day, or 15K/86400 requests per second. When combining the parts, a trick is to round aggressively. Noone wants to divide by 86400. So let\u0026rsquo;s round to 20K/100K, leaving 0.2 seconds time available to serve a single request. If we know that a single request roughly takes 0.7 seconds to serve, we need to bring up at least 4 machines. Of course you don\u0026rsquo;t want to live on the edge, so let\u0026rsquo;s add some buffer and make that 10 machines (which is also a nicer number). Dimensions to approximate Find typical limited dimensions, along with exercises below.\nNetwork bandwidth Assuming 1Gbps link per machine, if we want to crawl 70TB of websites every day, how many machines would a crawler system need?\nStorage space How much space would it take to store the contents of 100M web pages? What if we substitute each word by an integer index? How many machines of 64GB SSD would it fit?\nIO throughput You store fetched web pages on a mechanical hard drive, with limited random access speed. Users issue requests with 100 query per sec (qps), each request typically returning the content of 20 pages. How many hard drives would you need to keep request latency low?\nEngineering effort. You need to deliver a new feature. There are 5 programmers and 40 tasks. How many weeks until possible launch?\nMoney. A user pays $10 a month for your image store service, storing all their photos, each downsized to 3MB. During a month a user fetches 1K photos. Find the pricing page of your favorite cloud provider, and calculate the cost associated with each user. How much is your revenue per user? Check for different assumed photo counts.\nOthers include CPU time, RAM size, latencies of various kinds (disk access, RAM access, network), thread count. Where to start? Enumerate typical use-cases of the system and determine the most critical resources they need. A document store will need lots of storage. Guesstimating document sizes and counts is a good start, but further details will depend on usage. How often are new documents added? Are the documents searchable? Do we need any indices? Is it a write-heavy or read-heavy store?\nDifferent use-cases will likely need very different shapes of resources. For example, serving the documents might need lots of RAM but not CPU, while preprocessing of new documents just the other way around. Hosting all those services on homogeneous machines would waste both CPU and RAM, since you need to get machines which are maxed on both dimensions.\nSuch differences indicate those features should be split to different services, hosted on independent sets of machines.\nIt\u0026rsquo;s always a good idea to estimate the scale of the system you\u0026rsquo;re going to design. This would also help later when you\u0026rsquo;ll be focusing on scaling, partitioning, load balancing and caching.\nWhat scale is expected from the system (e.g., number of new tweets, number of tweet views, how many timeline generations per sec., etc.) How much storage would we need? This will depend on whether users can upload photos and videos in their tweets? What network bandwidth usage are we expecting? This would be crucial in deciding how would we manage traffic and balance load between servers. Enumerate typical use-cases of the system and determine the most critical resources they need. .. I have seen 2 approaches taken when calculating the back of the envelope calculations. The first approach as you have listed out in the bullet points starts with an overall picture of the system and calculations move to a single server and memory requirements. That is if there are 330 million active users and 5700 tweets a second, how do I get to what specs will be required for a single server and thereby calculating how many servers/DBs are needed, etc. Under the interview pressure, I always felt this process to be a bit difficult when performing larger divisions. To quote your example \u0026ldquo;\u0026hellip; So we need 60000 * 0.95 / 320 = 178 servers\u0026rdquo;. There is no way I can do this calculation on the whiteboard in live interview without sweating myself. The second approach, which I always preferred is to start small and grow bigger with quite a few approximations. After all, the back of the envelope calculation is supposed to be a T-shift level \u0026ldquo;estimation\u0026rdquo;. I also often start with a small number of variables preferably one. For example, instead of managing 2 variables like \u0026ldquo;Number of active users\u0026rdquo; and \u0026ldquo;Number of tweets\u0026rdquo;, I start at the server level and ask myself a question, what factor affects my server the most? a number of active users coming to the server or number of tweets coming to the server. If my server gets 10 tweets per second, does it matter in terms of memory and threads requirements if 10 active users send 1 tweet/sec or 1 active user sends 10 tweets/sec? If it does not matter then I, for now, I will ignore the number of active users and focus on how many tweets the server receives per second. My 2 variables are down to 1. I also make sure, I never talk on the specifics of the functionality and instead talk/focus on the raw/common server requirements. That is instead of saying, the server receives 10 tweets per second, I will say the server receives 10 \u0026ldquo;requests\u0026rdquo; per second. Converting tweets to requests helps me memorize the same logic across twitter design where the server receives tweets and facebook design where the server receives photo upload and comments requests. Everything is incoming request no differentiation. Example. Ok so focusing on the twitter calculations, I would start something like this. I will start saying, I will at minimum calculate servers, memory, and storage requirements\nStarting small, I will say, assuming, the application gets 1000 requests per second, (1000 is an easy nice number for any calculation and we can scale up or down easily depending on the requirement. The real twitter number would be much higher)\n1000 requests/sec 3600 seconds per hour, it will be 1000 4000 (approximating 3600 to nearest whole number 4000 as multiplication by 4K is much easier orally than 3.6K) =\u0026gt; we get 4 million requests/hr 4M requests/hr translates to 4M 30 hours (instead of 24 hours in a day as its much easier to multiply by 30 than 24) =\u0026gt; 120 million requests/day 120 million requests/day translates to 120M 400 days (instead of 365) = 50 billion requests/year (instead of 48B) Assuming the capacity planning estimates are for 5 years, we get 50B 5 = 250 Billion request data is what we may end up storing in our system. Now to calculate the number of servers, From the experience of running applications in production, I know say a tomcat/jetty server running Spring boot application at a minimum will have 100 worker thread (200 default) to handle HTTP requests\nThe server will handle tweets, photos, video uploads handling 1000 requests with 100 threads I would use 10 + 20% more = 12 servers. If 1000 requests change to 10000 requests, 12 servers would more or less convert to 120 servers. For server memory requirements of the server: Now for server memory requirements, the capacity required to handle requests with video and images would be much higher compared to tweet,\nAssuming photos are 1MB in upload size (Usually a UI side compression library will reduce a photo image size to be around 500KB, but 1 MB is easy for calculation) and videos to be 50 MB in size To handle 100 requests/sec for video uploads, 100 * 50MB = 5GB of memory for each commodity server. For Storage requirement, assuming we need to store data for 5 years\nAs previously calculated, 250 Billion request data to store for 5 years, assuming 10% to be for videos (50MB avg), 20% for photos (1MB avg) and 70% for tweets (200KB avg) we need \u0026ndash; Note, usual conversions are (1000 translates to KB storage, 1 million translates to 1 MB of storage, 1 billion translates to 1 GB of stroage) 10% Video: 250 Billion request data (that is 250 GB) 10% =\u0026gt; 25 GB 50MB ~~ 25000MB 50MB ~~ 1250000 MB =\u0026gt; 1250 GB =\u0026gt; 1.2TB 20% Photos: 250 Billion request data 20 % =\u0026gt; 50 GB 1MB =\u0026gt; 50000 MB 1MB =\u0026gt; 50000 MB =\u0026gt; 50GB 70% Tweets: 250 Billion request data 70% ~~ 200 GB 200 bytes =\u0026gt; 200000MB * 0.002MB =\u0026gt; 400MB Total (1.2TB + 50 GB + 400 MB) ~~ 1.2TB (in reality this capacity will be much higher as video/photo storage size requirements will be much higher but I hope reader gets the point) Summary\nStart with a single variable and translate specific design requirement into raw server requirements like requests/sec (instead of tweets/sec or photos/server) Start from a single server requirement instead of trying to divide total tweets or total storage by servers. Remember to get all the calculations done in 5 mins. Unless the interviewer wants to focus on the specifics calcuations. Remember these contents are high-level estimates.\n","date":"2025-02-22","id":23,"permalink":"/system-design/topics/envelope_estimations/","summary":"\u003ch1 id=\"sources\"\u003eSources\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.hellointerview.com/blog/mastering-estimation\"\u003ehttps://www.hellointerview.com/blog/mastering-estimation\u003c/a\u003e #todo\u003c/p\u003e\n\u003ch1 id=\"how-to-do-it\"\u003eHow to do it\u003c/h1\u003e\n\u003cp\u003eAccording to Jeff Dean, Google Senior Fellow, \u0026ldquo;back-of-the-envelope calculations are estimates you create using a combination of thought experiments and common performance numbers to get a good feel for which designs will meet your requirements\u0026rdquo;\nCommonly asked back-of-the-envelope estimations: QPS, peak QPS, storage, cache, number of servers, etc. You can practice these calculations when preparing for an interview. Practice makes perfect.\u003c/p\u003e","tags":[],"title":"Envelope Estimations"},{"content":"Heading One text\nHeading Two projects\n","date":"2025-02-22","id":24,"permalink":"/system-design/elements/background/events/","summary":"\u003ch1 id=\"heading-one\"\u003eHeading One\u003c/h1\u003e\n\u003cp\u003etext\u003c/p\u003e\n\u003ch2 id=\"heading-two\"\u003eHeading Two\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://localhost:1313/projects/about/\"\u003eprojects\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"Events"},{"content":"Sources https://microservices.io/patterns/data/saga.html coursehunter https://www.youtube.com/watch?v=SbL3a9YOW7s https://eventuate.io/docs/manual/eventuate-tram/latest/about-eventuate-tram.html https://www.youtube.com/watch?v=gA2-eqDVSng https://aws.amazon.com/blogs/architecture/lets-architect-designing-event-driven-architectures/ https://aws.amazon.com/messaging/ https://aws.amazon.com/blogs/compute/choosing-between-messaging-services-for-serverless-applications/ https://aws.amazon.com/event-driven-architecture/ https://www.youtube.com/watch?v=28B4L1fnnGM https://www.youtube.com/@CodeOpinion/\nhttps://www.youtube.com/watch?v=A_mstzRGfIE https://www.developertoarchitect.com/lessons/lesson165.html\nhttps://medium.com/wix-engineering/6-event-driven-architecture-patterns-part-1-93758b253f47\nevent tips: https://www.youtube.com/watch?v=9r9WDzzTcr0 https://serverlessland.com/event-driven-architecture/visuals/good-and-hard-parts-of-event-architectures https://www.aklivity.io/post/the-continued-rise-of-event-driven-architectures https://www.infoq.com/presentations/event-driven-arch-challenges/ https://www.equalexperts.com/blog/tech-focus/event-driven-architecture-the-good-the-bad-and-the-ugly/ https://serverlessland.com/event-driven-architecture/visuals/common-issued-with-eda\ncommon https://www.developertoarchitect.com/resources.html\ntopics: tools: Kafka, RabbitMQ, SQS, SNS, EventBridge models: pub/sub, \u0026hellip; managing distributed models frameworks: https://eventuate.io/\npatterns\nCommon Pros: refactor v isolation delivery semantics ordering replaying or repeated consuming (by persistence mechanishm) content-based filtering retention scalability? In Kafka there is distributing partitions over brokers (horizontal scalability).\nEvents used to:\nEvent Sourcing Event Carried State Transfer Domain Events Integration Events Workflow Events event schemas idempotency\nArchitectural Charateristics:\nMQ should be configurable to high throughput or low-latency Terms Integration (definition?) (protocol, format, data schema \u0026amp; evolution) Data Stream - continuous flow of data. Some examples of data streams include sensor data, activity logs from web browsers, and financial transaction logs. General Characteristics of Data Streams: time sensitive, continuous, heterogeneous, imperfect. Messaging models The most popular messaging models are point-to-point and publish-subscribe.\nPoint-to-point Point-to-point model is common for traditional message queues. In this model, a message sent to a queue can be consumed by one and only one consumer.\nPub/Sub pattern Before discussing the specifics of Apache Kafka, it is important for us to understand the concept of publish/subscribe messaging and why it is a critical component of data-driven applications. Publish/subscribe (pub/sub) messaging is a pattern that is characterized by the sender (publisher) of a piece of data (message) not specifically directing it to a receiver. Instead, the publisher classifies the message somehow, and that receiver (subscriber) subscribes to receive certain classes of messages. Pub/sub systems often have a broker, a central point where messages are published, to facilitate this pattern.\nISR (In-Sync Replicas) Delivery Semantics implementing delivery semantics in Kafka\nRetry composition Parameters and Limits Event-Sourcing ES is about state, about persisting state. It\u0026rsquo;s an iternal implementation detail.\n","date":"2025-02-22","id":25,"permalink":"/system-design/projects/events/","summary":"\u003ch1 id=\"sources\"\u003eSources\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://microservices.io/patterns/data/saga.html\"\u003ehttps://microservices.io/patterns/data/saga.html\u003c/a\u003e\ncoursehunter\n\u003ca href=\"https://www.youtube.com/watch?v=SbL3a9YOW7s\"\u003ehttps://www.youtube.com/watch?v=SbL3a9YOW7s\u003c/a\u003e\n\u003ca href=\"https://eventuate.io/docs/manual/eventuate-tram/latest/about-eventuate-tram.html\"\u003ehttps://eventuate.io/docs/manual/eventuate-tram/latest/about-eventuate-tram.html\u003c/a\u003e\n\u003ca href=\"https://www.youtube.com/watch?v=gA2-eqDVSng\"\u003ehttps://www.youtube.com/watch?v=gA2-eqDVSng\u003c/a\u003e\n\u003ca href=\"https://aws.amazon.com/blogs/architecture/lets-architect-designing-event-driven-architectures/\"\u003ehttps://aws.amazon.com/blogs/architecture/lets-architect-designing-event-driven-architectures/\u003c/a\u003e\n\u003ca href=\"https://aws.amazon.com/messaging/\"\u003ehttps://aws.amazon.com/messaging/\u003c/a\u003e\n\u003ca href=\"https://aws.amazon.com/blogs/compute/choosing-between-messaging-services-for-serverless-applications/\"\u003ehttps://aws.amazon.com/blogs/compute/choosing-between-messaging-services-for-serverless-applications/\u003c/a\u003e\n\u003ca href=\"https://aws.amazon.com/event-driven-architecture/\"\u003ehttps://aws.amazon.com/event-driven-architecture/\u003c/a\u003e\n\u003ca href=\"https://www.youtube.com/watch?v=28B4L1fnnGM\"\u003ehttps://www.youtube.com/watch?v=28B4L1fnnGM\u003c/a\u003e\n\u003ca href=\"https://www.youtube.com/@CodeOpinion/\"\u003ehttps://www.youtube.com/@CodeOpinion/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=A_mstzRGfIE\"\u003ehttps://www.youtube.com/watch?v=A_mstzRGfIE\u003c/a\u003e\n\u003ca href=\"https://www.developertoarchitect.com/lessons/lesson165.html\"\u003ehttps://www.developertoarchitect.com/lessons/lesson165.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://medium.com/wix-engineering/6-event-driven-architecture-patterns-part-1-93758b253f47\"\u003ehttps://medium.com/wix-engineering/6-event-driven-architecture-patterns-part-1-93758b253f47\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eevent tips:\n\u003ca href=\"https://www.youtube.com/watch?v=9r9WDzzTcr0\"\u003ehttps://www.youtube.com/watch?v=9r9WDzzTcr0\u003c/a\u003e\n\u003ca href=\"https://serverlessland.com/event-driven-architecture/visuals/good-and-hard-parts-of-event-architectures\"\u003ehttps://serverlessland.com/event-driven-architecture/visuals/good-and-hard-parts-of-event-architectures\u003c/a\u003e\n\u003ca href=\"https://www.aklivity.io/post/the-continued-rise-of-event-driven-architectures\"\u003ehttps://www.aklivity.io/post/the-continued-rise-of-event-driven-architectures\u003c/a\u003e\n\u003ca href=\"https://www.infoq.com/presentations/event-driven-arch-challenges/\"\u003ehttps://www.infoq.com/presentations/event-driven-arch-challenges/\u003c/a\u003e\n\u003ca href=\"https://www.equalexperts.com/blog/tech-focus/event-driven-architecture-the-good-the-bad-and-the-ugly/\"\u003ehttps://www.equalexperts.com/blog/tech-focus/event-driven-architecture-the-good-the-bad-and-the-ugly/\u003c/a\u003e\n\u003ca href=\"https://serverlessland.com/event-driven-architecture/visuals/common-issued-with-eda\"\u003ehttps://serverlessland.com/event-driven-architecture/visuals/common-issued-with-eda\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ecommon \u003ca href=\"https://www.developertoarchitect.com/resources.html\"\u003ehttps://www.developertoarchitect.com/resources.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003etopics:\ntools: Kafka, RabbitMQ, SQS, SNS, EventBridge\nmodels: pub/sub, \u0026hellip;\nmanaging distributed models\nframeworks:\n\u003ca href=\"https://eventuate.io/\"\u003ehttps://eventuate.io/\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"Events"},{"content":"","date":"2025-02-22","id":26,"permalink":"/projects/foundations/","summary":"","tags":[],"title":"Foundations"},{"content":"Delivery Framework Our delivery framework is a sequence of steps and timings we recommend for your interview. By structuring your interview in this way, you\u0026rsquo;ll stay focused on the bits that are most important to your interviewer. An added benefit is that you\u0026rsquo;ll have a clear path to fall back if you\u0026rsquo;re overwhelmed. Many candidates are (understandably!) nervous in their interview. It\u0026rsquo;s easy to get lost if you aren\u0026rsquo;t building up a solution in a linear way.\nFollow a Structure\nWhile a firm structure to your approach is important and your interviewer is not trained specifically to assess you on your delivery (often this gets bucketed into \u0026ldquo;communication\u0026rdquo;), in practice we\u0026rsquo;ve seen many candidates that perform significantly better by following a structure which both keeps them from getting stuck and ensures they deliver a working system.\nRequirements (~5 minutes) The goal of the requirements section is to get a clear understanding of the system that you are being asked to design. To do this, we suggest you break your requirements into two sections.\nFunctional Requirements Functional requirements are your \u0026ldquo;Users/Clients should be able to\u0026hellip;\u0026rdquo; statements. These are the core features of your system and should be the first thing you discuss with your interviewer. Oftentimes this is a back and fourth with your interviewer. Ask targeted questions as if you were talking to a client, customer, or product manager (\u0026ldquo;does the system need to do X?\u0026rdquo;, \u0026ldquo;what would happen if Y?\u0026rdquo;) to arrive at a prioritized list of core features.\nFor example, if you were designing a system like Twitter, you might have the following functional requirements:\nUsers should be able to post tweets Users should be able to follow other users Users should be able to see tweets from users they follow A cache meanwhile might have requirements like:\nClients should be able to insert items Clients should be able to set expirations Clients should be able to read items Keep your requirements targeted!\nThe main objective in the remaining part of the interview is to develop a system that meets the requirements you\u0026rsquo;ve identified \u0026ndash; so it\u0026rsquo;s crucial to be strategic in your prioritization. Many of these systems have hundreds of features, but it\u0026rsquo;s your job to identify and prioritize the top 3. Having a long list of requirements will hurt you more than it will help you and many top FAANGs directly evaluate you on your ability to focus on what matters.\nNon-functional Requirements Non-functional requirements are statements about the system qualities that are important to your users. These can be phrased as \u0026ldquo;The system should be able to\u0026hellip;\u0026rdquo; or \u0026ldquo;The system should be\u0026hellip;\u0026rdquo; statements.\nFor example, if you were designing a system like Twitter, you might have the following non-functional requirements:\nThe system should be highly availability, prioritizing availability over consistency The system should be able to scale to support 100M+ DAUs The system should be low latency, rendering feeds in under 200ms Quantification\nIt\u0026rsquo;s important that non-functional requirements are put in the context of the system and, where possible, are quantified. For example, \u0026ldquo;the system should be low latency\u0026rdquo; is obvious and not very meaningful—nearly all systems should be low latency.\n\u0026ldquo;The system should have low latency search, \u0026lt; 500ms,\u0026rdquo; is much more useful as it identifies the part of the system that most needs to be low latency and provides a target.\nComing up with non-functional requirements can be challenging, especially if you\u0026rsquo;re not familiar with the domain. Here is a checklist of things to consider that might help you identify the most important non-functional requirements for your system. You\u0026rsquo;ll want to identify the top 3-5 that are most relevant to your system.\nCAP Theorem: Should your system prioritize consistency or availability? Note, partition tolerance is a given in distributed systems. Environment Constraints: Are there any constraints on the environment in which your system will run? For example, are you running on a mobile device with limited battery life? Running on devices with limited memory or limited bandwidth (e.g. streaming video on 3G)? Scalability: All systems need to scale, but does this system have unique scaling requirements? For example, does it have bursty traffic at a specific time of day? Are there events, like holidays, that will cause a significant increase in traffic? Also consider the read vs write ratio here. Does your system need to scale reads or writes more? Latency: How quickly does the system need to respond to user requests? Specifically consider any requests that require meaningful computation. For example, low latency search when designing Yelp. Durability: How important is it that the data in your system is not lost? For example, a social network might be able to tolerate some data loss, but a banking system cannot. Security: How secure does the system need to be? Consider data protection, access control, and compliance with regulations. Fault Tolerance: How well does the system need to handle failures? Consider redundancy, failover, and recovery mechanisms. Compliance: Are there legal or regulatory requirements the system needs to meet? Consider industry standards, data protection laws, and other regulations. Capacity Estimation Many guides you\u0026rsquo;ve read will suggest doing back-of-the-envelope calculations at this stage. We believe this is often unnecessary. Instead, perform calculations only if they will directly influence your design. In most scenarios, you\u0026rsquo;re dealing with a large, distributed system – and it\u0026rsquo;s reasonable to assume as much. Many candidates will calculate storage, DAU, and QPS, only to conclude, \u0026ldquo;ok, so it\u0026rsquo;s a lot. Got it.\u0026rdquo; As interviewers, we gain nothing from this except that you can perform basic arithmetic.\nOur suggestion is to explain to the interviewer that you would like to skip on estimations upfront and that you will do math while designing when/if necessary. When would it be necessary? Imagine you are designing a TopK system for trending topics in FB posts. You would want to estimate the number of topics you would expect to see, as this will influence whether you can use a single instance of a data structure like a min-heap or if you need to shard it across multiple instances, which will have a big impact on your design.\nRegardless of how you end up using it in the interview, learning to estimate relevant quantities quickly will help you quick reason through design trade-offs in your design. Don\u0026rsquo;t worry if you\u0026rsquo;re not good at mental arithmetic under pressure, most people aren\u0026rsquo;t.\nCore Entities (~2 minutes) Next you should take a moment to identify and list the core entities of you system. This helps you to define terms, understand the data central to your design, and gives you a foundation to build on. These are the core entities that your API will exchange and that your system will persist in a Data Model. In the actual interview, this is as simple as jotting down a bulleted list and explaining this is your first draft to the interviewer.\nWhy not list the entire data model at this point? Because you don\u0026rsquo;t know what you don\u0026rsquo;t know. As you design your system, you\u0026rsquo;ll discover new entities and relationships that you didn\u0026rsquo;t anticipate. By starting with a small list, you can quickly iterate and add to it as you go. Once you get into the high level design and have a clearer sense of exactly what state needs to update upon each request you can start to build out the list of relevant columns/fields for each entity.\nFor our Twitter example, our core entities are rather simple:\nUser Tweet Follow A couple useful questions to ask yourself to help identify core entities:\nWho are the actors in the system? Are they overlapping? What are the nouns or resources necessary to satisfy the functional requirements? Aim to choose good names for your entities. While most problems are small enough that you could probably sub in foo and bar for any entity in your system, some interviewers use this as an opportunity to see whether you\u0026rsquo;re any good at one of the hardest problems in computer science.\nAPI or System Interface (~5 minutes) Before you get into the high-level design, you\u0026rsquo;ll want to define the contract between your system and its users. Oftentimes, especially for full product style interviews, this maps directly to the functional requirements you\u0026rsquo;ve already identified (but not always!). You will use this contract to guide your high-level design and to ensure that you\u0026rsquo;re meeting the requirements you\u0026rsquo;ve identified.\nYou have a quick decision to make here \u0026ndash; do you want to design a RESTful API or a GraphQL API?\nRESTful API: The standard communication constraints of the internet. Uses HTTP verbs (GET, POST, PUT, DELETE) to perform CRUD operations on resources.\nGraphQL API: A newer communication protocol that allows clients to specify exactly what data they want to receive from the server.\nWire Protocol: If you\u0026rsquo;re communicating over websockets or raw TCP sockets, you\u0026rsquo;ll want to define the wire protocol. This is the format of the data that will be sent over the network, usually in the format of messages.\nDon\u0026rsquo;t overthink this. Bias toward creating a REST API. Use GraphQL only if you really need clients to fetch only the requested data (no over- or under- fetching). If you\u0026rsquo;re going to use websockets, you\u0026rsquo;ll want to describe the wire protocol.\nFor Twitter, we would choose REST and would have the following endpoints. Notice how we can use our core entities as the objects that are exchanged via the API.\nPOST /v1/tweet body: { \u0026quot;text\u0026quot;: string } GET /v1/tweet/:tweetId -\u0026gt; Tweet POST /v1/follow/:userId GET /v1/feed -\u0026gt; Tweet[]\n[!danger] Notice how there is no userId in the POST /v1/tweet endpoint? This is because we will get the id of the user initiating the request from the authentication token in the request header. Putting sensitive information like user ids in the request body is a security risk and a mistake that many candidates make. Don\u0026rsquo;t be one of them!\n[Optional] Data Flow (~5 minutes) For some backend systems, especially data-processing systems, it can be helpful to describe the high level sequence of actions or processes that the system performs on the inputs to produce the desired outputs. If your system doesn\u0026rsquo;t involve a long sequence of actions, skip this!\nWe usually define the data flow via a simple list. You\u0026rsquo;ll use this flow to inform your high-level design in the next section.\nFor a web crawler, this might look like:\nFetch seed URLs Parse HTML Extract URLs Store data Repeat High Level Design (~10-15 minutes) Now that you have a clear understanding of the requirements, entities, and API of your system, you can start to design the high-level architecture. This consists of drawing boxes and arrows to represent the different components of your system and how they interact. Components are basic building blocks like servers, databases, caches, etc. This can be done either in person on a whiteboard or virtually using whiteboarding software like Excalidraw. The Key Technologies section below will give you a good sense of the most common components you\u0026rsquo;ll need to know.\nDon\u0026rsquo;t over think this! Your primary goal is to design an architecture that satisfies the API you\u0026rsquo;ve designed and, thus, the requirements you\u0026rsquo;ve identified. In most cases, you can even go one-by-one through your API endpoints and build up your design sequentially to satisfy each one.\nStay focused!\nIt\u0026rsquo;s incredibly common for candidates to start layering on complexity too early, resulting in them never arriving at a complete solution. Focus on a relatively simple design that meets the core functional requirements, and then layer on complexity to satisfy the non-functional requirements in your deep dives section. It\u0026rsquo;s natural to identify areas where you can add complexity, like caches or message queues, while in the high-level design. We encourage you to note these areas with a simple verbal callout and written note, and then move on.\nAs you\u0026rsquo;re drawing your design, you should be talking through your thought process with your interviewer. Be explicit about how data flows through the system and what state (either in databases, caches, message queues, etc.) changes with each request, starting from API requests and ending with the response. When your request reaches your database or persistence layer, it\u0026rsquo;s a great time to start documenting the relevant columns/fields for each entity. You can do this directly next to your database visually. This helps keep it close to the relevant components and makes it easy to evolve as you iterate on your design. No need to worry too much about types here, your interviewer can infer and they\u0026rsquo;ll only slow you down.\nFocus on relevant\nDon\u0026rsquo;t waste your time documenting every column/field in your schema. For example, your interviewer knows that a User table has a name, email, and password hash so you don\u0026rsquo;t need to write these down. Instead, focus on the columns/fields that are particularly relevant to your design.\nFor our simple Twitter example, here is how you might build up your design, one endpoint at a time:\nDeep Dives (~10 minutes) Astute readers probably noticed that our simple, high-level design of Twitter is going to be woefully inefficient when it comes to fetching user\u0026rsquo;s feeds. No problem! That\u0026rsquo;s exactly the sort of thing you\u0026rsquo;ll iterate on in the deep dives section. Now that you have a high-level design in place you\u0026rsquo;re going to use the remaining 10 or so minutes of the interview to harden your design by (a) ensuring it meets all of your non-functional requirements (b) addressing edge cases (c) identifying and adressing issues and bottlenecks and (d) improving the design based on probes from your interviewer.\nBe Proactive\nThe degree in which you\u0026rsquo;re proactive in leading deep dives is a function of your seniority. More junior candidates can expect the interviewer to jump in here and point out places where the design could be improved. More senior candidates should be able to identify these places themselves and lead the discussion.\nSo for example, one of our non-functional requirements for Twitter was that our system needs to scale to \u0026gt;100M DAU. We could then lead a discussion oriented around horizontal scaling, the introduction of caches, and database sharding \u0026ndash; updating our design as we go. Another was that feeds need to be fetched with low latency. In the case of Twitter, this is actually the most interesting problem. We\u0026rsquo;d lead a discussion about fanout-on-read vs fanout-on-write and the use of caches.\nMistake: don\u0026rsquo;t talk very much\nA common mistake candidates make is that they try to talk over their interviewer here. There is a lot to talk about, sure, and for senior candidates being proactive is important, however, it\u0026rsquo;s a balance. Make sure you give your interviewer room to ask questions and probe your design. Chances are they have signal they want to get from you and you\u0026rsquo;re going to miss it if you\u0026rsquo;re too busy talking. Plus, you\u0026rsquo;ll hurt your evaluation on communication and collaboration.\n","date":"2025-02-22","id":27,"permalink":"/system-design/frameworks/framework_hi/","summary":"\u003ch1 id=\"delivery-framework\"\u003eDelivery Framework\u003c/h1\u003e\n\u003cp\u003eOur delivery framework is a sequence of steps and timings we recommend for your interview. By structuring your interview in this way, you\u0026rsquo;ll stay focused on the bits that are most important to your interviewer. An added benefit is that you\u0026rsquo;ll have a clear path to fall back if you\u0026rsquo;re overwhelmed. Many candidates are (understandably!) nervous in their interview. It\u0026rsquo;s easy to get lost if you aren\u0026rsquo;t building up a solution in a linear way.\u003c/p\u003e","tags":[],"title":"Framework Hi"},{"content":"Fundamentals of this framework This framework is predicated on the following fundamentals:\nWe propose an opinionated method that can be applied to reason over most system design interviews. The less you have to think about the process, the more you can focus on the problems at hand. Our method is broken down into 3 steps. Each of these steps can be seen as functions. They take inputs and have one output. As such, they can be practiced in isolation. We\u0026rsquo;re teaching you a process. This means you\u0026rsquo;ll be ahead of most people who might be very knowledgeable in a few topics but lack a process. You\u0026rsquo;ll come across as a systematic problem solver who\u0026rsquo;s able to decompose a problem and come up with a reasonable architecture in 30 minutes. Again, you only have to understand 20% of the concepts to address 80% of problems you\u0026rsquo;ll encounter. How to use this framework In order to use this framework effectively, each step needs to be practiced separately. The three steps are:\nRequirements Data types, access patterns, and scale Design Each step has its inputs and outputs at the start of the chapter. There are exercises at the end. Focus on understanding the process. The idea is that once you\u0026rsquo;ve learned the process, you can work on improving each of these separately.\nLimitations This is a prescriptive method. As such, the approach might not fit 100% of the system design questions that you encounter. We designed this method to help mid-level / senior engineers get started with system design interviews by using a systematic approach and developing a framework to tackle these questions effectively. You should take this and adapt it so it works best for you.\nWe intentionally do not tell you how much time to take in each step because we believe that would be an ineffective way to learn a new behavior. If you\u0026rsquo;re told that something takes 5 minutes, and as a beginner you lack 5 minutes of content, you\u0026rsquo;ll end up getting good at \u0026ldquo;wasting 5 minutes of time.\u0026rdquo; But if you focus on the task instead of the time, you\u0026rsquo;ll get good at doing the thing. This may seem counterintuitive, but it\u0026rsquo;s better for you in the long run. Thank us later.\nOverview of the 3 steps Don\u0026rsquo;t worry, we\u0026rsquo;ll go through each of these one-by-one in detail. And by the end of this chapter, we\u0026rsquo;ll run entire problems through the whole 3-step framework so you can understand how it works holistically. In the meantime, here\u0026rsquo;s a high-level overview of the 3 steps and their inputs and outputs so you can get a rough idea of the process.\nStep 1: Requirements Inputs Problem statement given by your interviewer. Outputs List of functional and non-functional requirements. Step 2: Data Types, API and Scale Inputs - Functional and non-functional requirements. - Problem statement given by your interviewer. Outputs - List of Data Types we need to store. - Access patterns for these data types. - Scale of the data and requests the system needs to serve. Step 3: Design Inputs - Functional and non-functional requirements. - Problem statement given by your interviewer. - List of Data Types we need to store. - Access patterns for these data types (API). - Scale of the data and requests the system needs to serve. Outputs - Data storage. - Microservices. 1.1 Functional Requirements You should start with the functional requirements first—that is, the core product features and use cases that the system needs to support.\nGo ahead and type out \u0026ldquo;Functional requirements\u0026rdquo; and start asking clarification questions to your interviewer. The number of questions you should ask will vary depending on how underspecified the given problem statement is. Your goal is to ask just enough questions to gather all use cases for the system.\nTreat the system as a black box. No thinking about design, implementation, or pretty much anything technical. The sole goal of this first step is to specify what needs to be built. Not how. Not the scale. Focus on the \u0026ldquo;what.\u0026rdquo;\nFor example, if you were asked to design Twitter, what clarification questions would you ask in order to scope out functional requirements?\nHere are a few steps to guide your requirement gathering:\nIdentify the main business objects and their relations Start by identifying the main business objects and their relations. For example, in the case of Twitter there are two main objects of interest: (1) Accounts and (2) Tweets.\nNow think about clarifying the relation between these objects. What\u0026rsquo;s the relation between accounts and tweets? What\u0026rsquo;s the relation between two accounts? What\u0026rsquo;s the relation between two tweets? You want to think about the cross product between objects to come up with ideas for use cases and ask relevant questions. For example:\nAccount Tweet Account Can follow Can publish Can like Can retweet Tweet Can reference (retweet) An account can follow other accounts (Account x Account) An account can publish a tweet (Account x Tweet) A tweet can reference another tweet, i.e., be a \u0026ldquo;retweet\u0026rdquo;. (Tweet x Tweet) Then dive deeper into each of the objects of interest. For example, what makes up a tweet? Can it contain media?\nConsider Media\nMedia is a common part of system design. Always ask yourself whether any of the business objects you identified can hold media.\nThink about the possible access patterns for these objects Access patterns are probably the single most influential part of design because they determine how data will be stored.\nLet\u0026rsquo;s think about the cross product of our objects again. This time we want to identify how data will be retrieved from the system.\nThe general shape of an access pattern requirement is:\nGiven [object A], get all related [object B] So, applying this idea to our Twitter example, we might end up with the following access patterns:\nGiven an account:\nGet all of its followers. (Account → Account) Get all the other accounts they follow. (Account → Account) Get all of its tweets. (Account → Tweet) Get a curated feed of tweets for accounts they follow. (Account → Tweet) Given a tweet:\nGet all accounts that liked it. (Tweet → Account) Get all accounts that retweeted it. (Tweet → Account) We\u0026rsquo;re not suggesting you blindly implement all of these, but rather that you consider them as possible access patterns for your clarification questions. For example, should we be able to get all accounts that liked a tweet? Or would the number be enough?\nAlso, bear in mind that when we say \u0026ldquo;get all,\u0026rdquo; we don\u0026rsquo;t necessarily mean that there will be a single endpoint that returns all of them in one go. It could be a paged endpoint, but that\u0026rsquo;s an implementation detail and falls out of scope for step 1. We want to focus on identifying the desired access patterns, not how they will be implemented.\nFor these access patterns, you should also consider ranking. Are there any access patterns that require ranking the object? In this example, \u0026ldquo;creating a curated feed of tweets\u0026rdquo; will require further clarification. Strive for simplicity first. Can you return them sorted by chronological time? Identify these access patterns of interest, like the curated feed, and get a feel for what your interviewer is looking for: do they want you to suggest an algorithm for a feed?\nConsider mutability Finally, as we do throughout this guide, you should always consider mutability. Can the objects the system holds be mutated? Or can they be assumed to be immutable?\nFor example: Can tweets be edited after they\u0026rsquo;re published?\nAnother flavor of mutability is deletion. Can these business objects be deleted? What would the consequences be?\nFor example: Can tweets be deleted? Can accounts be deleted? What happens to tweets when an account is deleted?\nIt might sound like a small detail at first, but mutability can limit our ability to use caching in our design (more on this in step 3).\nRemember: Functional Requirements Identify the main objects and their relations. What information do these objects hold? Are they mutable? Think about access patterns. \u0026ldquo;Given object X, return all related objects Y.\u0026rdquo; Consider the cross product of all related objects. List all the requirements you\u0026rsquo;ve identified and validate with your interviewer. Example 1: Design TikTok Tik_tok\nExample 2: Design a code deployment system aimed for developers at a company. They should be able to tag a release, and our system will package it and deploy it to some servers.\nLet\u0026rsquo;s start by identifying the main objects. In this case it looks like the only thing the service will need to store is code artifacts. What properties might we store? Feel free to discuss with your interviewer. For the sake of this example, let\u0026rsquo;s say we want to store:\nArtifact: (product name, version, commit hash)\nYou might also ask: \u0026ldquo;Can artifacts be mutated after being published?\u0026rdquo; Your interviewer might say the artifacts will be immutable (as in \u0026ldquo;not able to be mutated after publishing).\nWhat are the access patterns? In this case we just have one object, so it\u0026rsquo;s probably going to be pretty straightforward:}\nTrigger a release: publishes a code artifact and deploys it to all servers.\nThis is actually a pretty good example of how some problems might be less intensive on the requirements side than others. The definition the interviewer gave us was pretty clear. Therefore, we can extract requirements with just a few questions.\nExtra points\nSome interviewers might also be interested in evaluating your user-orientation. Can you think of how a developer might want to use this system beyond just publishing a release? #sd/tips\nFor example, you might want to ask about the possibility to recall a release. Do they want a page to see all releases and the deployment status? Should we alert them about failures? #sd/tips\nThis is what we mean when we say that it can be as easy or as hard as you want to make it. Try to get a feel for what the interviewer is looking for, and make sure to check back with them and ensure you both are on the same page.\n1.2 Non-Functional Requirements Once functional requirements have been laid out, you should move onto non-functional requirements (NFRs). These are quality attributes that specify how the system should perform a certain function.\nThe most common non-functional requirements you should consider in a system design interview are:\nPerformance Availability Security Note\nYou may be wondering why we chose \u0026ldquo;security\u0026rdquo; and not \u0026ldquo;consistency.\u0026rdquo; We think there are already enough resources out there that will tell you about consistency in NFRs. Again, this is our opinionated approach, and after much debate we decided this would be the most valuable way to teach it. As always in system design interviews: your interviewer cares less about your decisions, and more about whether you are able to talk about the trade-offs (positives and negatives) of your decisions. Therefore, if you wanted to, there are many NFRs that could make a shortlist.\nNFRs will strongly influence our design. They define what we should be optimizing for. Bear in mind that you cannot optimize for everything, and you should not overcomplicate your solution. This is a game of trade-offs.\nRule of thumb\nNon-functional requirements can feel like platitudes. Who doesn\u0026rsquo;t want every system they design to be redundant, scalable, available, consistent, and so on and so forth? But it\u0026rsquo;s a trap to say, \u0026ldquo;Non-functional requirements are always the same.\u0026rdquo;\nGood candidates can view non-functional requirements mainly as opportunities to relax one specific requirement, such as \u0026ldquo;We don\u0026rsquo;t need to focus on [Insert requirement, such as \u0026ldquo;consistency\u0026rdquo;] as much in this case because [Insert reason, such as \u0026ldquo;it\u0026rsquo;s okay in this scenario of TikTok if some users get access to certain videos later than the rest of our users\u0026rdquo;].\u0026rdquo;\nIf NFRs are over-specified, the solution may be too expensive to be practical; if they are under-specified, the system will not be suitable for its intended purpose. Use your common sense, and ask the right questions to land on a set of non-functional requirements that make sense for the system you are designing.\nLet\u0026rsquo;s explore each of the quality attributes that make up non-functional requirements and see some examples of when to optimize for them.\nPerformance Performance is pretty straightforward. It\u0026rsquo;s the system\u0026rsquo;s ability to respond quickly to user requests. While speed is always welcome, it might not be the right thing to optimize for in every system. Better performance may come at the cost of consistency or just an overall more complex solution.\nSo when does it make sense to optimize for performance?\nRule of thumb\nIt makes the most sense when we have synchronous user-facing workflows. That is, the user is expecting an immediate response from the system. In addition, we want to optimize for the synchronous workflows that are accessed the most frequently.\nTake a look at the access patterns you identified in your functional requirements. Which ones are user-facing, expected to happen synchronously, and accessed frequently? Let\u0026rsquo;s use our Twitter example: Is there any access pattern that we might want to optimize for performance?\nThe user feed feature stands out as a good candidate for a performant access pattern. It\u0026rsquo;s synchronous, meaning that users expect an immediate response, and it\u0026rsquo;s likely to be a hot path for users because it\u0026rsquo;s the landing page of the app.\nAvailability Availability refers to how much downtime the service can tolerate. Just like with performance, we might not always want to optimize for availability. A good question to guide this decision is: What\u0026rsquo;s the cost of downtime? This is as easy as it sounds. If taking downtime will result in financial losses or correctness issues, we might want to put some thought into making the system highly available.\nIn the case of Twitter, the need for high availability is pretty obvious. The system should ideally not take any downtime. We measure availability by the percentage of the time the system is up and running. A common goal is to aim for five nines, i.e., 99.999% availability—that\u0026rsquo;s less than 6 minutes of downtime a year.\nAnecdote from an interviewer\nDon\u0026rsquo;t make your interview harder than it has to be.\nIn an actual interview, if a candidate says, \u0026ldquo;Do we want 4 nines? 5 nines? 6 nines?\u0026rdquo; My first follow-up question will be, \u0026ldquo;What\u0026rsquo;s going to change in your system?\u0026rdquo; And they don\u0026rsquo;t know what to answer (obviously) because the system they\u0026rsquo;ll design won\u0026rsquo;t change at all whether it\u0026rsquo;s 4 nines or 5 nines or 6 nines.\nSo what I mean is that talking about availability is good, but talking about \u0026ldquo;we want 6 nines\u0026rdquo; in an actual interview can be a signal that this person is behaving like an imposter. This will cause the interviewer to ask harsher follow-up questions than if the candidate hadn\u0026rsquo;t said anything about the \u0026ldquo;number of nines\u0026rdquo; in the first place!\nAn example of a problem where we might be fine with taking a hit on availability is one where consistency is very important. Think, for example, about a banking system. One of the most important mandates of the system would be consistency. Operations need to be transactional. In this case, it might be acceptable if our system is unavailable/stale for small periods of time, as long as it is consistent. However, with Twitter, we\u0026rsquo;d rather have it be inconsistent than unavailable. When in doubt about what to prioritize, ask your interviewer whether consistency is preferred over availability.\nAsk yourself\nWhat\u0026rsquo;s an example of a system you can think of where high availability might not be essential?\nSecurity Security can be tricky. No one wants to design an insecure system, and there\u0026rsquo;s a baseline level of security that can be expected of any modern architecture (HTTPS, OAuth, password/data encryption). But that\u0026rsquo;s not what we are trying to figure out in this step.\nWe want to learn if there\u0026rsquo;s some workflow that might require a special design to account for security. For example, imagine you were designing LeetCode, an online judge for coding questions. One security constraint that would come to mind is that user-submitted code should be run in isolation. User submissions should run in some sort of sandbox where they get limited resources and are guaranteed not to affect or see other submissions.\nIsolation\nWhenever there is user-generated code execution involved (aka low trust code), running it in isolation should be a non-functional security requirement.\nRemember: Non-Functional Requirements Consider the three main non-functional requirements: performance, availability, and security.\nPerformance: Which access patterns, if any, require good performance? Availability: What\u0026rsquo;s the cost of downtime for this system? Security: Is there any workflow that requires special security considerations (e.g., code execution)? Example of Non-Functional Requirement gathering\nExample 2: Design a code deployment system aimed for developers at a company. They should be able to tag a release, and our system will package it and deploy it to some servers.\nPerformance: After asking your interviewer, you learn that code artifacts should be deployed within 1 hour after being published.\nAvailability: The system should be highly available as it would block the company from rolling out new releases. Some downtime here and there might be tolerated (especially if during weekends).\nSecurity: Since there\u0026rsquo;s code involved, it might be worth asking if the code is assumed to be trusted or not. In this case, since it\u0026rsquo;s a code deployment service for a company, let\u0026rsquo;s say that we can assume the code will be high-trust, so there are no special security constraints.\nStep 2: Data Types, API and Scale Inputs - Functional and non-functional requirements. - Problem statement given by your interviewer. Outputs - List of Data Types we need to store. - Access patterns for these data types. - Scale of the data and requests the system needs to serve. We\u0026rsquo;ve gathered functional and non-functional requirements. At this point we understand what the system is supposed to do as a black box. It\u0026rsquo;s now time to take our first steps toward designing it.\nHowever, you should not begin drawing boxes and discussing implementation right away. There\u0026rsquo;s a bit of pre-work needed before we can start thinking about a concrete design. We need to answer the following three questions:\nWhat data types does the system need to store? What does the API look like? What volume of requests do we need to support? These can be answered pretty quickly from your requirements. In fact, you can probably answer these in just a few minutes. Let\u0026rsquo;s walk through how we might answer each of these questions for our Twitter example:\n2.1 What data types does the system need to store? Think about the objects the system needs to hold and their data type. There are largely two types of data we might need to store:\nStructured data. Think business objects, like accounts, tweets, likes. Media and blobs. Think images, videos, or any type of large binary data such as TAR or ZIP files. For our Twitter example, we will store: Structured data\nAccounts Tweets Media Images or videos in Tweets 2.2 What does the API look like? Rule of thumb\nMore than 90% of the time, users will interact with the system through HTTPS, and as such we encourage you to think about the API in terms of HTTPS requests.\nIf you are curious about the rare cases where one might want to use a different protocol (like WebSockets), refer to future iterations of this guide (release dates TBD) where we will dive into these exceptions.\nBut even in these rare cases, it helps to start thinking about the API in terms of HTTPS requests.\nLook at the access patterns you defined in the functional requirements to write your API. For example:\n1 2 getTweets: 3 GET /{accountId}/tweets?nextPageToken={token} 4 returns: Paged list of tweets sorted by creation time desc. 5 6 getFeed: 7 GET /{accountId}/feed?nextPageToken={token} 8 returns: Paged list of tweets for the given users feed. 9 10 putTweet: 11 PUT /{accountId}/tweets 12 body: content of the tweet. 13 14 retweet: 15 POST /{accountId}/retweet 16 body: id of the tweet that is retweeted. 17\r2.3 What volume of requests do we need to support? Tell your interviewer\nIt seems like we\u0026rsquo;ve identified the main requirements, we have an API in place, and we know how the distribution of requests looks. If I were designing this system for real, I\u0026rsquo;d probably want to do some back-of-the-envelope math to estimate the number of requests and average volume of data we need to store. Do you want me to do the math or do you want me to skip it?\u0026quot;\nIf they agree, you should assign these requests some ballpark numbers in terms of writes/minute and reads/minute. It really does not matter at all if you are right or wrong. In fact, you\u0026rsquo;ll most likely be wrong. Believe me, your interviewer doesn\u0026rsquo;t care. We just want to agree on some numbers so we can do some back-of-the-envelope math.\nSee more in Envelope_estimations\nOutlaw Idea\nWe\u0026rsquo;ve seen online resources that spend so much time showing you how to calculate these numbers down to byte precision. For example: \u0026ldquo;Remember there are this many bytes in a GB, so if you have 7 GB then you have this many bytes…\u0026rdquo; But for 90% of problems… who cares? Go for some ballpark numbers, and make the math easy. You know you are probably wrong anyway, and it\u0026rsquo;s irrelevant as long as you are in the ballpark and have something to work with.\nUse some nice round numbers to make the math easy. In fact, exclusively use powers of ten to make your life even easier. How far off can you be from the closest power of ten? When in doubt, just guess higher—it\u0026rsquo;s called margin of safety. For our Twitter example we can go for these numbers:\nReads/minute: 100k Writes/minute: 1k\rFinally, what\u0026rsquo;s the volume of data we need to store? Go back to your \u0026ldquo;data types\u0026rdquo; (section 2.1) and think about how big these can get and multiply that by the number of writes/minute to get how much data you need to store per minute.\n1 2 Structured data (tweets, accounts): 100 KB each 3 Media (images, videos): 10MB each 4 5 Average size of a write: 1MB (just a guess!) 6\rAgain, it does not matter at all if you get these numbers right as long as you are in the ballpark. Please don\u0026rsquo;t spend too much time on this. Just use powers of ten, and when in doubt, pick the higher number. This makes our final math super easy:\n2 1k writes/minute with an average size of 1MB each = 1k * 1MB = 1000MB = 1GB/m\rSo we\u0026rsquo;ll need to store around a gigabyte of data per minute.\nData Types, Scale, and Access patterns\nOnce you know your requirements, it\u0026rsquo;s time to get specific.\nData Types: Start by identifying the main business objects that you need to store. API: How are these going to be accessed? Scale: Is the system read-heavy or write-heavy?\nExample. Code Deployment System. See more in Code_deployment\nStep 3: Design Inputs - Functional and non-functional requirements. - Problem statement given by your interviewer. - List of Data Types we need to store. - Access patterns for these data types (API). - Scale of the data and requests the system needs to serve. Outputs - Data storage. - Microservices. The time has come. We\u0026rsquo;ve got all the information we need to start drawing boxes and calling this a \u0026ldquo;system.\u0026rdquo; Yay!\nThere are several reasons that we spent considerable time in steps 1 and 2. Too often people dive straight into design and fail in spectacular ways. It\u0026rsquo;s easy to make that mistake—isn\u0026rsquo;t this interview called \u0026ldquo;system design\u0026rdquo; after all? No one told these candidates that good design is 70%+ requirements and planning.\nIn fact, we can go as far as saying that if you\u0026rsquo;ve executed the last two steps correctly, design should be pretty systematic. This is because system design questions are usually open ended and don\u0026rsquo;t have one single correct answer. Let\u0026rsquo;s use this to our advantage!\nOnce we know our use cases and what to optimize for, it comes down to knowing a few rules of thumb. Want speed? Use a cache. Want availability? Put in some redundancy. It\u0026rsquo;s really that simple. That\u0026rsquo;s the beauty of systems design. It can be as simple or as complicated as we want to make it.\nAt the risk of oversimplifying, we suggest that you start small. Just follow some rules of thumb depending on what you identified in steps 1 and 2. We can guarantee you that you\u0026rsquo;ll get a decent design. Then you can use the remaining time to iterate on it. Design is an iterative process.\nTell your interviewer\nI\u0026rsquo;m going to start drawing some boxes. I\u0026rsquo;m just thinking out loud for now, so don\u0026rsquo;t hold me to any of this. We can come back to it later.\nThis is basically giving you a free pass to flush your brain and be wrong. Which is exactly what you want when there\u0026rsquo;s a clean slate in front of you. Again, design is an iterative process. Expecting that you\u0026rsquo;ll go from clean slate to perfect design in one go is just… foolish.\nSo what is \u0026ldquo;Design\u0026rdquo;? We should first align on our outputs. Design simply means two components:\nData storage. We already know from previous steps \u0026ldquo;what\u0026rdquo; we are storing. Now the question is where are we storing it? Microservices. How do we store our data? How do we retrieve it to the API? Think of these as the middlemen between storage and the API. We know the what (steps 1 and 2), so now we focus on the where and the how. We will start with designing the data storage layer first and then think about the microservices that access this data.\nDesign Steps\rOn the far left we have our users and the API, and on the far right we have storage. Microservices are the connective tissue between these. As such, it pays off to think about them last. Otherwise, how will we know what we need to connect?\n3.1 Data storage Blob storage Let\u0026rsquo;s get some of the more obvious components out of the way first. Did you identify any type of media or blobs in step 2.1? If so, these are great candidates to store in blob storage. A blob (Binary Large Object) is basically just binary data. We store and retrieve these as a single item. For example, ZIP files or other binaries.\nSome popular blob stores are Amazon S3 and Azure Blob storage. In general, you don\u0026rsquo;t need to worry too much about the specific brand you\u0026rsquo;d be using. Just tell your interviewer that these images/blobs you identified are good candidates to store in some blob storage, and then draw a \u0026ldquo;blob\u0026rdquo; box.\nGoing back to our Twitter example, we\u0026rsquo;ll want to store media from tweets in some kind of blob storage.\nRule of thumb\nSay the generic name of the component, not the brand name. Unless you are very familiar with a specific brand (like S3), don\u0026rsquo;t say the specific brand. Instead, say \u0026ldquo;some kind of blob storage.\u0026rdquo; Because if you say, \u0026ldquo;we should use S3 here,\u0026rdquo; the next question out of your interviewer\u0026rsquo;s mouth will be, \u0026ldquo;why not Azure blob instead of S3?\u0026rdquo;\nThere\u0026rsquo;s a chance you might want to couple the blob storage with a CDN, but that\u0026rsquo;s something we\u0026rsquo;ll look into in step 3.2. This step is all about identifying how to store content, not how to distribute it.\nDatabase There are a few considerations for this step:\nRelational vs. Non-Relational Entities to store 3.1.1 Relational vs. Non-Relational Relational vs. Non-Relational, sometimes referred to as SQL vs. NoSQL, is one of the foundational decisions of database design. There are many trade-offs involved when it comes to picking one or the other. In many interview questions, an argument can be made for any choice you make. It\u0026rsquo;s important that you don\u0026rsquo;t succumb to paralysis through over-analysis. Just pick one, and make your rationale clear for why you chose it. Score extra brownie points if you include a drawback of making the pick you made.\nRemember\nThere\u0026rsquo;s no right or wrong answer—it\u0026rsquo;s all about how to justify your picks.\nDon\u0026rsquo;t oversell a solution. Every solution has positive and negative aspects and needs to be approached with a sense of realism. If you\u0026rsquo;re being unrealistic, you probably won\u0026rsquo;t change your mind (even when it benefits you to change your mind!). For example, sometimes the interviewer will give you an out by asking some follow-up questions, giving you a chance to see your own mistake and change your mind. But if you\u0026rsquo;re too fixated on being right, you\u0026rsquo;ll miss the opportunity.\nTherefore, we\u0026rsquo;re giving you two very powerful tools: (1) A rule of thumb to pick Relational (SQL) vs. Non-Relational (NoSQL), and (2) A list of trade-offs that you should mention to your interviewer after stating your decision.\nEntities to store It\u0026rsquo;s time to look at the data and access patterns we defined in step 2 and design our database schema. This will look like a list of tables/documents, and a description of the queries that you\u0026rsquo;ll use to access them.\nA good starting point is to sketch out a table for each entity you identified, and then go over the access patterns. Think about how they will be fulfilled. Then you can adapt your tables to better fit these access patterns. Repeat after me: design is an iterative process.\nThink about these iterations as small cycles where you identify the requirement, implement a solution, assess its limitation, and then improve it.\nRinse and repeat until you have a solution that accommodates your requirements from step 1. When you zoom out, you are just going through several of these cycles:\n(paste image with cyclic design)\nLet\u0026rsquo;s see what this might look like through the lens of our Twitter example.\nWe identified two entities in step 2.1: (1) Accounts, and (2) Tweets. Therefore, we\u0026rsquo;ll start out nice and simple with two tables:\nAccounts: id, name, surname. Tweets: id, content, author_id, media_url. Note that each of these has an id. This id is immutable and uniquely identifies each object so that we can easily change any of the object\u0026rsquo;s metadata without the need to update other records. Also, the media_url would point to the address of the blob storage bucket containing the tweet\u0026rsquo;s media, if any.\nNow look at the access patterns and make some adjustments. We identified two access patterns in step 2:\ngetTweets gets all the tweets for a given user. getFeed gets the feed for a given user. getTweets should be pretty straightforward given the tweets table. We\u0026rsquo;d just need to select all tweets with a given author_id. Databases usually support the concept of an index, which provides faster access to entities given a property (called the index). Indexing tweets by their author seems like a sensible choice to fulfill this access pattern.\nTell your interviewer\nBe mindful of any \u0026ldquo;get all\u0026rdquo; access patterns. These usually need to be guarded by paging. You don\u0026rsquo;t want a single endpoint returning the entire tweet history of an account. Depending on the account, that might be a very expensive query, and degrade user experience. Usually these will be behind logic that pages the response. That\u0026rsquo;s why Twitter will load pages of tweets, even if it seems like an \u0026ldquo;infinite scroll\u0026rdquo; in the UI.\nNow onto getFeed. Let\u0026rsquo;s define feed to be a list of tweets of all the accounts the given account follows, sorted chronologically. There\u0026rsquo;s one thing we are missing here already: the information about who follows whom. Let\u0026rsquo;s say we add that relation in some new table:\nFollowers: account_id, follower_id We can again have indexes to speed up certain access patterns, such as getting all followers for a given account_id. This is totally fine. You can add tables/indexes as you realize they are needed. Remember: Identify → Implement → Assess → Improve.\nComputing the feed for a given user would require us to get all the accounts they follow and then get all their recent tweets. We have an implementation—time to assess it. Is this solution acceptable for our requirements?\nGiven the fact that we identified the system to be read-heavy and getFeed is expected to be called quite frequently, the computation can become prohibitive. Consider that each account might follow thousands of users, and those might have hundreds of tweets. Computing the feed seems like quite a compute-intensive process.\nHowever, don\u0026rsquo;t fall into the common pitfall of prematurely optimizing your system. Your interviewer might not even care about this problem. After you assess the limitations of your solution, check back with your interviewer before continuing to improve the solution.\nTell your interviewer\n\u0026ldquo;Although this would work from a functional perspective, I\u0026rsquo;m afraid it might not fulfill our non-functional requirements. Concretely speaking, we\u0026rsquo;ve identified the system to be read-heavy, and this approach would be prone to a slow read performance. I assume we\u0026rsquo;d like to optimize it—what do you think?\u0026rdquo;\nAssess your current solution, provide your opinion, and then ask your interviewer for their thoughts. This is the best way to iterate on system design. You don\u0026rsquo;t want to rely solely on your interviewer without expressing your thoughts because it may convey a lack of criticality/independence.\nYou also don\u0026rsquo;t want to move forward without any input from your interviewer because it may be perceived as poor collaboration. We find that stating your rationale followed by a subtle \u0026ldquo;what do you think?\u0026rdquo; or \u0026ldquo;let me know if you think I\u0026rsquo;m approaching this the wrong way\u0026rdquo; is the perfect balance between being independent but also collaborative.\nFor the sake of learning, let\u0026rsquo;s say that our interviewer agrees with us and wants to move forward with optimizing this. Whenever you are looking to optimize runtime, trading it off with memory should be your first go-to.\nRule of thumb\nWhen looking to optimize performance for a read-heavy access pattern that requires several queries, consider storing the final result in a separate table and keeping it up to date.\nIn this example, we might want to store the user feeds in a table and keep that up to date as new tweets come up. That way, we have an instant mapping from user to its feed, making getFeed fast at the cost of using more memory and the added complexity of having to maintain feeds up to date.\nYou can also get creative as well. Do you need to keep the feeds for all users up to date? Maybe we can prioritize users who log in frequently. For users who rarely log into the app, we can compute the feed on-demand. These kinds of ideas are worth mentioning. They don\u0026rsquo;t really influence the design that much, but they show thoughtfulness around usage patterns.\n3.2 Microservices Once we have our storage layer somewhat defined, the last step is connecting our API to the storage layer. There are a few decisions that often arise at this stage:\nCaching Load balancing Queuing systems Caching Ask yourself: Are there any access patterns that would benefit from caching the results in-memory? Candidates sometimes add caching to their solution just because. This is often a mistake.\nNot Always Use Cache\nNot all systems designed in system design interviews require caching.\nRemember that every decision you make has some trade-off. There\u0026rsquo;s no such thing as a free lunch in system design. Therefore, we urge you to consider the downsides of your design decisions and mention them during the interview.\nRule of thumb\nConsider using caching when all three of these are true:\nComputing the result is costly Once computed, the result tends to not change very often (or at all) The objects we are caching are read often A common technology used when caching is needed is Redis. If you are not familiar with it, all you need to know is that it is a way for you to cache parts of your database in memory such that it\u0026rsquo;s faster to access them.\nRemember\nIf you haven\u0026rsquo;t used Redis, don\u0026rsquo;t say, \u0026ldquo;Let\u0026rsquo;s use Redis here\u0026rdquo; in the interview; instead, say \u0026ldquo;Let\u0026rsquo;s add a cache here.\u0026rdquo; Brand names are a riskier bet than generic names of components unless you have thorough experience with a specific brand, because the first follow-up question will likely be, \u0026ldquo;Why Redis and not Memcached?\u0026rdquo;\nWhat are some of the downsides of caching, you may ask? To begin with, it introduces two replicas of the same data. Even though our source of truth remains to be persistent storage (our database), it is now possible to get the result from the cache as well. This might introduce inconsistencies if the cache is out of date from the source of truth. This is why it\u0026rsquo;s wiser to cache objects that don\u0026rsquo;t usually change too often. It\u0026rsquo;s also costly to maintain and adds complexity to the system. Now every time we want to update our logic, we\u0026rsquo;ll need to consider the caching layer as well.\nLoad balancing Load balancing helps us scale horizontally and maintain high availability. While horizontal scaling is desired in most systems, it is again advisable to consider whether it is strictly necessary given the requirements you\u0026rsquo;ve identified.\nLoad balancing is easier when our API servers are stateless because requests can be routed to any node. In our Twitter example, we\u0026rsquo;d probably want to load balance incoming traffic into several replicas of our stateless API servers which will just hold the logic to access the database.\nAs mentioned, this will give us two key benefits:\nHorizontal scaling. We can add more API servers to handle more load. High Availability. Whenever we need to upgrade or restart our API servers, we can perform a rolling restart. This means that only one node would go down at a time, while others continue to serve requests. That\u0026rsquo;s how you normally are able to upgrade logic in these systems without taking downtime.\nThere are different strategies for deciding how to balance load across a set of servers, but most of the time you\u0026rsquo;ll be dealing with round robin.\nEnd-to-end example of the process Design interviewing.io: A website where people can schedule anonymous technical interviews with other engineers.\nStep 1: Requirements The following simulates a conversation you could have with your interviewer and a final result of the requirements you agree on. While the areas the interviewer wants to focus on might vary, this should give you a good overview of what to consider and how to ask relevant questions.\nLet\u0026rsquo;s start with business objects first.\nCandidate: I identify two business objects right off the bat: users and interviews. Let me start with interviews—do we need to save the video recording of the interview?\nInterviewer: Yes. There is also a showcase feature, where if both parties agree, the interview will be displayed in our \u0026ldquo;showcase\u0026rdquo; for others to see.\nCandidate: I see. Are there any other properties about interviews that I should be aware of?\nInterviewer: Well, generally, interviews are done in one programming language. We want to keep track of that so that users can then filter to only \u0026ldquo;Java\u0026rdquo; interviews, for example.\nCandidate: Got it. And what about matching interviewers and interviewees? How is that done?\nInterviewer: Users set up an availability where they are able to interview or be interviewed, and then they get matched with their counterpart.\nSo far it seems like we need to track information about:\nUser (name, surname, pseudonym, availability) Interview (interviewer, interviewee, video recording, programming language) Booking (time, interviewer, interviewee) Looks like we\u0026rsquo;ve got a good idea of the business objects involved in this system and a rough sense of the properties they\u0026rsquo;ll hold. Next, we should chat about different access patterns and agree on the functional requirements. Remember, you\u0026rsquo;ll want to think about the cross product of these entities to come up with ideas of possible access patterns that relate these objects.\nGiven a user, get all of the interviews they took part in. See showcased interviews. Set availability. Book interview. Join interview. Non-functional requirements Availability seems like the obvious thing to optimize for since the platform needs to be up for candidates to be able to interview.\nThere\u0026rsquo;s nothing notable in terms of performance here that warrants taking a note. Most likely, the thing we\u0026rsquo;ll care the most about will be audio quality during the interview. There\u0026rsquo;s also writing and running code during the interview.\nWe probably want to note good audio quality during the interview as a non-functional requirement. This could as well fall into availability. If the connection is flaky, that\u0026rsquo;s pretty much downtime.\nFinally, for code execution, we\u0026rsquo;d probably want to execute the code in isolation. Candidate submissions are low-trust and should have no side effects on the system. For example, you cannot DOS the system with code submissions.\nStep 2: Data Types, API and Scale Data Types 1 2 User: id, name, surname, pseduonym, availability. (Structured data) 3 Interview: id, interviewer_id, interviewee_id, video_recording_id, programming_lang, (Structured data) 4 Booking: id, interviewer_id, interviewee_id, time. (Structured data). 5 6 Recording: video (media). 7\rREST API 1 2 putAvailability: 3 POST /users/{userId}/availability 4 getInterviews: 5 GET /users/{userId}/interviews 6 returns: interview_id 7 getInterview: 8 GET /interviews/{interview_id} 9 returns: streaming interview 10 getShowcase: 11 GET /showcase 12 returns: list of interview ids and metadata. 13 bookInterview: 14 PUT /users/{userId}/bookings 15 returns: booking_id 16 getBookings: 17 GET /users/{userId}/bookings 18 returns: list of booking_ids 19 joinInterview: 20 POST /interview/{booking_id} 21 returns: link to coder pad. 22\rNo Code\nIn an interview, the less you code you write, the more you seem like a senior engineer. And the opposite is true as well: The more code you write in a system design interview, the more you seem like you\u0026rsquo;re below the senior level. Writing this much code for your API would probably be too much if you\u0026rsquo;re aiming for senior or senior plus roles. But if you\u0026rsquo;re a mid-level candidate trying to secure your mid-level position, this is the perfect amount of code to write.\nScale Candidate: How many interviews are we expecting?\nInterviewer: In the order of thousands per day.\nCandidate: How long does one interview last?\nInterviewer: About an hour.\nCandidate: What do we need to store as part of the recording?\nInterviewer: Just the audio and coder pad.\nBased on these items, if our interviewer wants us to do some back-of-the-envelope math, we could assume:\n100MB per interview x 1000 interviews per day = ~100GB of data per day\nStep 3: Design Let\u0026rsquo;s start with data— where shall we store it and how? If we go back to the data types, we\u0026rsquo;ve identified mainly two of them: interview recordings and metadata (users, interviews, bookings).\nAs for the interview recordings, since they are videos, a blob store would be a decent choice. We can have a table where each interview holds the link back to its recording. Furthermore, we can index them by things like programming language, allowing us to filter down by certain properties. One would imagine that any modern database should be capable of efficiently indexing the volume of records we are expecting (~1000 a day). In order to improve user experience, we would probably make the getShowcase endpoint a paged endpoint, and return paginated results for matching interviews.\nWhen considering which database to use to store metadata, it\u0026rsquo;s not immediately clear that any one technology would be a better fit. We\u0026rsquo;d probably go for a relational database like MySQL because schemas are quite well defined and entities are tightly related to each other. We\u0026rsquo;d also probably want to update bookings transactionally so we make sure we are not double booking interviewers. Having said that, an argument can be made for pretty much any modern database. We\u0026rsquo;d mention to the interviewer that our schemas might be a bit more rigid and we\u0026rsquo;ll likely have to prefer vertical scaling.\n","date":"2025-02-22","id":28,"permalink":"/system-design/frameworks/framework_iv/","summary":"\u003ch1 id=\"fundamentals-of-this-framework\"\u003eFundamentals of this framework\u003c/h1\u003e\n\u003cp\u003eThis framework is predicated on the following fundamentals:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWe propose an opinionated method that can be applied to reason over most system design interviews. The less you have to think about the process, the more you can focus on the problems at hand.\u003c/li\u003e\n\u003cli\u003eOur method is broken down into 3 steps. Each of these steps can be seen as functions. They take inputs and have one output. As such, they can be practiced in isolation.\u003c/li\u003e\n\u003cli\u003eWe\u0026rsquo;re teaching you a process. This means you\u0026rsquo;ll be ahead of most people who might be very knowledgeable in a few topics but lack a process. You\u0026rsquo;ll come across as a systematic problem solver who\u0026rsquo;s able to decompose a problem and come up with a reasonable architecture in 30 minutes. Again, you only have to understand 20% of the concepts to address 80% of problems you\u0026rsquo;ll encounter.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"how-to-use-this-framework\"\u003eHow to use this framework\u003c/h2\u003e\n\u003cp\u003eIn order to use this framework effectively, each step needs to be practiced separately. The three steps are:\u003c/p\u003e","tags":[],"title":"Framework Iv"},{"content":"","date":"2025-02-22","id":29,"permalink":"/system-design/frameworks/","summary":"","tags":[],"title":"Frameworks"},{"content":"Hasning strategies Rendezvous Hashing https://randorithms.com/2020/12/26/rendezvous-hashing.html Rendezvous hashing - rank server set.\nConsistent Hashing Consistent hashing is a way to effectively distribute the keys in any distributed storage system—cache, database, or otherwise—to a large number of nodes or servers while allowing us to add or remove nodes without incurring a large performance hit.\nsources https://akshatm.svbtle.com/consistent-hash-rings-theory-and-implementation interviewing.io\nxxx\nYou should use good hashing function.\nIn both of these stategies we have some form of consistency in mapping (for example between clients and servers).\n","date":"2025-02-22","id":30,"permalink":"/system-design/topics/hashing/","summary":"\u003ch1 id=\"hasning-strategies\"\u003eHasning strategies\u003c/h1\u003e\n\u003ch2 id=\"rendezvous-hashing\"\u003eRendezvous Hashing\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://randorithms.com/2020/12/26/rendezvous-hashing.html\"\u003ehttps://randorithms.com/2020/12/26/rendezvous-hashing.html\u003c/a\u003e\nRendezvous hashing - rank server set.\u003c/p\u003e\n\u003ch2 id=\"consistent-hashing\"\u003eConsistent Hashing\u003c/h2\u003e\n\u003cp\u003eConsistent hashing is a way to effectively distribute the keys in any distributed storage system—cache, database, or otherwise—to a large number of nodes or servers while allowing us to add or remove nodes without incurring a large performance hit.\u003c/p\u003e","tags":[],"title":"Hashing"},{"content":"The concept of a hypothesis can be used in many different context, frameworks, and processes. The hypothesis-driven approach helps ensure that decisions and innovations are based on evidence (and continuous learning?), [cons]-\u0026gt; making it valuable in any field that requires informed, data-backed decisions.\nHypothesis-driven design decisions The process focuses on starting with assumptions that can be tested, collecting data, and making informed iterations.\nHypothesis-driven design Identify a problem or opportunity. Clear problem statement Formulate a hypothesis. Plan an experiment to test the hypothesis. Collect Data and Feedback Analyze Results. Compare the results from the control and test groups against the hypothesis. Outcome Evaluation. The hypothesis is supported by data, indicating that the design decision had the intended (?) effect. The data does not support the hypothesis, suggesting that the change did not yield the expected outcome. Draw Conclusions and Iterate. Based on the analysis, decide the next steps: Refine the Design: Make adjustments based on insights. Pivot: If the hypothesis was disproved, consider alternative solutions. Implement Fully: If the hypothesis was validated, roll out the design more broadly. McKinsey Flow There is a repeating cycle of forming and testing hypotheses. McKinsey consultants follow three steps in this cycle:\nForm a hypothesis about the problem and determine the data needed to test the hypothesis Gather and analyze the necessary data, comparing the result to the hypothesis Update the model of the problem space and form a new hypothesis Consultants are careful to form very specific hypotheses. This enables them to quickly determine the data they need to test their hypotheses. It also allows them to only collect the data they need to test their hypotheses. Consultants do not gather data to fish for an answer, they seek to test their hypotheses.\nTesting a hypothesis. There are times when one needs a high degree of certainty in their answer. However, for most professionals in most situations, this is not the case. This creates a straightforward litmus test: Do you need a great deal of certainty in your answer? Or is a bias towards speed more valuable?\nlinks: https://www.linkedin.com/pulse/what-i-learned-mckinsey-part-3-how-hypothesis-driven-devin-kasper/ https://en.wikipedia.org/wiki/Hypothesis\nconcepts framer of a hypothesis data-backed decisions\nto_sort In common usage in the 21st century, a hypothesis refers to a provisional idea whose merit requires evaluation. For proper evaluation, the framer of a hypothesis needs to define specifics in operational terms. A hypothesis requires more work by the researcher in order to either confirm or disprove it.\nIn entrepreneurial setting, a hypothesis is used to formulate provisional ideas about the attributes of products or business models. The formulated hypothesis is then evaluated, where the hypothesis is proven to be either \u0026ldquo;true\u0026rdquo; or \u0026ldquo;false\u0026rdquo; through a verifiability- or falsifiability-oriented experiment. https://en.wikipedia.org/wiki/Hypothesis#cite_note-11 (https://en.wikipedia.org/wiki/Hypothesis#cite_note-12)\n","date":"2025-02-22","id":31,"permalink":"/projects/common/hypothesis/","summary":"\u003cp\u003eThe concept of a hypothesis can be used in many different context, frameworks, and processes. The \u003cem\u003ehypothesis-driven approach\u003c/em\u003e helps ensure that decisions and innovations are based on evidence (and continuous learning?), [cons]-\u0026gt; making it valuable in any field that requires informed, data-backed decisions.\u003c/p\u003e","tags":[],"title":"Hypothesis"},{"content":"you need to develop long-term vision. Why? 1. As a motivation.. 2. Identification.. 3. Organizational factor (define). \u0026hellip; This could be a non-useful approach, when you\u0026rsquo;ve decide what to use instead of solving problems and other more priority things.\nI need a range of various resources, including time, mental and physical energy, and intellectual capabilities.\npython, PyTorch Start putting these skills to use. #learning #learning/principle\ntodo rephrase and collect all the learning principles to one page You need to include different activities to your day. These kinds include: creative, code, linguistics, creating, project work, task solving, etc.\nGoals: ◦ S.M.A.R.T Goals (specific, measurable, attainable, realistic, and time-bound) ◦ measure and take outside feedback, socialize ◦ teach what you know\nPDSA also we have https://en.wikipedia.org/wiki/Decision_cycle - more common term A decision cycle or decision loop is a sequence of steps used by an entity on a repeated basis to reach and implement decisions and to learn from the results. The \u0026ldquo;decision cycle\u0026rdquo; phrase has a history of use to broadly categorize various methods of making decisions, going upstream to the need, downstream to the outcomes, and cycling around to connect the outcomes to the needs.\n","date":"2025-02-22","id":32,"permalink":"/projects/common/improvements/","summary":"\u003cp\u003eyou need to develop long-term vision. Why? 1. As a motivation.. 2. Identification.. 3. Organizational factor (define). \u0026hellip; This could be a non-useful approach, when you\u0026rsquo;ve decide what to use instead of solving problems and other more priority things.\u003c/p\u003e","tags":[],"title":"Improvements"},{"content":"Design common cloud client service as Instagram\ntodo next: see data sharding (Twitter, Grokking) see News Feed generation (Facebook, Grokking) see CDN and load balancers make blueprints of HLD\nsharding algorithm\ninstagram sharding: https://instagram-engineering.com/sharding-ids-at-instagram-1cf5a71e5a5c\ninstagram caching https://instagram-engineering.com/making-instagram-com-faster-part-3-cache-first-6f3f130b9669\ninstagram caching and promises: https://instagram-engineering.com/thundering-herds-promises-82191c8af57d\nmoving to multiple data centers: https://medium.com/instagram-engineering/instagration-pt-2-scaling-our-infrastructure-to-multiple-data-centers-5745cbad7834\nscaling instagram https://www.youtube.com/watch?v=hnpzNAPiC0E\nsome calculations: https://medium.com/interviewnoodle/instagram-system-architecture-fdbec22e48ee\narticle https://instagram-engineering.com/what-powers-instagram-hundreds-of-instances-dozens-of-technologies-adf2e22da2ad\narticle one: https://towardsdatascience.com/system-design-analysis-of-instagram-51cd25093971\nscaling instagram from founder https://www.youtube.com/watch?v=bLyv8zKa5DU some technical details https://www.youtube.com/watch?v=E708csv4XgY\nAll the web and async servers run in a distributed environment and are stateless. High-Level Design We need to support two scenarios at a high-level, one is to upload photos, and another is to view/search photos. Our system would need some object storage servers to store photos and some database servers to store metadata information.\nDefining the database schema is the first phase of understanding the data flow between different components of the system.\n","date":"2025-02-22","id":33,"permalink":"/system-design/projects/instagram/","summary":"\u003cp\u003eDesign common cloud client service as Instagram\u003c/p\u003e\n\u003cp\u003etodo next:\nsee data sharding (Twitter, Grokking)\nsee News Feed generation (Facebook, Grokking)\nsee CDN and load balancers\nmake blueprints of HLD\u003c/p\u003e","tags":[],"title":"Instagram"},{"content":"","date":"2025-02-22","id":34,"permalink":"/system-design/interview/","summary":"","tags":[],"title":"Interview"},{"content":"Checklist You should create a paper with checklist items to do during the interview, like main steps, inputs-outputs, what not to forget, etc.\nPreparation to some interview:\n(tool) Ask your recruiter what software you\u0026rsquo;ll be using for your interview and practice with it ahead of time. You don\u0026rsquo;t want to be fumbling with the software during your interview. Advices | Principles | Tricks (beginning) What it\u0026rsquo;s like to walk into a system design interview When beginning an interview, try to imagine what the interviewer is looking for. What are their goals for the session? How can you help them achieve those goals in a way that persuades them that you\u0026rsquo;ll be a strong hire?\nPut simply, the interviewer\u0026rsquo;s goal is to find enough data to hire you. Given the limited time available to them, an interviewer has to try to get enough positive signal about your ability so they can justify giving you a \u0026ldquo;hire\u0026rdquo; rating. In one hour you have to show your interviewer that you understand the fundamentals of a system (end to end). You also should be able to name and explain (at least at a high level) each part of the system, describe the tradeoffs you make, and find a solution.\nThe best way to accomplish this is to imagine that you\u0026rsquo;re explaining a design doc to a group of more junior engineers. They will ask you questions about your decisions and want to know what you\u0026rsquo;re trying to solve. Anticipating these questions and your responses will set you up for success in the interview.\n(your output format) The less code you write in a system design interview, the better. Structured. Detailed. \u0026hellip;\nProactivity Being proactive in a system design (SD) interview is essential to demonstrate leadership, structured thinking, and problem-solving skills. A proactive candidate does not wait for the interviewer to lead the discussion but instead takes charge, guiding the conversation effectively while remaining open to feedback.\n✔ Guide the Interview: Set a clear structure and lead the discussion.\n✔ Be Adaptable: Accept feedback and modify your approach.\n✔ Engage the Interviewer: Continually check in to align on priorities.\n✔ Explain Trade-offs: Justify decisions with pros and cons.\n✔ Communicate Clearly: Structure your thoughts and articulate them effectively.\nDefine a High-Level System Approach\nSet a Roadmap: Before diving into details, outline a step-by-step process: \u0026ldquo;I\u0026rsquo;ll start with high-level architecture, then discuss data storage, API design, scaling, and trade-offs. Does that sound good?\u0026rdquo; Propose an Initial Architecture: Start with a simple design that meets core requirements. Example: \u0026ldquo;I\u0026rsquo;ll begin with a monolithic service since the initial scale is low, but I\u0026rsquo;ll discuss how we can migrate to a microservices-based architecture as traffic grows.\u0026rdquo; Encourage Feedback: Keep the interviewer engaged by asking: \u0026ldquo;Does this approach align with what you had in mind?\u0026rdquo; \u0026ldquo;Would you like me to explore alternative solutions at this stage?\u0026rdquo; Asking High-Impact, Non-Trivial Questions (#todo)\nThere is not one single best solution Trade-Offs\nAlways consider trade-offs especially in SD interview\nIn system design, there isn\u0026rsquo;t a single \u0026ldquo;best\u0026rdquo; solution; instead, multiple viable approaches exist, each with its own set of trade-offs. This is because system design problems are often complex and multifaceted.\nFor engineers new to system design, this complexity can be daunting. A common pitfall is approaching design challenges as if they have one correct answer, similar to solving a well-defined engineering problem. However, system design requires a different mindset—one that embraces ambiguity and focuses on evaluating different options based on their trade-offs.\nTrade-off analysis is central to effective system design. It involves systematically comparing different design alternatives to understand their advantages and disadvantages in the context of specific requirements and constraints. By conducting a thorough trade-off analysis, engineers can make informed decisions that align with the project\u0026rsquo;s goals and constraints.\nCreation Lens Instead of finding (or \u0026ldquo;retrieving\u0026rdquo;) a solution, you are creating a solution. In this way, coding is akin to a science, while system design is more like an art.\n(Mental Way) Here\u0026rsquo;s another way to think about it. You aren\u0026rsquo;t solving a problem—you\u0026rsquo;re creating a map to help someone else find the solution. Instead of coloring inside some lines, you\u0026rsquo;ll need to draw the lines for someone else to color in. In a system design interview, there are no correct answers—though there are certainly incorrect ones—so there is nothing to solve. Instead, you\u0026rsquo;ll ask questions, make stuff, and explain how and why the stuff you made was reasonable.\n\u0026ldquo;Pretend it\u0026rsquo;s 1999, so a lot of the stuff we have access to today simply doesn\u0026rsquo;t exist. You and a group of your schoolmates are in your garage, hoping to make something. You\u0026rsquo;re the most senior one there. You will design it and your friends will code it up, and the thing is: the Minimum Viable Product has to be completed by tomorrow. So, there\u0026rsquo;s no time to prep and no need to worry about the intricacies of system architecture that you don\u0026rsquo;t know. Just answer this: How would you design this system so your friends could code it up today, right now? It doesn\u0026rsquo;t have to be pretty. It doesn\u0026rsquo;t have to be complicated. It doesn\u0026rsquo;t have to impress anyone. It just has to get done.\u0026rdquo;\nTo succeed in a system design interview, you want to collaborate with your interviewer, try crazy stuff, and try more crazy stuff until the design \u0026ldquo;feels right.\u0026rdquo;\nLean towards your strengths What if your measurement of success in an interview isn\u0026rsquo;t what you say, but instead it\u0026rsquo;s what you get the interviewer to say? Imagine you say something that engages the interviewer\u0026rsquo;s curiosity so much that they have no choice but to follow up with a \u0026ldquo;tell me more about that.\u0026rdquo; If the areas you get them to dig into the deepest, are things you\u0026rsquo;re good at: congratulations you are doing the Jedi mind trick.\nA candidate doing a system design interview will usually experience confusion. The interview format is so open-ended, you cannot know about everything they\u0026rsquo;ll ask you. However, there will be opportunities to strut your stuff. There will be moments when they ask you about something you know very well, and when this happens put your shoulders back and flex your muscles on this topic.\nTwo Types of Interviewers We need to act differently with cold and warm interviewers (take strategies from interviewing.io website).\nThere\u0026rsquo;s no right way to design a system By now you\u0026rsquo;ve heard (or read) that \u0026rsquo;there\u0026rsquo;s no right way to design a system,\u0026quot; and you might think it\u0026rsquo;s true. But how do you know for sure?\nWatch this video of two experts designing the same system side by side. By the time you\u0026rsquo;re done, you\u0026rsquo;ll have a practical example that proves \u0026ldquo;there\u0026rsquo;s no right way to design a system\u0026rdquo;. Pay attention, and you\u0026rsquo;ll notice how effective it is when you guide the interview toward your strengths and when you\u0026rsquo;re open about gaps in your understanding. The video is split into two parts.\nThis is one of the most important lessons!\nWe have \u0026ldquo;rules of thumb\u0026rdquo; scattered throughout this guide. In those cases, they apply directly to the material. But in this instance, these rules of thumb don\u0026rsquo;t fit anywhere, because they apply to, well, everything.\nInterviewer behavior As an interviewer, it\u0026rsquo;s hard to tell the difference between a bad candidate and a good candidate who is stuck.\nIf the interviewer interrupts you, it\u0026rsquo;s probably because you\u0026rsquo;re going off track.\nIf your interviewer interrupts you to suggest that you explore another avenue, then most likely you\u0026rsquo;re designing the system in contradiction to what the interviewer expects. In this case, let the interviewer explain what they expect, and then you should ask clarifying questions to ensure you understand the new direction before moving on.\nIt\u0026rsquo;s fine if the interviewer asks you questions, but it\u0026rsquo;s a bad sign if the interviewer starts telling you how to do things. This is a negative signal because the interviewer feels that you need help to move forward, and this will lower your score.\nPrior experience affects both sides In a system design interview, you may encounter two different situations:\nThe interviewer has read your resume and wants to see you demonstrate your experience in building something you\u0026rsquo;re familiar with. This should be easy because you can apply your knowledge from your current/previous position.\nThe interviewer has read your resume and decides to purposely challenge you by asking you to design something you have not worked on. In this case, don\u0026rsquo;t worry—just remember that \u0026ldquo;there is no right way to design a system.\u0026rdquo; Use your best judgment and industry knowledge to come up with something reasonable. Also, be honest about gaps in your knowledge and don\u0026rsquo;t be afraid to ask questions. Demonstrate that you are curious and willing to learn.\nWhen the interviewer decides to challenge you with something new, it may be a topic that is based on their own particular expertise or skill set.\nKnow an Interviever\nIf you know a little about your interviewer\u0026rsquo;s background, you should have a hint about what to expect, which can allow you to prepare a little ahead of time.\nTime management It\u0026rsquo;s more important to cover everything broadly than it is to explain every small thing in detail.\nBy the end of the interview, the interviewer is inherently asking themselves \u0026ldquo;Could this person get an MVP off the ground?\u0026rdquo; If the answer is \u0026ldquo;no\u0026rdquo;, then you\u0026rsquo;ve drastically reduced your chances of passing the interview.\nApproaching the problem Whatever decision you make, explain why. In a system design interview, why is more important than what. For anything you say, be prepared to explain why.\nReasoning your design\nYour interviewer cares less about whether your design is good in itself, and more about whether you are able to talk about the trade-offs (positives and negatives) of your decisions.\nKeep it simple.\nThe first rule of distributed systems is that you should avoid them if you don\u0026rsquo;t need them! Always consider maintenance costs. People don\u0026rsquo;t build distributed systems for fun. If all of Google could run on just one machine, you can bet they would do it.\nIn other words, if there is a simple way to do things and a complex way to do things, aim for the simple path. Not because the simple way is more likely to be correct, but because you have to make more assumptions for more complicated explanations to be true.\nAccept that there are some things that you will not know, and be ready to admit this to your interviewer. In the third core concept (below), we will teach you exactly how to say this without losing points in the interview.\nDesign is an iterative process Iterative process: Systems, in reality, improve over iterations. We often start with something simple, but when bottlenecks arise in one or more of the system\u0026rsquo;s parts, a new design becomes necessary. In some design problems, we make one design, identify bottlenecks, and improve on it.\nWorking under time constraints might not permit iterations on the design. However, we still recommend two iterations—first, where we do our best to come up with a design (that takes about 80 percent of our time), and a second iteration for improvements. Another choice is to change things as we figure out new insights. Inevitably, we discover new details as we spend more time working with a problem.\nStart with simple solution and evolve it during the interview to the MVP\nAt the risk of oversimplifying, we suggest that you start small. Just follow some rules of thumb depending on what you identified in steps 1 and 2. We can guarantee you that you\u0026rsquo;ll get a decent design. Then you can use the remaining time to iterate on it.\nYou can name it as First Pass, as well as Naive Design, or Baseline Solution\ncollect only main features components prioritization\nDrawing\nDraw only critical components.\nno premature optimization - follow the steps in this interview SD framework\nprioritization of the components to further process\nImmutability Trick First Going Immutable\nHere is one way to get unstuck during a system design interview: consider the immutable case. This is a practical way to dumb the problem down. Tackle the dumber problem, and then add in complexity after that. Considering the immutable case also helps with identifying bottlenecks, as well as with capacity planning.\nOther (invention point of view) \u0026ldquo;When you have a desired outcome (a truly portable laptop computer) but no clear solution in sight, that\u0026rsquo;s when you brainstorm, try crazy stuff, improvise, and keep ‘building your way forward\u0026rsquo; until you come up with something that works. You know it when you see it. A great design comes together in a way that can\u0026rsquo;t be solved with equations and spreadsheets and data analysis. It has a look and feel all of its own - a beautiful aesthetic that speaks to you.\u0026rdquo;\nBe prepared to adapt. Be flexible and ready to adjust your design based on feedback from the interviewer.\nTypes of SD Interviews Each company (and sometimes, each interviewer) will conduct a system design interview a little differently. Often, the differences are not important and you can prepare for all of them with the same material. But some interview types require different preparation.\nProduct Design Product design interviews (sometimes called \u0026ldquo;Product Architecture\u0026rdquo; interviews, or ambiguously \u0026ldquo;System Design\u0026rdquo; interviews) are the most common type of system design interview. In these interviews, you\u0026rsquo;ll be asked to design a system behind a product. For example, you might be asked to design the backend for a chat application, or the backend for a ride sharing application. Often these interviews are described in terms of a \u0026ldquo;use case\u0026rdquo; - for example, \u0026ldquo;design the backend for a chat application that supports 1:1 and group chats\u0026rdquo; and frequently referred to by the most noteworthy company that uses that use case. For example, \u0026ldquo;design the backend for a chat application like Slack\u0026rdquo;.\nInfrastructure Design Infrastructure design interviews are less common than product design interviews, but still relatively common. In these interviews, you\u0026rsquo;ll be asked to design a system that supports a particular infrastructure use case. For example, you might be asked to design a message broker or a rate limiter. Since these interviews are deeper in the stack, your interviewer will be looking for more emphasis on system-level mastery (e.g. consensus algorithms, durability considerations) than high-level design.\nThis guide will be useful for Infrastructure Design interviews, with a stronger emphasis on the Concepts section.\nObject Oriented Design Object oriented design (sometimes called \u0026ldquo;Low Level Design\u0026rdquo;) interviews are less common than product design interviews, but still occur at particularly at companies that use an object-oriented language like Java (Amazon is notable for these interviews). In these interviews, you\u0026rsquo;ll be asked to design a system that supports a particular use-case, but the emphasis on the interview is assembling the correct class structure, adhering to SOLID principles, coming up with a sensible entity design, etc. For example, you might be asked to design a Parking Lot reservation system or a Vending Machine, but rather than breaking this problem down into services and describing the backend database you\u0026rsquo;re instead asked to describe the class structure of a solution.\nThis guide is not as useful for an Object Oriented Design interview. We instead recommend (until we get to it!) Grokking the Low Level Design Interview.\nInterview Assessment At the senior level, system design interviews are common.\nThe difference in levelling is most frequently the depth of the solution and your knowledge. While all candidates are expected to complete a full design satisfying the requirements, a mid-level engineer might only do this with 80% breadth and 20% depth, while a senior engineer might do this with 60% breadth and 40% depth.\nRemember that the top-level goal for your interview is to give your interviewer sufficient confidence to advocate for a hire decision. While the mechanics of your interview are important, they are ultimately in service of signaling to your interviewer that you are a strong candidate.\nRemember\nThe most common reason for a candidate to fail a system design interview is not delivering a working system. This is often due to a lack of structure in their approach. We recommend following the structure outlined in the Delivery section.\nProblem Navigation Your interviewer is looking to assess your ability to navigate a complex problem. This means that you should be able to break down the problem into smaller, more manageable pieces, prioritize the most important ones, and then navigate through those pieces to a solution. This is often the most important part of the interview, and the part that most candidates (especially those new to system design) struggle with.\nThe most common ways that candidates fail with this competency are:\nInsufficiently exploring the problem and gathering requirements. Focusing on uninteresting/trivial aspects of the problem vs the most important ones. Getting stuck on a particular piece of the problem and not being able to move forward. High-Level Design With a problem broken down, your interviewer wants to see how you can solve each of the constituent pieces. This is where your knowledge of the Core Concepts comes into play. You should be able to describe how you would solve each piece of the problem, and how those pieces fit together into a cohesive whole.\nThe most common ways that candidates fail with this competency are:\nNot having a strong enough understanding of the core concepts to solve the problem. Ignoring scaling and performance considerations. \u0026ldquo;Spaghetti design\u0026rdquo; - a solution that is not well-structured and difficult to understand. Technical Excellence To be able to design a great system, you\u0026rsquo;ll need to know about best practices, current technologies, and how to apply them. This is where your knowledge of the Key Technologies is important. You should be able to describe how you would use current technologies, with well-recognized patterns, to solve the problems.\nThe most common ways that candidates fail with this competency are:\nNot knowing about available technologies. Not knowing how to apply those technologies to the problem at hand. Not recognizing common patterns and best practices. Communication and Collaboration Technical interviews are also a way to get to know what it would be like to work with you as a colleague. Interviews are frequently collaborative, and your interviewer will be looking to see how you work with them to solve the problem. This will include your ability to communicate complex concepts, respond to feedback and questions, and in some cases work together with the interviewer to solve the problem.\nThe most common ways that candidates fail with this competency are:\nNot being able to communicate complex concepts clearly. Being defensive or argumentative when receiving feedback. Getting lost in the weeds and not being able to work with the interviewer to solve the problem. (interviewing.io point of view) With that said, sometimes you\u0026rsquo;ll have an interviewer who is cold or not very collaborative. Dealing with these interviewers requires practice. The more senior you become, the more important it is to learn how to adjust your communication style to match your audience. We recommend completing mock interviews with a variety of interviewers to help you become a seasoned, fearless veteran of system design interviews.\nInterview Assessment (Other) What your interviewer looks for, and what they don\u0026rsquo;t\nWith this basic model in mind, let\u0026rsquo;s consider the main elements that system design interviewers look for, and the elements that don\u0026rsquo;t matter.\nWhat your interviewer wants to see\na broad, base-level understanding of system design fundamentals. back-and-forth about problem constraints and parameters. well-reasoned, qualified decisions based on engineering trade-offs. the unique direction your experience and decisions take them. a holistic view of a system and its users. What your interviewer is not looking for\ndeep expertise in the given problem domain. assumptions about the prompt. specific answers with ironclad certainty. a predefined path from the beginning to end of the problem. strictly technical considerations. Understanding Fundamentals You do not need to display deep expertise in the given problem domain. Interviewers want to see that you have a broad, base-level understanding of system design fundamentals.\nYour interviewer will expect you to have knowledge of a wide range of basic topics, but they won\u0026rsquo;t expect you to be an expert in any of them. For instance, you should understand the difference between SQL and NoSQL databases, their broad performance characteristics, and the types of applications each might be useful for (which we\u0026rsquo;ll teach you later in this guide). But you would not need to know how the internals of either type of database work at any kind of detailed level.\nIn spite of this, you still might be asked to design those internals! Keep in mind, though, that your answer doesn\u0026rsquo;t need to be optimal or reflect real-world implementations. For example, if an interviewer asks you to design a database/SQL query engine, they\u0026rsquo;re not trying to discern if you\u0026rsquo;re familiar with the academic literature on query engines or discover how much time you\u0026rsquo;ve spent working on database internals.\nInstead, they want to see how you would approach the problem based on what you do know, starting from first principles and collaborating with them. Your answer will probably not be anywhere near optimal, and that\u0026rsquo;s OK! The interviewer will focus on the process, not the result.\nProblem Navigation Interviewers want to engage you in a back-and-forth conversation about problem constraints and parameters, so avoid making assumptions about the prompt. Initial prompts to system design problems tend to be intentionally light on detail. Many candidates make a mistake by extrapolating details from the initial prompt and crafting a solution based on those assumptions.\nFor example, imagine that the interviewer instructs you to design a \u0026ldquo;photo sharing service\u0026rdquo; with some minimally defined capabilities. This may cause some candidates to imagine that they\u0026rsquo;re rebuilding Instagram and start designing around the assumption that all images will be relatively small, not examined closely, and that extensive compression to save storage and bandwidth is acceptable.\nBut the interviewer didn\u0026rsquo;t tell you to rebuild Instagram, so you\u0026rsquo;ll need to keep in mind that there are many different types of photo sharing services. The interviewer may have had in mind something like Imgur or Photobucket, sites that cater more to basic image hosting for the web. Or they could be thinking about something like Flickr or 500px, services built for photographers to show off their work in high resolution.\nSo how do you figure out what type of service the interviewer wants you to build? Ask them! A basic prompt leaves room for you to start a conversation with your interviewer about the system you\u0026rsquo;re designing—what type of users does it serve, what type of traffic can it expect, what limits will it have? Demonstrating that you can think critically about the parameters of your service is the first step in any system design interview.\nIronclad Certainty Interviewers are not looking for specific answers with ironclad certainty. They want to see well-reasoned, qualified decisions based on engineering trade-offs.\nBe very careful any time you find yourself responding immediately to a prompt in a system design interview. Even aspects of your design that seem insignificant need at least cursory consideration. Let\u0026rsquo;s use IDs as an example.\nA candidate will often start a discussion of a data model with a statement like, \u0026ldquo;I\u0026rsquo;ll use auto incrementing IDs,\u0026rdquo; or \u0026ldquo;I\u0026rsquo;ll use GUID here\u0026rdquo; as kind of a default approach to assigning IDs to data. In many applications, however, the type of ID you assign to your data has practical consequences.\nIs this ID going to be exposed to users? If so, how long does it need to be to avoid collisions? If we auto-increment it, are we worried about the visibility that will give third parties into our traffic patterns or the possibilities of users guessing the IDs to each others\u0026rsquo; data? If it\u0026rsquo;s intended to be shared, is it convenient to type? If you print it on a business card or a flier, does it contain characters that you could confuse for each other (e.g., \u0026ldquo;1\u0026rdquo; and \u0026ldquo;I\u0026rdquo;, \u0026ldquo;0\u0026rdquo; and \u0026ldquo;O\u0026rdquo;)?\nYou don\u0026rsquo;t need to hold an inquiry for every minor detail, but always be sure to give some justification for the decisions you make and let your interviewer know how your decisions would change in different circumstances. System design problems don\u0026rsquo;t have a single definitive answer, so interviewers just want to see that you can justify your answers.\nChoose your Own Adventure Interviewers are not looking for a predefined path from the beginning to end of the problem. They want to see the unique direction your experience and decisions take them. Coding problems usually have an expected path. Typically you\u0026rsquo;ll begin with an obvious but inefficient solution, and then the interviewer will prompt you for a series of improvements. Those improvements lead you to increasingly efficient solutions until you finally arrive at the optimal implementation.\nSystem design problems, on the other hand, resemble a Choose Your Own Adventure book rather than a linear novel. A complex system contains a multitude of sub-components, each one of which could serve as a design problem on its own. After you\u0026rsquo;ve sketched the overall layout of your system, an interviewer may decide to keep your focus on the big picture or dive into a deeper examination of one particular component.\nThe path your interview takes will be steered by your interviewer, but they\u0026rsquo;re likely to take cues from the sub-problems in which you display interest or aptitude. In some cases they may explicitly ask you which part of the problem you\u0026rsquo;d prefer to focus on.\nEven if you\u0026rsquo;re not choosing directly, you can still influence an interview\u0026rsquo;s direction. As you talk your way through a solution, it\u0026rsquo;s OK to specifically note the parts that you have experience in and explain when you\u0026rsquo;re making educated guesses. Your interviewer won\u0026rsquo;t expect you to know everything, but giving them a better idea of what you do know will help them steer the interview in ways that reveal your strengths and problem-solving ability.\nHolistic View Interviewers seek a holistic view of a system and its users.\nWhen faced with a choice in a design interview, it\u0026rsquo;s easy to focus on the technical details, but remember that computer systems serve human users, so you\u0026rsquo;ll want to anchor your technical decisions to the user experience they enable.\nSuppose, for instance, that the image sharing service you\u0026rsquo;re designing will require users to log in before uploading an image. In technical terms, you might want to avoid login to keep the database schema simpler, or you could introduce login to gather better metrics. An anonymous experience may be best for a public image-hosting site intended for quick turnaround and low interaction, while a logged-in experience offers the possibility of community features like commenting and sharing, personalized metrics, and the ability to restrict an upload to authorized viewers. You may want to take either approach or even both, allowing a limited anonymous experience with extra features for logged-in users.\nUser Experience\nThe important thing is to discuss the possible approaches and their consequences for the user experience with your interviewer before making a decision. You can never go wrong by making the end user the driving force in your design.\nFlags Think of red and green flags as signposts you can use to orient yourself in the interview. Green flags indicate that things are going well, that you\u0026rsquo;re engaging with the interviewer and making a positive impression. Red flags warn you that you may be going astray and should try to get the interview back on track.\nRed Flag #1 You believe that to pass a system design interview, you should just \u0026ldquo;play the game, keep talking, and make sure nobody explodes.\u0026rdquo;\nFollowing this quote\u0026rsquo;s advice has steered many interviewees in the wrong direction. There is no game, and talking for the sake of talking is one way to hang yourself with the rope the interviewer gives you. Also, if the goal is to not explode, well, you\u0026rsquo;re wasting your and your interviewer\u0026rsquo;s time.\nGreen Flag #1 You communicate honestly about what you know and what you don\u0026rsquo;t As we mentioned earlier, this guide will teach you the basic information that you\u0026rsquo;ll be asked about in 80% of system design interviews. Although these are great odds, you still may encounter a scenario that\u0026rsquo;s beyond your level of understanding. If this happens to you, don\u0026rsquo;t worry! Just engage in an honest dialogue with your interviewer, explaining when you lack certain knowledge or have gaps in your understanding. When you do have a sense of how to proceed, but you\u0026rsquo;re uncertain, you should communicate from first principles. Later in this guide, we will explain how to overcome that uncertainty and still score points with your interviewer.\nRed Flag #2: You find yourself pushing against interviewer feedback Keep in mind that your interviewers use the same problems over and over again, and they frequently see candidates make the same mistakes. If they try to divert you from a course of action, it\u0026rsquo;s likely because they\u0026rsquo;ve seen others flounder when using the same approach. You may be the one candidate in a hundred who finds a unique and better solution—we\u0026rsquo;ve had this happen before!—but carefully consider the odds before proceeding with a solution against the interviewer\u0026rsquo;s advice.\nWith that said, there is an art to pushing back against your interviewer when the situation calls for it, and later in this guide we\u0026rsquo;ll teach you how and when to employ this strategy .\nGreen Flag #2: The interview feels like a collaboration between you and the interviewer When the interviewer offers feedback, you integrate it into your design. You ask probing questions and receive useful answers about the system you\u0026rsquo;re designing, its users, and its traffic. Try to establish a tone as if you were working through a problem with a coworker rather than proving yourself to an interviewer. In the real world, when you\u0026rsquo;re assigned a project, you\u0026rsquo;ll have to ask a variety of people several questions to ensure that you fully understand the problem before making decisions. That\u0026rsquo;s what interviewers want to see.\nRed Flag #3: You skip over questions and ignore interviewer prompts, trying to move the interview ahead without addressing their concerns It\u0026rsquo;s OK to not know things—no one will have every answer—but it\u0026rsquo;s better to admit that to your interviewer than to avoid the questions altogether. Your interviewer may be able to offer you a hint or help you reason about alternatives if they know you\u0026rsquo;re struggling, but if you skip right ahead you\u0026rsquo;ll miss the opportunity to provide them with any positive signal from that portion.\nGreen Flag #3: Your role determines who should drive the focus and pace of the interview If you\u0026rsquo;re looking for a mid-level position or below, your interviewer should determine the direction and speed of the interview. Given an initial overview of your design, they may ask you for clarification on some aspects of it. They may ask you to produce a more detailed design for one or more components. And they may also change the requirements and ask how you could adapt your solution to accommodate this new view of the world. Wherever they take the interview, follow along and focus on the areas they direct you to.\nIf you\u0026rsquo;re applying for a senior role (or above), it\u0026rsquo;s a good sign if you direct more of the interview. In junior system design interviews, the interviewer expects to drive the interview, but as you reach senior levels the expectation shifts to the interviewee.\nAnecdote from a seasoned interviewer\n​​Being overly confident and talking too much might count against a mid-level candidate. Some interviewers (especially off-script ones) love giving candidates more rope to hang themselves with, and then they ask specific questions that focus on what the candidate struggles with.\nIf your goal is to maximize a mid-level offer, not improve your \u0026ldquo;average passing rate\u0026rdquo; (i.e., if you are comfortable sacrificing some senior-plus chances to increase your mid-level chances), then you might be better off consciously \u0026ldquo;giving control away\u0026rdquo; to your interviewer.\nSimply put, at the above-senior level an awkward pause will be held against you—that\u0026rsquo;s basically guaranteed. But at mid-level, most of your attempts to fill in an awkward pause may hurt you more than keeping silent.\nAnother way to think of it: when you are not leading the conversation, you signal that you\u0026rsquo;re not really far above mid-level. (But if you are comfortable at mid-level, this is not a downside!)\nThe saying, ‘Better to remain silent and be thought a fool than to speak out and remove all doubt\u0026rsquo; can be true for mid-level interviews but not for seniors or above-senior.\u0026quot;\nRed Flag #4: You leave long stretches (several minutes) of silence multiple times throughout the interview If you\u0026rsquo;re struggling to provide an answer, give yourself a little bit of time to come up with something. If you\u0026rsquo;re truly stuck, however, you should ask your interviewer for help. They can\u0026rsquo;t tell that you\u0026rsquo;re at an impasse unless you tell them, and you may waste valuable interview time while they debate whether it\u0026rsquo;s been long enough to interrupt you.\nGreen Flag #4: You take time to collect your thoughts and refine solutions before offering them up out loud/on the board An interview doesn\u0026rsquo;t need to be a continuous stream of consciousness, and it never hurts to sanity check your ideas before verbalizing them.\nMake a decision A common failure point occurs when candidates don\u0026rsquo;t make decisions\nOften, candidates will say things like: \u0026ldquo;we could use this type of DB, or this other, or that other, and these are some pros and cons…\u0026rdquo; and then they move on to another component. It\u0026rsquo;s a good practice to talk about benefits and tradeoffs, but then you have to make a decision. In the real world you have to make decisions—the same thing applies to the interview. If the interviewer challenges you with some questions, it\u0026rsquo;s totally fine to change your mind and alter the component (if you think there are better choices).\nDon\u0026rsquo;t say. We could use this type of DB, or this other, or that other, and these are some pros and cons… Do say. \u0026ldquo;We could use this type of DB, or this other, or that other, and these are some pros and cons… And based on all these tradeoffs, I\u0026rsquo;ll use THAT type of DB.\u0026rdquo;\nBrand Names Interviewers want to identify \u0026ldquo;impostors\u0026rdquo;: people who just learned a few words and try to pass the interview.\nDon\u0026rsquo;t say things because you think you\u0026rsquo;re supposed to say them. This often occurs when candidates name specific brands of technologies (e.g., \u0026ldquo;Kafka\u0026rdquo; or \u0026ldquo;Cassandra\u0026rdquo;). Not being familiar with specific databases or other components is fine. Be smart and don\u0026rsquo;t say brand names just for the sake of saying them.\nDon\u0026rsquo;t say\nI\u0026rsquo;m going to use Cassandra\u0026hellip;\u0026quot; unless you are VERY familiar with that, because the next question will be: \u0026ldquo;Why Cassandra and not some_other_db?\nDo say\nI\u0026rsquo;m going to use a NoSQL db because of [insert brief rationale].\nDon\u0026rsquo;t say\nI will use Kafka…\u0026quot; unless you\u0026rsquo;re prepared to explain how Kafka works. Don\u0026rsquo;t say \u0026ldquo;I will use Kafka\u0026rdquo; unless you are prepared to talk about other types of queues, because they may ask you: \u0026ldquo;Oh, Kafka, interesting choice. Why that instead of [some other queue]?\nDo say\nI will use a queue because of [insert brief rationale].\nRemember\nSay the generic name of the component, not the brand name unless you are very familiar with it. Don\u0026rsquo;t say Kafka. Instead, say \u0026ldquo;a queue\u0026rdquo;.\nTechnical (kind of) Flags Red: Overengineering A good interviewer also looks for red flags. Over-engineering is a real disease of many engineers as they delight in design purity and ignore tradeoffs. They are often unaware of the compounding costs of over-engineered systems, and many companies pay a high price for that ignorance. You certainly do not want to demonstrate this tendency in a system design interview. Other red flags include narrow mindedness, stubbornness, etc.\ntodo and other engineering / architect practices and methods Over-engineering or making rigid design choices.\nModel blueprint evaluation\nCandidate Evaludation Code, Solve, Communicate\nintroduce real-world constraints, UX practices, working backward, time consideration (change of FR, NFR after time), user expectations, system dynamic (what can we do in future if there will be more users, changes in traffic, geopositions) product approach\nhow can we be sure that this system does work? system evaluation\nThe System\u0026rsquo;s Users (End-Users) Role: The consumers of the system who interact with the product.\n🔹 Key Considerations:\nWho are the primary users? (e.g., general consumers, enterprise clients, developers via APIs) What are their key expectations? (low latency, reliability, security, ease of use) How does user behavior impact system design? (e.g., peak traffic patterns, read vs. write ratios) back-of-the-envelope estimations You should check with your interviewer to see if they want to see you do some math or if they\u0026rsquo;d rather go into design.\nTell your interviewer:\nIt seems like we\u0026rsquo;ve identified the main requirements, we have an API in place, and we know how the distribution of requests looks. If I were designing this system for real, I\u0026rsquo;d probably want to do some back-of-the-envelope math to estimate the number of requests and average volume of data we need to store. Do you want me to do the math or do you want me to skip it?\nIf they agree, you should assign these requests some ballpark numbers in terms of writes/minute and reads/minute. It really does not matter at all if you are right or wrong. In fact, you\u0026rsquo;ll most likely be wrong. Believe me, your interviewer doesn\u0026rsquo;t care. We just want to agree on some numbers so we can do some back-of-the-envelope math.\nlink to envelope estimations\n","date":"2025-02-22","id":35,"permalink":"/system-design/interview/interview/","summary":"\u003ch1 id=\"checklist\"\u003eChecklist\u003c/h1\u003e\n\u003cp\u003eYou should create a paper with checklist items to do during the interview, like main steps, inputs-outputs, what not to forget, etc.\u003c/p\u003e","tags":[],"title":"Interview"},{"content":"How to push back against your interviewer in a helpful way Example of a candidate pushing back:\nInterviewer: \u0026ldquo;I wouldn\u0026rsquo;t use a cache here in my opinion\u0026rdquo; Interviewee: \u0026ldquo;Sure we can take out the cache, though the reason I think a cache might be useful here is [insert technical reasoning here]. Do you think there might be a better way we can approach this?\u0026rdquo;\nChoose your words carefully Acknowledge and affirm your interviewer. Start with \u0026ldquo;sure,\u0026rdquo; \u0026ldquo;OK,\u0026rdquo; or \u0026ldquo;yes,\u0026rdquo; so they are open to what you\u0026rsquo;re saying. This results in them being less likely to push back against your pushback. The best language to cause your counterpart to let their guard down is \u0026ldquo;I\u0026rsquo;m sorry\u0026rdquo;. If you want to push back, and you don\u0026rsquo;t want the interviewer to challenge it, start your push back with \u0026ldquo;Ok. I\u0026rsquo;m sorry, but if we take out the cache won\u0026rsquo;t that result in [insert technical reasoning here]…\u0026rdquo;\nRemember that you are colleagues on the same team. The best way to demonstrate this is to use collaborative language. This is the easiest way to build empathy and score micro points to get them on your side. The best language to demonstrate collaboration is \u0026ldquo;We\u0026rdquo; or \u0026ldquo;Let\u0026rsquo;s.\u0026rdquo; For example: \u0026ldquo;We could take out the cache. However, if we did that one drawback would be [insert technical reasoning here]…\u0026rdquo;\nHandwave stuff for the sake of time No one can design a real-world system in 40-60 minutes. You can only design partial non-usable systems in that amount of time. As a result, you won\u0026rsquo;t be able to cover everything in depth.\nRemember\nIt\u0026rsquo;s more important to cover everything broadly than it is to explain every small thing in detail.\nNote\nIt is common for interviewers to ask a candidate to \u0026ldquo;design Gmail.\u0026rdquo; There are so many different dimensions to the product we know as \u0026ldquo;Gmail,\u0026rdquo; so no candidate can actually design Gmail in the amount of time they have in an interview. Whenever people tell you to ‘design gmail\u0026rsquo; that is to scare you.\u0026quot;\n[!note] Handwaving stuff is a smart time-management choice. It\u0026rsquo;s also a tactic to avoid getting derailed.\nWhen we say \u0026ldquo;handwave stuff,\u0026rdquo; this means that you can say, \u0026ldquo;I\u0026rsquo;m going to skip going into [detailed thing] for now, but if we want, we can come back to it later.\u0026rdquo; If you dive deeply into the details of every single thing, you\u0026rsquo;ll fall down too many rabbit holes to be productive. But if you notice and address this so your interviewer understands, by saying something like, \u0026ldquo;Hey, here is a rabbit hole we could go down, but let\u0026rsquo;s skip it for now,\u0026rdquo; you\u0026rsquo;re killing a few birds with one stone. You\u0026rsquo;re demonstrating your knowledge by calling it out, and simultaneously you\u0026rsquo;re keeping the interview on track, because in an open-ended situation like this, it\u0026rsquo;s just as important to know where you\u0026rsquo;re not going as it is to know where you are going.\nExample of a smart way to handwave stuff (taken from one of our mock interviews)\nBe Proactive Be proactive when you know there\u0026rsquo;s a particularly tricky part coming up, and you know you want to take it on\nIf you feel some part of the question could/would become problematic, fight the instinct to avoid it and instead pinpoint it right away.\nYou can say something like this:\n\u0026ldquo;The challenge we would face very soon would be with [multiple workers updating their offsets while grabbing the next task concurrently?]. Let me finish the [API specs / DB schema / etc] part and then begin attacking that challenge. I know that would be a hard part, and might kindly ask you to navigate it if I begin approaching the problem from the wrong end.\u0026rdquo;\n","date":"2025-02-22","id":36,"permalink":"/system-design/interview/interview_phrases/","summary":"\u003ch1 id=\"how-to-push-back-against-your-interviewer-in-a-helpful-way\"\u003eHow to push back against your interviewer in a helpful way\u003c/h1\u003e\n\u003cp\u003eExample of a candidate pushing back:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInterviewer:\u003c/strong\u003e \u0026ldquo;I wouldn\u0026rsquo;t use a cache here in my opinion\u0026rdquo;\n\u003cstrong\u003eInterviewee:\u003c/strong\u003e \u0026ldquo;Sure we can take out the cache, though the reason I think a cache might be useful here is [insert technical reasoning here]. Do you think there might be a better way we can approach this?\u0026rdquo;\u003c/p\u003e","tags":[],"title":"Interview Phrases"},{"content":"","date":"2025-02-22","id":37,"permalink":"/system-design/projects/interviewing.io/","summary":"","tags":[],"title":"Interviewing.Io"},{"content":"Sources https://interviewing.io/guides/system-design-interview/part-four\n","date":"2025-02-22","id":38,"permalink":"/system-design/projects/interviewing.io_example/","summary":"\u003ch1 id=\"sources\"\u003eSources\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://interviewing.io/guides/system-design-interview/part-four\"\u003ehttps://interviewing.io/guides/system-design-interview/part-four\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"Interviewing.Io Example"},{"content":"Requirements actors: user, job, schedule\nuser can upload job user can start job manually or by schedule\njob set up flow: upload job, start manually or establish schedule view and notifications/alerts?\naccess patterns\ncontent\njobs and schedules are immutable?\nNFR: security, availability maximum execution time: hours user experience -\u0026gt; job per customer: 10e5 100B jobs per day\nhttps://www.youtube.com/watch?v=Bt6mVg5ivyQ\n","date":"2025-02-22","id":39,"permalink":"/system-design/projects/job-scheduler/","summary":"\u003ch1 id=\"requirements\"\u003eRequirements\u003c/h1\u003e\n\u003cp\u003eactors: user, job, schedule\u003c/p\u003e\n\u003cp\u003euser can upload job\nuser can start job manually or by schedule\u003c/p\u003e\n\u003cp\u003ejob set up flow: upload job, start manually or establish schedule\nview and notifications/alerts?\u003c/p\u003e","tags":[],"title":"Job Scheduler"},{"content":"There is no perfect design. Each design archieves a specific balance regarding the tradeoffs of the read, write and memory considerations.\nWrite-through: data is written in cache \u0026amp; DB; I/O completion is confirmed only when data is written in both places Write-around: data is written in DB only; I/O completion is confirmed when data is written in DB Write-back: data is written in cache first; I/O completion is confirmed when data is written in cache; data is written to DB asynchronously (background job) and does not block the request from being processed\nterms: \u0026lt;- should be in the paper\nconsistent hashing: Hashing\n","date":"2025-02-22","id":40,"permalink":"/system-design/projects/key-value-store/","summary":"\u003cp\u003eThere is no perfect design. Each design archieves a specific balance regarding the tradeoffs of the read, write and memory considerations.\u003c/p\u003e","tags":[],"title":"Key Value Store"},{"content":"Advices, Best Practices, Principles Learn in Public\nContinuously seek feedback and be open to learning Demonstrate your value through action and collaboration. Advice: Build things from the ground up\nConstanstly use knowledge sharing Get Your Hands Dirty Do many projects, start small. Incremental improvements.\nLearning examples as a well-focused projects with a paper and documentation.\nEngage more deeply with topics by experimenting more; start by crafting simple concepts and connecting them effectively.\nyou need to constantly apply learning principles (and apply) always remember your \u0026lsquo;why?\u0026rsquo; \u0026ldquo;What I cannot build, I do not understand\u0026rdquo; Richard Feynman\nConsistency is key: dedicate at least one hour or more each day to coding.\ngrow habit of creating Learn in public\nPracticing is really important.\nPracticing Practicing generally refers to the act of repeatedly doing something in order to develop a skill, improve performance, or gain mastery. It involves consistent effort and dedication over time. For example, practicing a musical instrument means regularly playing scales, exercises, or pieces to become more proficient. Practicing a sport involves going through drills, techniques, and gameplay strategies to enhance agility or teamwork. Essentially, to practice is to engage in an activity repeatedly and deliberately with the goal of growing one\u0026rsquo;s expertise or ability.\npracticing vs application vs usage\nbest practices and advices:\nconnection with improvement\npracticing objects\u0026hellip;\ntop down vs botton-up approach\nConstructivist Learning Define main principles and use them https://en.wikipedia.org/wiki/Experiential_learning\nto_sort https://en.wikipedia.org/wiki/Learning_styles#Criticism\nhttps://en.wikipedia.org/wiki/Kolb%27s_experiential_learning https://en.wikipedia.org/wiki/Learning_cycle\nhttps://en.wikipedia.org/wiki/Corrective_feedback\nHigh-Impact Practices George D. Kuh identified High-Impact practices (HIPs) as \u0026quot; a Specific set of practices that tended to lead to meaningful experiences for students.\u0026quot;\nlearning / understanding has different meanings for different things. So, ok. What else? -\u0026gt; Collect and practice LN for LB, methods, processes. Iterate PDCA/PDSA.\nMemorise some common tradeoffs (you\u0026rsquo;ll see some patterns once you learn about 7-8 designs)\nhow to make a notes? How to make them useful? what do you need to note and how?\nkind of \u0026ldquo;systemic\u0026rdquo; approach - in the end you should give working prototypes with all other things (documentation, reasoning)\nhow would you learn some thing: sources (articles, papers) + notes + practical usage reflection (in the cases list - you need to create a list of them)\nLife cycle of a thing to use a Feature (thing, tool, method, practice etc. ): learn -\u0026gt; modify -\u0026gt; practice -\u0026gt; analyse -\u0026gt; teach and get feedback -\u0026gt; improve\nSocial Bot: learn \u0026gt; publish \u0026gt; modify-\u0026gt;use-\u0026gt;analyse\u0026amp;publish-\u0026gt; discuss \u0026amp; get feedback\nhow to effectively get feedback?\nuse specific examples teach others what you have learned socializing writing posts about this repeating, practicing, combining decomposition \u0026amp; small task progress List:\ncommunicate, teach, collaborate solve problems (amazon thing) feedback practical and well-focused projects (one main target) problem-first approach for some of the topics glossary / (other word) terms and things: roadmap, plan, steps, guides, cirriculum,\nmethodologies reflexive_learning and connections with Reflective_practice\nIJSDL10.1 self-directed learning\nknowledge_sharing Offer to host workshops or lunch-and-learn sessions on topics bridging DevOps, architecture, and ML. Share insights from your learning journey or projects you\u0026rsquo;ve undertaken.\nlaunch and learn sessions as an example of knowledge sharing. how they\u0026rsquo;re organized review Launch And Learn https://trueaccord.atlassian.net/wiki/spaces/ENG/pages/1442185233/2024+Lunch+and+Learns\ndiscussion platforms; discussions;\nhttps://en.wikipedia.org/wiki/Double-loop_learning also triple-loop learning\nLessons Learned https://en.wikipedia.org/wiki/Lessons_learned book: The Lessons Learned Handbook (file) #todo worth reading file: eput lessons team (some interesting things)\nLessons learned (American English) or lessons learnt (British English) are experiences distilled from past activities that should be actively taken into account in future actions and behaviors.\nCommunication and Collaboration discuss problems in https://www.reddit.com/r/MLQuestions/\nHow to land a job – go to ML panel events. Talk to the panelists. Ahead of time, do some research on the panelists so that when you chat with them, you can impress them and feed into their ego that you know what they\u0026rsquo;re working on. If you have also done some side projects, be sure to mention it. They\u0026rsquo;ll likely ask for a resume and see if you\u0026rsquo;re a good fit. Landing a job requires both knowledge of the ML and being a hustler and networking with folks. Most of people I know who landed an ML role after working in traditional software engineering simply networked and knew the right people. You can network from anywhere in the world as long as you have an internet connection.\n? collaborative learning \u0026lt;- collaborative problem solving, etc.. collaborative? collective\nOffer to host workshops or lunch-and-learn sessions on topics bridging DevOps, architecture, and ML. Share insights from your learning journey or projects you\u0026rsquo;ve undertaken.\nDefinitions Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences.\nHere are various definitions of learning from multiple perspectives, each emphasizing a unique aspect:\nCognitive Definition: Learning is the process by which individuals encode, store, and retrieve information, using it to make sense of the world and solve problems. It focuses on internal mental processes and knowledge acquisition.\nConstructivist Definition: Learning is an active, constructive process where individuals create meaning from experiences. This view suggests that learners build knowledge by integrating new experiences with prior knowledge through exploration and reflection.\nConnectivist Definition: Learning is the ability to make connections within networks of information sources, often facilitated by technology. This view, designed for the digital age, focuses on networking, adaptability, and finding relevant information in complex systems.\nBiological Definition: Learning is the formation and strengthening of neural connections in response to new stimuli or experiences. This perspective considers learning as a physical process within the brain, driven by neuroplasticity.\nEducational Perspective: Learning is the acquisition and application of knowledge, skills, values, and attitudes essential for personal, social, and professional development. It highlights structured instruction and lifelong growth.\nExperiential Learning Definition: Learning is the process of gaining knowledge or skills through direct experience and reflection, involving active engagement with real-world challenges (as per David Kolb\u0026rsquo;s Experiential Learning Theory).\nInformal Learning Definition: Learning is a spontaneous, unstructured process through which individuals acquire knowledge from everyday interactions and experiences, outside of formal education or training.\nEach definition underscores different elements, like behavior change, cognitive processing, social influence, network connections, brain adaptation, and real-world applications, showing how learning is a multifaceted and adaptive process.\nlearn_in_public https://www.swyx.io/learn-in-public\nv1-principles-learn-in-public.pdf\nThere\u0026rsquo;s a lot to learn, and you will never be done learning, especially with AI, when new revolutionary papers and ideas are released weekly.\nThe biggest mistake you can make is to learn in private. You don\u0026rsquo;t create any opportunities for yourself if you do that. You don\u0026rsquo;t have anything to show for it besides being able to say you completed something. What matters more is what you made of the information, how you turned it into knowledge to be shared with the public, and what novel ideas and solutions came from that information.\nThat means having a habit of creating.\nTo escape tutorial hell and really learn, you have to get hands-on, write algorithms from scratch, implement papers, and do fun side projects using AI to solve problems.\nuse proper theory , use a theory, experimentation, modeling, simulation.. Do many projects, start small.\nThis can mean:\nwriting blogs and tutorials join hackathons and collaborate with others ask and answer questions in Discord communities work on side projects you\u0026rsquo;re passionate about tweeting about something interesting you discovered new And speaking about Twitter,\nml Organization Studying online courses for hours per day can be hard, it\u0026rsquo;s very active engaged learning. I\u0026rsquo;ve found 6 hours on days off and 2-4 hours on work days is a nice middle ground. I usually read 2 hours, work on math for 2 hours, work on ML courses for 2 hours. I\u0026rsquo;ve had a couple of nice work related data science projects that I fully commit time to when they come up. I always apply methods to my own datasets and build my own implementations alongside the coursework.\nI literally learned most of the bases from chatgpt. Then practiced on kaggle and trying to replicate papers result or models\nIf you want to improve your knowledge in a certain area, I\u0026rsquo;d actually recommend making a project and looking up whatever issues you face and whatever you need to learn for it. But for starting out, it is horrible. I\u0026rsquo;d definitely recommend learning from traditional courses first, starting with linear algebra and statistics, then move on to the basics of machine learning.\nlearning this curriculum may be extremely challenging to take on alone. It is highly recommended to find a more experienced mentor, or at the very least a study partner. There are suggestions for more accessible alternatives, which have the same prerequisites, below.\nhttps://medium.com/bitgrit-data-science-publication/a-roadmap-to-learn-ai-in-2024-cc30c6aa6e16 interesting one with a lot of advices\nhttps://towardsdatascience.com/how-id-learn-machine-learning-if-i-could-start-over-c68d697e6a8a https://medium.com/geekculture/how-to-become-very-good-at-machine-learning-a78e3d93fe96 https://medium.com/@amirziai/a-semi-random-collection-of-advice-that-ive-found-very-useful-as-an-ml-engineer-d7ae59209a76\nhow to learn https://www.reddit.com/r/learnmachinelearning/comments/1fre362/a_note_to_my_six_month_younger_self/\ntopic: behavioral questions. learning is also here\nHere is another type of learning: learn from mistakes learn from incidents to prevent future problems and improve (what?)\nUnderstanding Understanding\nUnderstanding is a relation. We can model it.\nCommon Learning Problem-Based Approach Problem-based learning #learning/principle\nproblem-first approach https://mkremins.github.io/blog/doors-headaches-intellectual-need/\nfirstly you need to understand a problem, a limitations of currently using tools and then, analyzing several possible solution options you have to construct your own #learning\nhttps://en.wikipedia.org/wiki/Problem-based_learning https://en.wikipedia.org/wiki/How_to_Solve_It\nProject-Based Approach parallels with problem-based, formulate differences\nhttps://en.wikipedia.org/wiki/Project-based_learning\nadvice from ML: If you want to improve your knowledge in a certain area, I\u0026rsquo;d actually recommend making a project and looking up whatever issues you face and whatever you need to learn for it. But for starting out, it is horrible. I\u0026rsquo;d definitely recommend learning from traditional courses first, starting with linear algebra and statistics, then move on to the basics of machine learning.\nI literally learned most of the bases from chatgpt. Then practiced on kaggle and trying to replicate papers result or models\nhttps://www.reddit.com/r/learnmachinelearning/comments/1d1u2aq/i_started_my_ml_journey_in_2015_and_changed_from/ \u0026ldquo;If you already know the topics that Andrej Karpathy covers in zero to hero, and you have applied them at work, you are already ahead of lots of FAANG ML engineers. If you\u0026rsquo;re feeling bored or hitting a ceiling, it\u0026rsquo;s time to start thinking about what problems you\u0026rsquo;d like to tackle at work or in the world and how you\u0026rsquo;d solve them with ML. You can then start to tweak your models to solve these problems. Real world ML requires a lot of data acquisition and sanitizing before you even get to the modeling step. Once you\u0026rsquo;re ready to develop a model, there are so many directions you can take it.\u0026rdquo; \u0026lt;- here modifying and applying cycle (not full one)\nPersonally I find that I need a substantial project to implement in a new interesting language. You can read a book at become familiar with syntax, and be exposed to interesting features, but a nothing gets you thinking in a new language like a project.\nYou can do 1-2 YouTube tutorials just to get a feel for it if that\u0026rsquo;s helpful to you, but you won\u0026rsquo;t learn nearly as much this way. It\u0026rsquo;s important to try to build something yourself as soon as possible. You\u0026rsquo;ll feel overwhelmed by not knowing what to do, but just read the documentation, google what you don\u0026rsquo;t know, and keep making progress forwards even if it feels slow\nCreation Lens As a backend developer, you can apply creation as a method to deepen your understanding, improve your skills, and become more effective in your role. Here\u0026rsquo;s how you can approach it in a practical and structured way.\nUnderstanding isn\u0026rsquo;t just theoretical; it must translate into the ability to create or replicate the concept.\nFeynman\u0026rsquo;s Approach in Action\nCreation is a way to \u0026ldquo;test\u0026rdquo; your understanding against reality. Feynman believed that if you could explain (or in this case, build) something from the ground up, then you truly understood it. If you struggled, it was a signal to revisit and refine your knowledge.\nLearning Through Construction\nThe process of building forces you to confront gaps in knowledge and strengthens your understanding.\nSoftware Development\nIf you can\u0026rsquo;t write code to implement a feature or system, it may indicate you don\u0026rsquo;t fully understand the underlying principles. Backend developers, for instance, need to grasp algorithms, databases, or APIs well enough to construct and optimize them. Problem-Solving\nThe ability to \u0026ldquo;rebuild\u0026rdquo; a solution from scratch is often the ultimate test of comprehension.\nTo Understand Complex Systems\nBuilding something forces you to break down complexity into manageable, understandable parts. It helps uncover hidden interdependencies, nuances, and challenges that theory alone cannot reveal. Example: Writing a backend API helps you understand how data flows through systems and why certain architectural choices are made. To Identify Knowledge Gaps\nThe process of creation highlights areas where your knowledge is incomplete or assumptions fail. It encourages asking critical questions and seeking out resources to fill those gaps. Example: While implementing a new feature, you might realize you lack understanding of certain database query optimizations. ? Internalize learning Learning a programming language becomes solidified when you build a project rather than just reading documentation.\nTo Communicate Understanding\nThe ability to build something demonstrates understanding in a way words cannot. It\u0026rsquo;s a practical way to show others how something works, making knowledge transferable and shareable. Example: Creating a proof of concept or demo showcases your understanding of a technology to stakeholders. To Build Confidence Seeing your ideas take shape boosts confidence and motivates further exploration and learning. Example: Successfully deploying a project reinforces your belief in your capabilities and encourages tackling larger challenges. Also - to push boundaries and to innovate\nCreation fosters innovation by combining existing knowledge in new ways. The act of building inspires creativity and often leads to unexpected insights. Example: Building a microservice architecture for a specific use case might inspire a novel way to improve scalability for other systems. Experimentation Experimentation: Testing Ideas and Learning Through Failure(?)\nEdge Cases Simulate extreme scenarios like high traffic, large datasets, or service failures. Experiment with throttling, retries, and circuit breaker patterns.\nMindset for Experimentation Fail Fast, Learn Faster: Expect failure and treat it as a learning opportunity. Document Observations: Keep a record of what worked and what didn\u0026rsquo;t to improve iteratively. Stay Curious: Ask \u0026ldquo;What if?\u0026rdquo; questions to push boundaries Applications [speaking]\nSystem-design\nGo Learning be a mentor on https://exercism.org/\nHere we destructuring (other word) the topic of learn to pieces: the pieces of a language: syntax, structures, standard libs, collections, ecosystem\ncommon engineering practices patterns\ntasks, exercises, katas, lessons, common methodology: https://www.reddit.com/r/learnprogramming/comments/uh96pf/fastest_way_to_learn_a_new_programming_language/ https://simpleprogrammer.com/learn-new-programming-language/\n","date":"2025-02-22","id":41,"permalink":"/projects/foundations/learning/","summary":"\u003ch1 id=\"advices-best-practices-principles\"\u003eAdvices, Best Practices, Principles\u003c/h1\u003e\n\u003cp\u003eLearn in Public\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eContinuously seek feedback and be open to learning\u003c/li\u003e\n\u003cli\u003eDemonstrate your value through action and collaboration.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAdvice: Build things from the ground up\u003c/p\u003e","tags":[],"title":"Learning"},{"content":"","date":"2025-02-22","id":42,"permalink":"/system-design/projects/live-commenting/","summary":"","tags":[],"title":"Live Commenting"},{"content":"https://www.youtube.com/watch?v=LcJKxPXYudE good video and comments\nhttps://martinfowler.com/articles/microservices.html\nhttps://www.martinfowler.com/articles/distributed-objects-microservices.html\nhttps://microservices.io/patterns/index.html https://eventuate.io/exampleapps.html\n","date":"2025-02-22","id":43,"permalink":"/system-design/topics/microservices/","summary":"\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=LcJKxPXYudE\"\u003ehttps://www.youtube.com/watch?v=LcJKxPXYudE\u003c/a\u003e\ngood video and comments\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://martinfowler.com/articles/microservices.html\"\u003ehttps://martinfowler.com/articles/microservices.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.martinfowler.com/articles/distributed-objects-microservices.html\"\u003ehttps://www.martinfowler.com/articles/distributed-objects-microservices.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://microservices.io/patterns/index.html\"\u003ehttps://microservices.io/patterns/index.html\u003c/a\u003e\n\u003ca href=\"https://eventuate.io/exampleapps.html\"\u003ehttps://eventuate.io/exampleapps.html\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"Microservices"},{"content":"","date":"2025-02-22","id":44,"permalink":"/system-design/nfr/","summary":"","tags":[],"title":"Nfr"},{"content":"","date":"2025-02-22","id":45,"permalink":"/system-design/projects/online-file-storage/","summary":"","tags":[],"title":"Online File Storage"},{"content":"Why do we need to partition For very large datasets, or very high query throughput, that is not sufficient: we need to break the data up into partitions, also known as sharding.\nPartitioning of Key-Value Data If the partitioning is unfair, so that some partitions have more data or queries than others, we call it skewed. The presence of skew makes partitioning much less effective. In an extreme case, all the load could end up on one partition, so 9 out of 10 nodes are idle and your bottleneck is the single busy node. A partition with disproportion‐ ately high load is called a hot spot.\nThe simplest approach for avoiding hot spots would be to assign records to nodes randomly. That would distribute the data quite evenly across the nodes, but it has a big disadvantage: when you\u0026rsquo;re trying to read a particular item, you have no way of knowing which node it is on, so you have to query all nodes in parallel.\nby key range One way of partitioning is to assign a continuous range of keys (from some minimum to some maximum) to each partition.\nhash partitioning Because of this risk of skew and hot spots, many distributed datastores use a hash function to determine the partition for a given key.\nFor partitioning purposes, the hash function need not be cryptographically strong: for example, Cassandra and MongoDB use MD5, and Voldemort uses the Fowler– Noll–Vo function.\nOnce you have a suitable hash function for keys, you can assign each partition a range of hashes (rather than a range of keys), and every key whose hash falls within a partition\u0026rsquo;s range will be stored in that partition.\nThis technique is good at distributing keys fairly among the partitions. The partition boundaries can be evenly spaced, or they can be chosen pseudorandomly (in which case the technique is sometimes known as consistent hashing).\nconsistent hashing. this particular approach actually doesn\u0026rsquo;t work very well for databases [8], so it is rarely used in practice (the documentation of some databases still refers to consistent hashing, but it is often inaccurate). Because this is so confusing, it\u0026rsquo;s best to avoid the term consistent hashing and just call it hash partitioning instead.\nUnfortunately however, by using the hash of the key for partitioning we lose a nice property of key-range partitioning: the ability to do efficient range queries.\nskewed workloads and relieving hot-spots\nPartitioning and Secondary Indexes Rebalancing Partitions Over time, things change in a database: • The query throughput increases, so you want to add more CPUs to handle the load. • The dataset size increases, so you want to add more disks and RAM to store it. • A machine fails, and other machines need to take over the failed machine\u0026rsquo;s responsibilities.\nAll of these changes call for data and requests to be moved from one node to another.\nThe process of moving load from one node in the cluster to another is called rebalancing.\nNo matter which partitioning scheme is used, rebalancing is usually expected to meet some minimum requirements: • After rebalancing, the load (data storage, read and write requests) should be shared fairly between the nodes in the cluster. • While rebalancing is happening, the database should continue accepting reads and writes. • No more data than necessary should be moved between nodes, to make rebalancing fast and to minimize the network and disk I/O load.\nrequest routing We have now partitioned our dataset across multiple nodes running on multiple machines. But there remains an open question: when a client wants to make a request, how does it know which node to connect to? As partitions are rebalanced, the assignment of partitions to nodes changes. Somebody needs to stay on top of those changes in order to answer the question: if I want to read or write the key \u0026ldquo;foo\u0026rdquo;, which IP address and port number do I need to connect to? This is an instance of a more general problem called service discovery, which isn\u0026rsquo;t limited to just databases. Any piece of software that is accessible over a network has this problem, especially if it is aiming for high availability (running in a redundant configuration on multiple machines). Many companies have written their own in- house service discovery tools, and many of these have been released as open source [30]. On a high level, there are a few different approaches to this problem (illustrated in Figure 6-7):\nAllow clients to contact any node (e.g., via a round-robin load balancer). If that node coincidentally owns the partition to which the request applies, it can handle the request directly; otherwise, it forwards the request to the appropriate node, receives the reply, and passes the reply along to the client. Send all requests from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly. This routing tier does not itself handle any requests; it only acts as a partition-aware load balancer. Require that clients be aware of the partitioning and the assignment of partitions to nodes. In this case, a client can connect directly to the appropriate node, without any intermediary. In all cases, the key problem is: how does the component making the routing decision (which may be one of the nodes, or the routing tier, or the client) learn about changes in the assignment of partitions to nodes? This is a challenging problem, because it is important that all participants agree— otherwise requests would be sent to the wrong nodes and not handled correctly. There are protocols for achieving consensus in a distributed system, but they are hard to implement correctly\nMany distributed data systems rely on a separate coordination service such as Zoo‐ Keeper to keep track of this cluster metadata, as illustrated in Figure 6-8. Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. Other actors, such as the routing tier or the partitioning-aware client, can subscribe to this information in ZooKeeper. Whenever a partition changes ownership, or a node is added or removed, ZooKeeper notifies the routing tier so that it can keep its routing information up to date.\nFor example, LinkedIn\u0026rsquo;s Espresso uses Helix [31] for cluster management (which in turn relies on ZooKeeper), implementing a routing tier as shown in Figure 6-8. HBase, SolrCloud, and Kafka also use ZooKeeper to track partition assignment. MongoDB has a similar architecture, but it relies on its own config server implemen‐ tation and mongos daemons as the routing tier. Cassandra and Riak take a different approach: they use a gossip protocol among the nodes to disseminate any changes in cluster state. Requests can be sent to any node, and that node forwards them to the appropriate node for the requested partition (approach 1 in Figure 6-7). This model puts more complexity in the database nodes but avoids the dependency on an external coordination service such as ZooKeeper. Couchbase does not rebalance automatically, which simplifies the design. Normally it is configured with a routing tier called moxi, which learns about routing changes from the cluster nodes [32]. When using a routing tier or when sending requests to a random node, clients still need to find the IP addresses to connect to. These are not as fast-changing as the assignment of partitions to nodes, so it is often sufficient to use DNS for this purpose.\n","date":"2025-02-22","id":46,"permalink":"/system-design/topics/partitioning/","summary":"\u003ch1 id=\"why-do-we-need-to-partition\"\u003eWhy do we need to partition\u003c/h1\u003e\n\u003cp\u003eFor very large datasets, or very high query throughput, that is not sufficient: we need to break the data up into partitions, also known as sharding.\u003c/p\u003e","tags":[],"title":"Partitioning"},{"content":"Your mentioned also that we should retrieve interviews I\u0026rsquo;m thinking about that actual interviewing.io\nExactly what words to say in specific scenarios What to say when you don\u0026rsquo;t know what to do Weak interview candidates are scared to ever utter the phrase \u0026ldquo;I don\u0026rsquo;t know.\u0026rdquo; Stronger interview candidates say \u0026ldquo;I don\u0026rsquo;t know\u0026rdquo; more often and then strengthen this with a buffer—the words you put around your uncertainty. A naked \u0026ldquo;I don\u0026rsquo;t know\u0026rdquo; is a dead end, so your interviewer has nowhere to go. Adding a buffer gives your interviewer several paths they can take to move the conversation forward.\nхх\nBefore we tell you exactly what words to say, let\u0026rsquo;s unpack a concept. There are different levels of \u0026ldquo;not knowing.\u0026rdquo; Sometimes you have no clue. Sometimes you have a clue. And other times, you\u0026rsquo;re certain.\nhttps://interviewing.io/guides/system-design-interview/part-two#c-exactly-what-words-to-say-in-specific-scenarios\nQuestions Asking High-Impact, Non-Trivial Questions Many candidates ask basic clarifying questions (e.g., \u0026ldquo;What\u0026rsquo;s the expected user base?\u0026rdquo;). A proactive candidate asks impactful, second-order questions, such as:\nThe goal is to uncover hidden constraints and trade-offs that shape the design\u0026rsquo;s direction.\nAlternative Maybe ask not the expected user base, but make a set of assumptions and then aks about the set itself. Does it reflect the actual situation?\nVerify phrases Is that correct? \u0026ldquo;Is that a fair way to describe this?\u0026rdquo; 2.\u0026ldquo;Would you say that\u0026rsquo;s an accurate representation of the concept?\u0026rdquo; 2.\u0026ldquo;Does that seem like a correct description to you?\u0026rdquo; 3.\u0026ldquo;Do you think I\u0026rsquo;m on the right track with that explanation?\u0026rdquo; 4.\u0026ldquo;Have I captured the essence of the idea adequately?\u0026rdquo; 5.\u0026ldquo;Do you agree with this portrayal of the system?\u0026rdquo; 6.\u0026ldquo;Does that capture the idea correctly, in your opinion?\u0026rdquo; 7.\u0026ldquo;Is my understanding in line with what we\u0026rsquo;re aiming for?\u0026rdquo; 8.\u0026ldquo;Would this be an appropriate way to articulate the design?\u0026rdquo; 9.\u0026ldquo;Does this align well with your understanding?\u0026rdquo; 10.\u0026ldquo;Is this depiction consistent with your perception of the system?\u0026rdquo; Assumption phrases Let\u0026rsquo;s assume that\n\u0026ldquo;For the purpose of this discussion, we can consider\u0026hellip;\u0026rdquo;\n\u0026ldquo;In order to simplify the problem, we will suppose that\u0026hellip;\u0026rdquo;\n\u0026ldquo;As a starting point, let\u0026rsquo;s take into account that\u0026hellip;\u0026rdquo;\n\u0026ldquo;To begin with, let\u0026rsquo;s presume that\u0026hellip;\u0026rdquo;\n\u0026ldquo;In the context of this system design, we\u0026rsquo;ll work under the assumption that\u0026hellip;\u0026rdquo;\n\u0026ldquo;For the sake of our analysis, we\u0026rsquo;ll assume that\u0026hellip;\u0026rdquo;\n\u0026ldquo;To simplify our approach, let\u0026rsquo;s suppose that\u0026hellip;\u0026rdquo;\n\u0026ldquo;For the sake of this discussion, let\u0026rsquo;s say that\u0026hellip;\u0026rdquo;\n\u0026ldquo;If we consider a situation where\u0026hellip;\u0026rdquo;\n\u0026ldquo;Presuming that\u0026hellip;\u0026rdquo;\n\u0026ldquo;Under the assumption that\u0026hellip;\u0026rdquo;\n\u0026ldquo;Suppose that\u0026hellip;\u0026rdquo;\n\u0026ldquo;In a scenario where\u0026hellip;\u0026rdquo;\n\u0026ldquo;Assuming, for argument\u0026rsquo;s sake, that\u0026hellip;\u0026rdquo;\n\u0026ldquo;If we take for granted that\u0026hellip;\u0026rdquo;\n\u0026ldquo;Let\u0026rsquo;s propose a hypothetical in which\u0026hellip;\u0026rdquo;\n\u0026ldquo;Should we accept the premise that\u0026hellip;\u0026rdquo;\n\u0026ldquo;Positing that\u0026hellip;\u0026rdquo;\n\u0026ldquo;Given the condition that\u0026hellip;\u0026rdquo;\n\u0026ldquo;On the understanding that\u0026hellip;\u0026rdquo;\n\u0026ldquo;Taking into consideration that\u0026hellip;\u0026rdquo;\n\u0026ldquo;Assuming the scenario where\u0026hellip;\u0026rdquo;\n\u0026ldquo;Let\u0026rsquo;s model a situation where\u0026hellip;\u0026rdquo;\nRemember to use these phrases to clearly outline assumptions or conditions to your design. This will help set up a clear context and constraints for your design solutions.\nAssumption questions And just a general question. Can I assume that [your auth and authorization is already handled?]\n\u0026ldquo;Is it safe to presume that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Am I correct in thinking that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Do we take it as a given that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Would it be reasonable to infer that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Is it correct to suppose that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;May I consider that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Would you agree if I say that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Is it accurate to assume that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Can we operate under the assumption that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Could we establish that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Would it be logical to propose that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Are we working on the premise that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Can we accept the notion that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Is it fair to suggest that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Should we proceed under the belief that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Can we base our discussion on the idea that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Would it be sensible to envisage that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Can we build our argument on the fact that\u0026hellip;?\u0026rdquo;\n\u0026ldquo;Could we take for granted that\u0026hellip;?\u0026rdquo;\nTrade-off Discussion Phrases These phrases highlight the pros and cons of a design decision.\n\u0026ldquo;If we use a relational database, we\u0026rsquo;ll have strong consistency and structured data, but it may not scale horizontally as well as a NoSQL database.\u0026rdquo;\n\u0026ldquo;Implementing microservices will provide scalability and isolation, but it might also introduce complexity in terms of service coordination and data consistency.\u0026rdquo;\n\u0026ldquo;While using a SQL database would give us the benefits of ACID properties and structured data, we might encounter scalability issues as the user base grows.\u0026rdquo;\n\u0026ldquo;If we choose to implement a microservices architecture, we would benefit from better scalability and separation of concerns, but we also need to consider the increased complexity in service communication and data consistency.\u0026rdquo;\n\u0026ldquo;Opting for a monolithic architecture could expedite our initial development process due to its simplicity. However, in the long run, we might face issues with scalability and agility.\u0026rdquo;\n\u0026ldquo;Implementing a caching layer can significantly improve our system\u0026rsquo;s performance by reducing database load. However, we would then need to address challenges related to cache invalidation and maintaining data consistency.\u0026rdquo;\n\u0026ldquo;While vertical scaling might provide us with a straightforward way to improve our system\u0026rsquo;s capacity, it\u0026rsquo;s not as cost-effective or flexible as horizontal scaling.\u0026rdquo;\n\u0026ldquo;Implementing data sharding can help manage large volumes of data more efficiently, but it also introduces additional complexity, such as handling distributed transactions and joins.\u0026rdquo;\n\u0026ldquo;Using a NoSQL database can offer us flexible data models and easy scalability, but we lose some of the benefits of structured querying and ACID properties that a SQL database provides.\u0026rdquo;\n\u0026ldquo;Storing data in multiple regions can improve latency and availability for users worldwide, but we\u0026rsquo;d have to carefully handle data replication and consistency.\u0026rdquo;\nWhen discussing trade-offs during a system design interview, there are several categories that you can focus on. Here are some examples:\nPerformance vs Scalability: These phrases discuss the trade-off between optimizing a system for performance (speed) versus scaling (handling growth).\n\u0026ldquo;Using a cache can significantly improve performance, but it also introduces the problem of cache management as we scale.\u0026rdquo; \u0026ldquo;We could optimize our database for read-heavy operations for faster performance, but this could affect our ability to scale for write-heavy operations.\u0026rdquo; Consistency vs Availability: In distributed systems, there\u0026rsquo;s often a trade-off between data consistency and system availability (as described in the CAP theorem).\n\u0026ldquo;We could design for strong consistency, but that might impact our system\u0026rsquo;s availability in case of network partition.\u0026rdquo; \u0026ldquo;If we aim for high availability, we might need to tolerate eventual consistency in our data.\u0026rdquo; Speed of Development vs System Performance: These phrases discuss the trade-off between quickly building a system and optimizing it for high performance.\n\u0026ldquo;Using an ORM could speed up our development process, but it might not be as performant as writing raw SQL queries.\u0026rdquo; \u0026ldquo;A monolithic architecture may allow us to develop quickly in the early stages, but it could impact performance and scalability in the long term.\u0026rdquo; Read Optimization vs Write Optimization: These phrases discuss the trade-off between optimizing for read operations versus write operations.\n\u0026ldquo;Denormalizing the database can speed up read operations, but it can slow down write operations due to the need for multiple updates.\u0026rdquo; \u0026ldquo;If we optimize for write operations, we might slow down read operations, as more complex queries might be required.\u0026rdquo; Cost vs Performance: These phrases discuss the trade-off between the cost of resources (like servers or services) and the performance they provide.\n\u0026ldquo;While using premium server hardware or services can improve our system\u0026rsquo;s performance, it will also increase our costs.\u0026rdquo; \u0026ldquo;We could use a CDN to improve our global load times, but this would also increase our operational expenses.\u0026rdquo; To Sort and Review Common phrases and synonims going to do something Intend to do something Aim to do something Requirements Can I assume that this system is already handled or do I need also implement it here?\nJust to simplify and for the context what I\u0026rsquo;m doing here I will assume that I\u0026rsquo;ll get an email address that is already validated and all that and that email address will be what I would use to refer to specific users.\nI just want to get on a general overwiev so we are sure that we are on the same page so basically we are going to have like a user [situation overview]\nSo the first thing that we are going to have is that both will have to access some type of front end. In it the logged in\nWe are also going to have a some way to compile and run the code\nI\u0026rsquo;m assuming here we are talking about a coding interview which is more interesting. But do you care about other types of interview.\nDo you want me to cover other parts? -\u0026gt; No\nRewiew phrases and their aim.\nAnd at the same time we need to have something that will be able to store the interview. This would be bassically the audio support where we are actually going to allow users to talk to each other.\nSo I think discover the main features that we are going to support.\nIs there anything else that is not on the list that you think I should cover?\nOk. Ok. This makes perfect sense. Okay. (Agreement)\nSo this service will actually have to support like multiple concurrent interviews of course like there might be more than one interview happening at the same time. But that should be fine \u0026hellip; so \u0026hellip; I\u0026rsquo;m making an assumption here and you might correct me if I\u0026rsquo;m wrong. But assistant like interviewer dot IO it might have maybe let\u0026rsquo;s say hundreds of users doing an interview at the same moment in time but it is it seems unlikely that they would have like hundreds of thousands with like different orders of magnitude right uh is this a fair assumption do you want me to assume that it will have like a reasonable number of users or do you want me to just for the sake of it assume that we are going to have I don\u0026rsquo;t know a million users are doing it at the same time or something like that I think talking about how your system can scale ^ question about assumption and REWRITED So, this service will need to support multiple concurrent interviews, as there may be more than one interview happening simultaneously. However, I want to clarify my assumption and invite your input if I\u0026rsquo;m mistaken. For instance, Assistant Interviewer.IO might have hundreds of users conducting interviews concurrently, but it seems unlikely that it would have hundreds of thousands of users or orders of magnitude higher. Is it fair to assume that we should consider a reasonable number of users, or would you like me to assume an extreme scenario, such as a million users conducting interviews simultaneously? I believe discussing how the system can scale is important in this context.\nxxxxxxxx\nDesign for me an interviewing.io\nI\u0026rsquo;ll try here to draw main components (shows drawing.io)\nSo, basically,\nFirst, we are going to have the online editor and\nso basically these these two users will would actually talk then as I mentioned with that at first with the general front end here uh this front end is obviously like a distributed piece of infrastructure. So for the purpose of this I will put it as a black box here. If we have time later we can try make it like to to give it a specific dimension to see how many like service we would need and all that. But because I will be more worried about allowing it to scale than to have a correct number of instance right off the bat. Then I will not discuss this at this point in time but I we can get to it like later. right? So basically yeah these users they are already authenticated\nAnd basically what it does.\nxxxx design Spotify (IGonAnOffer)\nI can think of thing I mean there\u0026rsquo;are song musics, [list of ] Lets establish some basic use cases.\nBut most core thing is that I play a song and it\u0026rsquo;s coming back through my phone maybe my car\nLet\u0026rsquo;s do some quick drilling down into some numbers here. So tell me about how many users you\u0026rsquo;re thinking about here for this design?\nWhat about number of songs?\nmake do a black amish a thing? \u0026hellip;, I barely had any suicides, did I? I\u0026rsquo;d say Robert\u0026rsquo;s about 40-odd - maybe 45.\nTo learn roots, you must understand the prefix and suffix pattern of words and their impact when added to different words.\nfrom grokking sd interview\nTo handle this, a more intelligent LB solution can be placed that periodically queries backend server about their load and adjusts traffic based on that.\n","date":"2025-02-22","id":47,"permalink":"/system-design/interview/phrases/","summary":"\u003cp\u003eYour mentioned also that we should retrieve interviews\nI\u0026rsquo;m thinking about that actual interviewing.io\u003c/p\u003e\n\u003ch1 id=\"exactly-what-words-to-say-in-specific-scenarios\"\u003eExactly what words to say in specific scenarios\u003c/h1\u003e\n\u003ch2 id=\"what-to-say-when-you-dont-know-what-to-do\"\u003eWhat to say when you don\u0026rsquo;t know what to do\u003c/h2\u003e\n\u003cp\u003eWeak interview candidates are scared to ever utter the phrase \u0026ldquo;I don\u0026rsquo;t know.\u0026rdquo; Stronger interview candidates say \u0026ldquo;I don\u0026rsquo;t know\u0026rdquo; more often and then strengthen this with a buffer—the words you put around your uncertainty. A naked \u0026ldquo;I don\u0026rsquo;t know\u0026rdquo; is a dead end, so your interviewer has nowhere to go. Adding a buffer gives your interviewer several paths they can take to move the conversation forward.\u003c/p\u003e","tags":[],"title":"Phrases"},{"content":"","date":"2025-02-22","id":48,"permalink":"/projects/","summary":"","tags":[],"title":"Projects"},{"content":"Events\nThis is a page about different useful topics.\nLearning here you find out how to learn, best practices\nHeading 2 Heading 3 text\n","date":"2025-02-22","id":49,"permalink":"/projects/about/","summary":"\u003cp\u003e\u003ca href=\"http://localhost:1313/system-design/elements/background/events/\"\u003eEvents\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is a page about different useful topics.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://localhost:1313/projects/foundations/learning/\"\u003eLearning\u003c/a\u003e here you find out how to learn, best practices\u003c/p\u003e\n\u003ch2 id=\"heading-2\"\u003eHeading 2\u003c/h2\u003e\n\u003ch3 id=\"heading-3\"\u003eHeading 3\u003c/h3\u003e\n\u003cp\u003etext\u003c/p\u003e","tags":[],"title":"Projects"},{"content":"","date":"2025-02-22","id":50,"permalink":"/system-design/projects/","summary":"","tags":[],"title":"Projects"},{"content":"questions from the \u0026lsquo;asking the great questions\u0026rsquo; article\nStakeholder communication will be a critical component to your success. Be prepared to provide compelling answers to questions such as: • Why are we doing this? • Why does it have to be done now? • What will happen if we don\u0026rsquo;t do it? • How does it impact me? How will I benefit? • How will this benefit the organization? • When will it go into effect? • Why are we doing a slow integration rather than all at once? • How does this affect my job? My pay? • How much additional time will this add when it comes to hiring employees, performance reviews, talent reviews, etc.? • How will we know if this is successful?\n","date":"2025-02-22","id":51,"permalink":"/projects/foundations/questions/","summary":"\u003cp\u003equestions from the \u0026lsquo;asking the great questions\u0026rsquo; article\u003c/p\u003e\n\u003cp\u003eStakeholder communication will be a critical component to your success. Be prepared to provide compelling answers to questions such as:\n• Why are we doing this?\n• Why does it have to be done now?\n• What will happen if we don\u0026rsquo;t do it?\n• How does it impact me? How will I benefit?\n• How will this benefit the organization?\n• When will it go into effect?\n• Why are we doing a slow integration rather than all at once?\n• How does this affect my job? My pay?\n• How much additional time will this add when it comes to hiring employees, performance reviews, talent reviews, etc.?\n• How will we know if this is successful?\u003c/p\u003e","tags":[],"title":"Questions"},{"content":"Types message queues\nAdvantages buffering traffic spikes If a message has to be processed by some very expensive code, you may also hold them in a queue while previous messages are being processed so you don\u0026rsquo;t overload (and potentially kill) servers. Queues can deliver messages to multiple systems, instead of the client having to send them to all the required systems. Queues decouple the client from the server by eliminating the need to know the server address. Based on the different implementations of message queues, there can be different combinations of the following properties: ~ Guaranteed delivery. ~ No duplicate messages are delivered. ~ Ensure that the order of messages is maintained. ~ At least once delivery with idempotent consumers.\n","date":"2025-02-22","id":52,"permalink":"/system-design/topics/queues/","summary":"\u003ch1 id=\"types\"\u003eTypes\u003c/h1\u003e\n\u003cp\u003emessage queues\u003c/p\u003e\n\u003ch1 id=\"advantages\"\u003eAdvantages\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003ebuffering traffic spikes\u003c/li\u003e\n\u003cli\u003eIf a message has to be processed by some very expensive code, you may also hold them in a queue while previous messages are being processed so you don\u0026rsquo;t overload (and potentially kill) servers.\u003c/li\u003e\n\u003cli\u003eQueues can deliver messages to multiple systems, instead of the client having to send them to all the required systems.\u003c/li\u003e\n\u003cli\u003eQueues decouple the client from the server by eliminating the need to know the server address.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBased on the different implementations of message queues, there can be different combinations of the following properties:\n~ Guaranteed delivery.\n~ No duplicate messages are delivered.\n~ Ensure that the order of messages is maintained.\n~ At least once delivery with idempotent consumers.\u003c/p\u003e","tags":[],"title":"Queues"},{"content":"Sources https://www.youtube.com/watch?v=FU4WlwfS3G0 chapter from SD interview insider\u0026rsquo;s guide https://medium.com/criteo-engineering/distributed-rate-limiting-algorithms-a35f7e24783 https://www.mailgun.com/blog/it-and-engineering/gubernator-cloud-native-distributed-rate-limiting-microservices/\ntodo: review video and discuss Rate Limiter with Vladimir write paper and place it to my site communicate with other people and get feedback about my composed solution\nstructure of the article introduction\nWhich things should be in the article?\nUnderstand the problem and establish the design scope As usual, let\u0026rsquo;s start with the problem statement.\nSo, here is an example. We have launched a web application which has become highly popular. Suddenly, one or several clients started to send much more requests than they did previously. And because of this, other clients of our application begin to experience higher latency for their requests, or a higher rate of failed requests. These situations may lead to a so-called \u0026ldquo;noisy neighbor problem\u0026rdquo; when one client utilizes too many shared resources on a service host, like CPU, memory, disk, or network I/O. Get another example. We may need cost reduction. Limiting excess requests means fewer servers and allocating more resources to high-priority APIs. As a solution to these kinds of problems, we can use throttling.\nArchitectural characteristics:\nlow latency accuracy scalability high availability fault tolerance integration easy Possible requirement types for RL:\nwhere to place: client-side or server-side scale: startup or big company what information give to clients of throttled requests kind of throttling: soft or hard throttling rules: user id, IP, other properties level of RL should RL be part of an application or done as a separate service Further, we can list some patterns to use:\nautoscaling capability of our service load balancer rate limiter \u0026lt;\u0026ndash; select this option Create a High-Level Design https://youtu.be/FU4WlwfS3G0?t=559\nCheck design with envelope estimation Desing Deep Dive There are 3 possible ways: OOP https://youtu.be/FU4WlwfS3G0?t=1009 Going Distributed: We need to decide between availability and performance trade-offs.\nDistributed algorithms What we need is a centralized and synchronous storage system and an algorithm that can leverage it to compute the current rate for each client. An in-memory cache (like Memcached or Redis) is ideal. But not all rate-limiting algorithms can be implemented with every caching system. So let\u0026rsquo;s see what kind of algorithm exists.\nWrapping Up Algorithms\nCriteo has said that a rate limiter is a critical piece of software, and its very goal is to protect the rest of the system from a heavy load. But I\u0026rsquo;m afraid I have to disagree with this point because there is a case where we can use it to suppress requests from a client on a free plan in a scheme with paid/free contracts.\nYou can refill a bucket at a fixed rate and on a token retrieval call.\nlist definitions of the chosen architectural charateristics Follow up: performance optimization\nSo we have two models: one metamodel that contains a broad problem analysis and the part containing things mostly belonging to the SD interview process.\n","date":"2025-02-22","id":53,"permalink":"/system-design/projects/rate_limiter/","summary":"\u003ch1 id=\"sources\"\u003eSources\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=FU4WlwfS3G0\"\u003ehttps://www.youtube.com/watch?v=FU4WlwfS3G0\u003c/a\u003e\nchapter from SD interview insider\u0026rsquo;s guide\n\u003ca href=\"https://medium.com/criteo-engineering/distributed-rate-limiting-algorithms-a35f7e24783\"\u003ehttps://medium.com/criteo-engineering/distributed-rate-limiting-algorithms-a35f7e24783\u003c/a\u003e\n\u003ca href=\"https://www.mailgun.com/blog/it-and-engineering/gubernator-cloud-native-distributed-rate-limiting-microservices/\"\u003ehttps://www.mailgun.com/blog/it-and-engineering/gubernator-cloud-native-distributed-rate-limiting-microservices/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003etodo:\u003c/strong\u003e\nreview video and\ndiscuss Rate Limiter with Vladimir\nwrite paper and place it to my site\ncommunicate with other people and get feedback about my composed solution\u003c/p\u003e","tags":[],"title":"Rate Limiter"},{"content":"Definitions Reflection involves examining experiences to gain insights and improve future actions.\nReflection is a concept that spans multiple disciplines, each offering a unique perspective on its nature, purpose, and mechanisms.\nReflection is a powerful tool for personal and professional growth. It allows individuals (only them ? ) to critically examine experiences, extract valuable insights, and apply lessons to future actions. Whether in education, healthcare, leadership, or personal development, mastering reflection enhances self-awareness, decision-making, and adaptability.\nIn summary, reflection is a multi-faceted process that can significantly enhance learning and personal growth across various domains. By understanding its levels, adhering to best practices, and utilizing established frameworks, individuals can cultivate a robust reflective practice that fosters continuous improvement.\nReflection (can be) deliberate and structured process of critically examining past experiences, actions, and thoughts to gain deeper insights, improve decision-making, and foster continuous personal and professional growth. It involves actively engaging with one\u0026rsquo;s own experiences to extract lessons, challenge assumptions, and apply newfound knowledge to future situations.\n(?) Scientific Method Perspective (Critical Inquiry \u0026amp; Evaluation) Definition: Reflection in scientific inquiry is the systematic review of methods, data, and conclusions to refine hypotheses, eliminate bias, and improve research accuracy. 📌 Key Principle: The ability to critically assess and adapt methodologies based on results.\n📌 Example: A scientist reflecting on unexpected results and adjusting experimental controls ensures more valid conclusions.\n6. Leadership \u0026amp; Business Perspective (Strategic Thinking \u0026amp; Decision-Making)\nDefinition: Reflection in leadership is the practice of evaluating past decisions, behaviors, and team dynamics to improve strategic thinking, enhance emotional intelligence, and drive continuous organizational growth.\n📌 Key Theories: Double-Loop Learning (Argyris \u0026amp; Schön), Adaptive Leadership\n📌 Example: A leader reflecting on a failed project, identifying gaps in communication, and adjusting leadership strategies for future success.\n7. Mindfulness \u0026amp; Well-being Perspective (Self-Awareness \u0026amp; Emotional Regulation)\nDefinition: Reflection is the conscious practice of observing thoughts and emotions without judgment, leading to greater self-awareness, emotional regulation, and well-being.\n📌 Key Practices: Mindfulness Meditation, Cognitive Behavioral Therapy (CBT), Journaling\n📌 Example: Someone practicing mindfulness reflects on negative thoughts without reacting, leading to emotional stability and better stress management.\ntodo here as I think is problem - many people don\u0026rsquo;t know about such concept and tool. don\u0026rsquo;t know how to use it properly (second phase problem) (?unknown unknowns) Reflection is not just passive recollection; it is an intentional and analytical process that enhances self-awareness, emotional intelligence, and problem-solving abilities. By systematically evaluating past experiences, individuals and organizations can refine strategies, avoid repeating mistakes, and cultivate adaptive thinking.\nPros / Outcomes: By consistently engaging in thoughtful reflection, you can develop a deeper understanding of yourself and your experiences, leading to continuous improvement and success in all aspects of life.\nLevels (Depths) (Level Set 1 ~ Phychological) Reflection progresses through different depths of thinking, each serving a distinct function:\nDescriptive Reflection (Descriptive Thinking): This initial level involves recounting experiences without deep analysis. It focuses on what happened and the context surrounding the event. Analytical Reflection (Exploratory Thinking). At this level, individuals examine the reasons behind events, behaviors, and emotions. It involves: Evaluating personal actions and their impact Identifying patterns and causes Considering alternative perspectives Critical Reflection (Transformational Thinking). The deepest level of reflection, critical reflection, challenges assumptions, beliefs, and biases. It fosters: A re-examination of core values and thought processes Awareness of social, cultural, or systemic influences Personal and professional transformation through intentional change (Level Set 2 ~ Businness and personal) Level 1: Surface-Level Reflection (Reactive) Business: Teams react to immediate feedback, such as bug reports, customer complaints, or performance alerts. Personal: Individuals reflect only when they hit obstacles or receive feedback from others.\nLevel 2: Process Reflection (Adaptive) Business: Teams review workflows and optimize based on past challenges, using retrospectives and data-driven adjustments. Personal: Individuals assess recurring patterns in their habits and adjust their approach proactively.\n👉 Example (Backend/AI Service): The team notices recurring AI service crashes and updates their deployment strategy to include better logging and resource allocation before future failures happen.\nLevel 3: Strategic Reflection (Proactive) Business: Teams anticipate challenges before they happen, using risk assessment, long-term product roadmaps, and knowledge-sharing culture. Personal: Individuals think beyond daily tasks and focus on personal development, career trajectory, and skill evolution. 👉 Example (Backend/AI Service): Instead of waiting for performance issues, the AI service team proactively redesigns their architecture, adopting serverless or microservices to scale more effectively.\nLevel 4: Transformational Reflection (Visionary) Business: Teams rethink their entire approach, innovating not just within their domain but shifting the industry or company strategy. Personal: Individuals reflect deeply on values, purpose, and alignment with long-term aspirations. 👉 Example (Backend/AI Service): The team realizes AI model performance is limited by the current hardware and shifts to edge computing, reducing reliance on cloud-based inference and gaining a competitive advantage.\nHow to Master Reflection: Practical Strategies Mastering reflection requires deliberate practice. Here are the key techniques to develop a strong reflective habit:\n1. Make Reflection a Routine Reflection should be a regular practice, not just an occasional activity. Establish a habit by:\n✔ Setting aside time daily or weekly for structured reflection\n✔ Using prompts to guide deeper thinking\n✔ Integrating reflection into meetings, journaling, or discussions\n1a. Create a Structured Routine.\n(this one is already in previous bullet) Regular Practice: Carve out dedicated time (daily, weekly) to systematically think back on important events and tasks. Set Clear Objectives (?): Focus your reflection on specific areas (e.g., leadership, communication, problem-solving) to gain targeted insights. 2. Use Guiding and Open-Ended Questions To deepen your reflection, ask Questions that encourage exploration:\nWhat went well? What didn\u0026rsquo;t? Why? What emotions did I experience, and what triggered them? What assumptions influenced my decisions? How can I apply this insight to future situations? 3. Write It Down Keeping a reflection journal enhances clarity and accountability. Benefits include:\n✔ Identifying patterns over time ✔ Processing emotions constructively ✔ Tracking growth and improvements\n4. Seek External Perspectives Receiving feedback from others can uncover blind spots. Ways to incorporate external insights:\n✔ Peer discussions or mentorship sessions ✔ Reviewing past experiences with a mentor ✔ Engaging in group reflection exercises\n5. Apply Insights to Action Reflection is only valuable if it leads to improvement. Strengthen the connection between reflection and action by:\n✔ Setting specific, actionable goals based on insights\n✔ Experimenting with new approaches based on lessons learned\n✔ Monitoring progress and adapting as needed\nKey Takeaways: Becoming a Master of Reflection ✅ Reflection deepens learning, enhances self-awareness, and improves decision-making\n✅ There are different levels of reflection, from basic description to deep critical thinking\n✅ Mastering reflection requires routine practice, open-ended questioning, and external feedback\n✅ Action-oriented reflection leads to meaningful personal and professional growth\n✅ Using structured frameworks like Gibbs\u0026rsquo; Cycle, Kolb\u0026rsquo;s Learning Cycle, or the DEAL Model strengthens the process\nKey Components of Reflection todo redefine proper reflection components, question: what does mean component? Awareness \u0026amp; Observation – Recognizing and recalling specific experiences, behaviors, or decisions.\n️Analysis \u0026amp; Interpretation – Evaluating what happened, why it happened, and the factors influencing the outcome.\nLearning \u0026amp; Adaptation – Identifying insights, questioning underlying beliefs, and forming strategies for improvement.\nApplication \u0026amp; Action – Implementing changes based on reflective insights to enhance future performance.\nAct! Really I think most effective (at least in straight reflection type, there are others) that you have to act on the results of reflection -\u0026gt; set goals / direction of reflection (?) to use it in future. Another alternatives - kind of \u0026lsquo;mindfullness\u0026rsquo; reflection.. In business? Think about it. Can you just now narrow the alternatives and scope using this method?\n(?) what about goals as a non-negotiable component of reflection\nFrameworks for Effective Reflection Several models provide structured approaches to reflection. Here are three widely used frameworks:\n0. The Reflection Loop (Observe → Analyze → Adjust → Act) A simple iterative model for improving both personal and team performance.\nSteps:\nObserve: Gather data, experiences, or feedback. Analyze: Identify patterns, strengths, and weaknesses. Adjust: Plan what to improve or change. Act: Implement changes and test new approaches. 1. Gibbs\u0026rsquo; Reflective Cycle (Ideal for structured reflection)\nA six-stage model that guides individuals through a systematic reflective process:\n1️⃣ Description – What happened?\n2️⃣ Feelings – What were your thoughts and emotions?\n3️⃣ Evaluation – What was good or bad about the experience?\n4️⃣ Analysis – Why did things happen the way they did?\n5️⃣ Conclusion – What have you learned?\n6️⃣ Action Plan – How will you apply this learning?\n2. Kolb\u0026rsquo;s Experiential Learning Cycle (Ideal for learning through experience)\nThis model emphasizes learning by doing and consists of four stages:\n1️⃣ Concrete Experience – Engaging in an experience\n2️⃣ Reflective Observation – Reviewing and understanding the experience\n3️⃣ Abstract Conceptualization – Extracting lessons and theories\n4️⃣ Active Experimentation – Applying insights to future situations\n3. The DEAL Model (Describe, Examine, Articulate Learning) (Ideal for professional growth)\nDesigned for structured learning, this model focuses on:\n1️⃣ Describe – Clearly outline the experience\n2️⃣ Examine – Analyze the impact and key takeaways\n3️⃣ Articulate Learning – Define how it will influence future actions\nQuestions What assumptions did I bring into this situation? How does this experience align with or challenge my existing beliefs? What would I do differently next time?\npdf file\nQuestions\nto sort you can analyze not only experience, but your thoughts\u0026hellip; this thoughts, also in the past..\n(modular, iterative approach to practicing) A modular approach to reflection structures it into distinct phases, ensuring both action and learning are continuous. This method integrates execution and reflection in cycles, avoiding overthinking before acting while still improving based on experience. Each module consists of Action ➝ Feedback ➝ Reflection ➝ Adjustment, forming an iterative loop.\n(?) creating a guide for reflective practice\nWhat Is Reflection Paper? (organization) At the first stage of exploration, two broad categories were identified: thinking processes, and qualifiers of thinking processes. The qualifiers were further clustered; this second stage of analysis identified seven categories encompassing the elements of reflective thinking processes. These were: content; process; self; change; conceptual frame; trigger, and context of reflection.\nProcess of developing a definition and model\nsources to review pdf files\nCONCEPTUAL ANALYSIS article Hommel etal Reflection at work\n(interesting one) Mohamed etal Conceptualizing the complexity of reflective practice in education\nhttps://en.wikipedia.org/wiki/Reflective_practice\nWhalen etal \u0026ldquo;Reflective Learning Framework v2.2\u0026rdquo; pdf\n\u0026ldquo;# Fostering collaboration in simulations: How advanced learners benefit from collaboration scripts and reflection\u0026rdquo; https://doi.org/10.1016/j.learninstruc.2024.101912\nhttps://www.peoplemanagement.co.uk/article/1814062/five-ways-leaders-practise-self-reflection-why-its-important-business-growth https://hbr.org/2022/03/dont-underestimate-the-power-of-self-reflection https://www.hays.nl/en/blog/self-reflection-examples\nBy building intentional reflection into our daily lives, we build the muscle we need to navigate life with greater clarity, resilience and authenticity.\nangle / lens / layer / optics\nAngles questions:\nwhat could be done better? performance angle, question so we can review the past experience (dt) from different angles and with different optics\nWhat about levels? What else can we differentiate. AAR(L2) Here we talking about reflection on reflection.\nAt the broadest level, the AAR is a technique that turns a recent event into a learning opportunity by systematically reviewing a task or event of interest.\n-\u0026gt; Common\nAfter-action review After action report Debriefing Hotwash Postmortem documentation Retrospective Benchmarking AAR Reflection and Improvement search and employ various AAR techniques to be a better team\nproblems with me: not strongly oriented on action, prefer reflection and thoughts about\nleadership: This theory proposes that leadership is established when individuals are able to understand the needs of the team and alter their behavior to satisfy those needs\nreflexive approach\nSelf-Reflection https://www.cultureamp.com/blog/performance-review-self-reflections\nIncluding an employee\u0026rsquo;s voice is defined as \u0026ldquo;allowing individuals who are affected by decisions to present information that they consider relevant to the decision.\u0026rdquo;\nProvide employees the opportunity to give their perspective on (and explanations of) how they performed over the review period, but not in the form of self-rating. In Culture Amp\u0026rsquo;s self-reflection template, we recommend 4 questions:\nDelivery on goals: What progress have you made on your goals over [time frame]? Describe the impact on the success of your team, department, and organization. Blockers to goals: What blockers or challenges did you experience over [time frame] that made it harder to achieve your goals? Progress on L\u0026amp;D goals: What formal and informal ways have you developed over [time frame]? Setting new L\u0026amp;D goals: What are 2-3 skills you\u0026rsquo;d like to acquire, develop, or refine over the next [time frame until next cycle]? Cool Thing (to get from) Below is an expanded and refined version of your reflection, integrating insights from reflective practice, growth principles, and structured problem-solving frameworks. It builds on the What → So What → Now What model (often attributed to Borton\u0026rsquo;s or Driscoll\u0026rsquo;s frameworks) and incorporates best practices to foster deeper understanding and actionable improvement.\nReflection on Improving Team Problem-Solving in My Career 1. What? (Describing the Experience) In my career, I\u0026rsquo;ve observed that teams can either excel under pressure—generating innovative ideas and implementing them seamlessly—or stall due to misalignment, unclear communication, and a lack of systematic thinking. Despite sincere efforts, problem-solving sometimes devolves into indecision or inefficiency.\nI\u0026rsquo;ve focused on enhancing my team\u0026rsquo;s problem-solving approach, emphasizing thorough issue analysis, collaborative dialogue, and high-impact solutions. Yet, recurrent obstacles still arise:\nLack of Clear Problem Definition – Teams often jump to solutions without fully exploring the true nature of the problem. Poor Communication – Individuals may not voice concerns or creative ideas due to fear of judgment or a lack of facilitation. Inefficient Decision-Making – Overanalysis or conflicting priorities lead to prolonged debates without clear outcomes. 2. So What? (Analyzing Why It Matters) Effective team problem-solving directly impacts productivity, innovation, and morale. When team members feel aligned and understand the root causes of issues, they devise more robust solutions and support one another in bringing these solutions to life.\nThe current shortcomings in my team\u0026rsquo;s problem-solving processes undermine this potential and can lead to:\nFrustration and Disengagement – When people don\u0026rsquo;t feel heard, they lose motivation. Missed Opportunities – Breakthrough ideas may remain untapped because no one felt comfortable proposing them or the team didn\u0026rsquo;t analyze the problem deeply enough. Reduced Efficiency – Time wasted on poorly structured discussions and unclear responsibilities lengthens the path to solutions. From a reflective practice perspective, reaching beyond surface-level reflection to analytical and critical reflection helps me see the deeper systemic or cultural factors at play. It\u0026rsquo;s not just the team\u0026rsquo;s knowledge that needs refining but the environment and processes we use to solve problems.\n3. Now What? (Actionable Steps to Improve Team Problem-Solving) Below are key strategies aligned with best practices in reflective learning and structured problem-solving to move our team from describing challenges to applying solutions effectively:\nA. Start with a Clear Problem Definition Root Cause Analysis (e.g., 5 Whys): Before suggesting solutions, identify the underlying reasons a problem exists. This reduces guesswork and narrows the team\u0026rsquo;s focus. Structured Problem Statements: Outline what the issue is, who is affected, and why it matters to ensure everyone is on the same page. B. Foster Open Communication \u0026amp; Psychological Safety Safe Environment: Encourage all team members to express ideas or concerns without fear of criticism. This cultivates trust and honest dialogue—key ingredients for innovation. Active Facilitation: Guide discussions to ensure balanced participation, preventing a single voice or subgroup from dominating. C. Implement a Structured Problem-Solving Framework IDEAL Model (Identify, Define, Evaluate, Act, Learn): This systematic approach breaks problem-solving into manageable stages. Varied Brainstorming Techniques: Use mind mapping, silent brainstorming, or digital collaboration tools to avoid groupthink and harness diverse perspectives. D. Streamline Decision-Making Decide on a Method: Whether it\u0026rsquo;s a quick vote-based approach for less critical decisions or a consensus-driven approach for complex issues, clarify how decisions will be finalized. Time-Boxing: Limit the duration for discussions to prevent overanalysis and maintain focus on key priorities. E. Integrate Continuous Learning \u0026amp; Reflection Regular Retrospectives: After each project or major decision, host a brief reflection session—What went well? What could improve? Document Lessons Learned: Keep a shared knowledge base of best practices and key insights to reinforce institutional memory. Critical Reflection \u0026amp; Growth Moving from analytical to critical reflection means challenging underlying assumptions about team dynamics, communication norms, and personal leadership styles. For instance:\nDo I inadvertently discourage certain voices by dominating discussions? Have we institutionalized the right processes for truly open communication? Is there a cultural norm that values ‘fixing\u0026rsquo; problems quickly over understanding them thoroughly? By addressing these deeper systemic factors, I\u0026rsquo;m aligning my efforts not only with improved techniques but with a culture shift toward more empowering, inclusive, and forward-thinking collaboration.\nFinal Thoughts: Embracing a Culture of Problem-Solving Reflection, at its core, is a transformative tool for growth—it reveals blind spots and encourages intentional action. In the context of team problem-solving, it\u0026rsquo;s not just about having superior methods (e.g., IDEAL or the 5 Whys), but about cultivating an environment where learning from mistakes and celebrating different perspectives are ingrained norms.\nBy approaching problem-solving as a shared responsibility and modeling reflective thinking at all levels, the team can become more resilient, innovative, and aligned with our overarching goals. Ultimately, continuous improvement emerges naturally in an atmosphere that respects each person\u0026rsquo;s insights and values structured, collaborative exploration.\n","date":"2025-02-22","id":54,"permalink":"/projects/foundations/reflective_practice/","summary":"\u003ch1 id=\"definitions\"\u003eDefinitions\u003c/h1\u003e\n\u003cp\u003eReflection involves examining experiences to gain insights and improve future actions.\u003c/p\u003e\n\u003cp\u003eReflection is a concept that spans multiple disciplines, each offering a unique perspective on its nature, purpose, and mechanisms.\u003c/p\u003e","tags":[],"title":"Reflective Practice"},{"content":"reliability \u0026amp; availability patterns There are two complementary patterns to support high availability: fail-over and replication. Active-Passive and Active-Active are two types of redundancy configurations that enable high availability in the event of systems failure.\nActive/passive failover. Active/passive failover configurations provide a fully redundant instance of each node that is brought online only if its associated primary node fails. Active-active failover (Active Redundancy) In active-active, both servers are managing traffic, spreading the load between them.\nMaster-slave replication. Master-master replication.\nPractices \u0026amp; principles. You wanna make sure that first and foremost that your system doesn\u0026rsquo;t have single point of failure. =\u0026gt; use redundancy\nlanguage: failover plan\nsources: fine-grained book,\n","date":"2025-02-22","id":55,"permalink":"/system-design/nfr/reliability--availability/","summary":"\u003ch1 id=\"reliability--availability\"\u003ereliability \u0026amp; availability\u003c/h1\u003e\n\u003ch2 id=\"patterns\"\u003epatterns\u003c/h2\u003e\n\u003cp\u003eThere are two complementary patterns to support high availability: \u003cstrong\u003efail-over\u003c/strong\u003e and \u003cstrong\u003ereplication\u003c/strong\u003e. Active-Passive and Active-Active are two types of redundancy configurations that enable high availability in the event of systems failure.\u003c/p\u003e","tags":[],"title":"Reliability \u0026 Availability"},{"content":"requirements: I want to make a server to remove develop\nremote development remote filesystem requirements: mount remote filesystem \u0026lt;- why? to use local obsidian - no you can use your local\ncourses watching movies watching backup\nRemote Example 1 To mount a remote Ubuntu filesystem on a Mac, you can use SSHFS (SSH Filesystem), which allows you to access remote directories over SSH as if they were local.\nbrew install macfuse brew install sshfs\r","date":"2025-02-22","id":56,"permalink":"/projects/remote_server/","summary":"\u003cp\u003erequirements:\nI want to make a server to remove develop\u003c/p\u003e\n\u003ch2 id=\"remote-development\"\u003eremote development\u003c/h2\u003e\n\u003ch2 id=\"remote-filesystem\"\u003eremote filesystem\u003c/h2\u003e\n\u003cp\u003erequirements:\nmount remote filesystem\n\u0026lt;- why?\nto use local obsidian - no you can use your local\u003c/p\u003e","tags":[],"title":"Remote Server"},{"content":"Types Database replication\nReplication means keeping a copy of the same data on multiple machines that are connected via a network.\nbenefits There are several reasons why you might want to replicate data: ~ To keep data geographically close to your users (and thus reduce latency) ~ To allow the system to continue working even if some of its parts have failed (and thus increase availability) ~ To scale out the number of machines that can serve read queries (and thus increase read throughput)\nproblem Each node that stores a copy of the database is called a replica. With multiple replicas, a question inevitably arises: how do we ensure that all the data ends up on all the replicas?\nEvery write to the database needs to be processed by every replica; otherwise, the replicas would no longer contain the same data. The most common solution for this is called leader-based replication (also known as active/passive or master–slave replication). It works as follows:\nOne of the replicas is designated the leader (also known as master or primary). When clients want to write to the database, they must send their requests to the leader, which first writes the new data to its local storage. The other replicas are known as followers (read replicas, slaves, secondaries, or hot standbys).Whenever the leader writes new data to its local storage, it also sends the data change to all of its followers as part of a replication log or change stream. Each follower takes the log from the leader and updates its local copy of the database accordingly, by applying all writes in the same order as they were processed on the leader. When a client wants to read from the database, it can query either the leader or any of the followers. However, writes are only accepted on the leader (the followers are read-only from the client\u0026rsquo;s point of view). regime An important detail of a replicated system is whether the replication happens synchronously or asynchronously. (In relational databases, this is often a configurable option; other systems are often hardcoded to be either one or the other.)\nsync / async replication semi-synchronous chain replication reasoning reasoning example from DDIA\npros/cons: The advantage of synchronous replication is that the follower is guaranteed to have an up-to-date copy of the data that is consistent with the leader. If the leader suddenly fails, we can be sure that the data is still available on the follower. The disadvantage is that if the synchronous follower doesn\u0026rsquo;t respond (because it has crashed, or there is a network fault, or for any other reason), the write cannot be processed. The leader must block all writes and wait until the synchronous replica is available again.\nreasoning: For that reason, it is impractical for all followers to be synchronous: any one node outage would cause the whole system to grind to a halt. In practice, if you enable synchronous replication on a database, it usually means that one of the followers is synchronous, and the others are asynchronous. If the synchronous follower becomes unavailable or slow, one of the asynchronous followers is made synchronous. This guarantees that you have an up-to-date copy of the data on at least two nodes: the leader and one synchronous follower. This configuration is sometimes also called semi-synchronous.\nOften, leader-based replication is configured to be completely asynchronous. In this case, if the leader fails and is not recoverable, any writes that have not yet been replicated to followers are lost. This means that a write is not guaranteed to be durable, even if it has been confirmed to the client. However, a fully asynchronous configuration has the advantage that the leader can continue processing writes, even if all of its followers have fallen behind.\nWeakening durability may sound like a bad trade-off, but asynchronous replication is nevertheless widely used, especially if there are many followers or if they are geographically distributed.\nWe will discuss three popular algorithms for replicating changes between nodes: single-leader, multi-leader, and leaderless replication.\nreplication log How does leader-based replication work under the hood? Several different replication methods are used in practice, so let\u0026rsquo;s look at each one briefly.\nreplication lag problems Unfortunately, if an application reads from an asynchronous follower, it may see out‐ dated information if the follower has fallen behind. This leads to apparent inconsis‐ tencies in the database: if you run the same query on the leader and a follower at the same time, you may get different results, because not all writes have been reflected in the follower. This inconsistency is just a temporary state—if you stop writing to the database and wait a while, the followers will eventually catch up and become consis‐ tent with the leader. For that reason, this effect is known as eventual consistency\nWhen the lag is so large, the inconsistencies it introduces are not just a theoretical issue but a real problem for applications. In this section we will highlight three exam‐ ples of problems that are likely to occur when there is replication lag and outline some approaches to solving them.\nWe looked at some strange effects that can be caused by replication lag, and we dis‐ cussed a few consistency models which are helpful for deciding how an application should behave under replication lag:\nReading Your Own Writes Users should always see data that they submitted themselves.\nproblem roots, formulation and consiquences. -\u0026gt; we need read-after-write consistency, also known as read-your-writes consistency\nHow can we implement read-after-write consistency in a system with leader-based replication? There are various possible techniques. To mention a few:\nMonotonic Reads After users have seen the data at one point in time, they shouldn\u0026rsquo;t later see the data from some earlier point in time.\nOur second example of an anomaly that can occur when reading from asynchronous followers is that it\u0026rsquo;s possible for a user to see things moving backward in time.\nThis can happen if a user makes several reads from different replicas.\nConsistent Prefix Reads Users should see the data in a state that makes causal sense: for example, seeing a question and its reply in the correct order.\nPreventing this kind of anomaly requires another type of guarantee: consistent prefix reads. This guarantee says that if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order.\nSolutions for Replication Lag how to think about this?\nWhen working with an eventually consistent system, it is worth thinking about how the application behaves if the replication lag increases to several minutes or even hours. If the answer is \u0026ldquo;no problem,\u0026rdquo; that\u0026rsquo;s great. However, if the result is a bad experience for users, it\u0026rsquo;s important to design the system to provide a stronger guarantee, such as read-after-write.\nAs discussed earlier, there are ways in which an application can provide a stronger guarantee than the underlying database—for example, by performing certain kinds of reads on the leader. However, dealing with these issues in application code is complex and easy to get wrong.\nIt would be better if application developers didn\u0026rsquo;t have to worry about subtle replication issues and could just trust their databases to \u0026ldquo;do the right thing.\u0026rdquo; This is why transactions exist: they are a way for a database to provide stronger guarantees so that the application can be simpler.\nAlso, the replication itself should be transparent to an external user.\nSingle-node transactions have existed for a long time. However, in the move to dis‐ tributed (replicated and partitioned) databases, many systems have abandoned them, claiming that transactions are too expensive in terms of performance and availability, and asserting that eventual consistency is inevitable in a scalable system. There is some truth in that statement, but it is overly simplistic, and we will develop a more nuanced view\nmulti-leader replication As multi-leader replication is a somewhat retrofitted feature in many databases, there are often subtle configuration pitfalls and surprising interactions with other database features. For example, autoincrementing keys, triggers, and integrity constraints can be problematic. For this reason, multi-leader replication is often considered danger‐ ous territory that should be avoided if possible [28].\ncrdt https://ably.com/blog/crdts-distributed-data-consistency-challenges\nwhat is the conflict?\nscalable approaches for detecting and resolving conflicts in a replicated system.\ntrade-offs There are many trade-offs to consider with replication: for example, whether to use synchronous or asynchronous replication, and how to handle failed replicas. Those are often configuration options in databases, and although the details vary by data‐ base, the general principles are similar across many different implementations. We will discuss the consequences of such choices in this chapter.\nChoosing the Right Replication Strategy Factors:\nsources: ddia,\nreplication Replication is done to achieve one or more of the following goals:\nTo avoid a single point of failure and increase availability when machines go down. To better serve the global users by organizing copies by distinct geological locations in order to serve users from copies that are close by. To increase throughput. With more machines, more requests can be served. language: replica, leader, follower\nsync and async replication, semy-sinc. pros, cons;\ncommon types of replication:\nsingle leader In system design, a single machine acts as a leader, and all write requests (or updates to the data store) go through that machine. All the other machines are used to cater to the read requests. This was previously known as \u0026ldquo;master-slave\u0026rdquo; replication, but it\u0026rsquo;s currently known as \u0026ldquo;primary-standby\u0026rdquo; or \u0026ldquo;active-passive\u0026rdquo; replication.\nThe leader also needs to pass down the information about all the writes to the follower nodes to keep them up to date. In case the leader goes down, one of the follower nodes (mostly with the most up-to-date data) is promoted to be the leader. This is called failover.\nmulti leader In system design, this means that more than one machine can take the write requests. This makes the system more reliable in case a leader goes down. This also means that every machine (including leaders) needs to catch up with the writes that happen over other machines.\n?? Conflict resolution for concurrent writes: #todo\nKeeping the update with the largest client timestamp. Sticky routing—writes from same client/index go to the same leader. Keeping and returning all the updates. Leaderless Replication In such a system, all machines can cater to write and read requests. In some cases, the client directly writes to all the machines, and requests are read from all the machines based on quorum. Quorum refers to the minimum number of acknowledgements (for writes) and consistent data values (for reads) for the action to be valid. In other cases, the client request reaches the coordinator that broadcasts the request to all the nodes.\n","date":"2025-02-22","id":57,"permalink":"/system-design/topics/replication/","summary":"\u003ch1 id=\"types\"\u003eTypes\u003c/h1\u003e\n\u003cp\u003eDatabase replication\u003c/p\u003e\n\u003cp\u003eReplication means keeping a copy of the same data on multiple machines that are connected via a network.\u003c/p\u003e","tags":[],"title":"Replication"},{"content":"conceptualization https://en.wikipedia.org/wiki/Conceptualization_(information_science)\nKnowledge representation and reasoning Ontology alignment Ontology (information science) Semantic integration Semantic matching Semantic translation https://en.wikipedia.org/wiki/Semantics\nhttps://en.wikipedia.org/wiki/Domain_of_discourse#Universe_of_discourse\nOntology https://en.wikipedia.org/wiki/Ontology_(information_science)\nIn information systems and artificial intelligence, where an ontology refers to a specific vocabulary and a set of explicit assumptions about the meaning and usage of these words, an ontological commitment is an agreement to use the shared vocabulary in a coherent and consistent manner within a specific context.(https://en.wikipedia.org/wiki/Ontological_commitment#cite_note-Guarino-2)\nThe notion of a semantic reasoner generalizes that of an inference engine, by providing a richer set of mechanisms to work with. The inference rules are commonly specified by means of an ontology language, and often a description logic language.\nsearch articles and reviews about current state of NLP, bots \u0026amp; ontologies \u0026amp; AGI discover ontologies: https://en.wikipedia.org/wiki/Artificial_intelligence#Knowledge_representation investigate method to combining ontologies and dialog moves and strategies with ML models\nEpistemology https://plato.stanford.edu/entries/epistemology/ https://en.wikipedia.org/wiki/Epistemology https://www.britannica.com/topic/epistemology \u0026hellip; To discover how knowledge arises, they investigate sources of justification, such as perception, introspection, memory, reason, and testimony.\nhttps://www.reddit.com/r/psychologystudents/comments/ikjj4m/can_someone_explain_the_term_operationalize_and/\nhttps://en.wikipedia.org/wiki/Research_question https://en.wikipedia.org/wiki/Empirical_research https://en.wikipedia.org/wiki/Research_design\nconcepts, idea,\nhttps://en.wikipedia.org/wiki/Fuzzy_concept https://en.wikipedia.org/wiki/Theoretical_definition https://en.wikipedia.org/wiki/Operational_definition\nhttps://en.wikipedia.org/wiki/Conceptual_framework https://en.wikipedia.org/wiki/Concept\noperationalization https://en.wikipedia.org/wiki/Operationalization\n","date":"2025-02-22","id":58,"permalink":"/projects/foundations/research/","summary":"\u003cp\u003econceptualization\n\u003ca href=\"https://en.wikipedia.org/wiki/Conceptualization_%28information_science%29\"\u003ehttps://en.wikipedia.org/wiki/Conceptualization_(information_science)\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning\" title=\"Knowledge representation and reasoning\"\u003eKnowledge representation and reasoning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Ontology_alignment\" title=\"Ontology alignment\"\u003eOntology alignment\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Ontology_%5c%28information_science%5c%29\" title=\"Ontology (information science)\"\u003eOntology (information science)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Semantic_integration\" title=\"Semantic integration\"\u003eSemantic integration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Semantic_matching\" title=\"Semantic matching\"\u003eSemantic matching\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Semantic_translation\" title=\"Semantic translation\"\u003eSemantic translation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Semantics\"\u003ehttps://en.wikipedia.org/wiki/Semantics\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"Research"},{"content":"Why scalability problems start with organizations and people, not technology, and what to do about it.\nOntology Definitions Definition. What is it that we really mean by scalability? A service is said to be scalable if when we increase the resources in a system, it results in increased performance in a manner proportional to resources added. Increasing performance in general means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.\nOur review of the literature showed two main uses of the term scalability: Definition 1. Scalability is the ability to handle increased workload (without adding resources to a system). Definition 2. Scalability by extension. Scalability is the ability to handle increased workload by repeatedly applying a costeffective strategy for extending a system\u0026rsquo;s capacity.\nCapacity.\nSources highscalability.com https://systemdesignprimer.netlify.app/getting-started Designing Fine-Grained Microservices Book https://www.lecloud.net/tagged/scalability/chrono some slides - https://www.slideshare.net/jboner/scalability-availability-stability-patterns http://horicky.blogspot.com/2010/10/scalable-system-design-patterns.html https://resources.sei.cmu.edu/asset_files/TechnicalNote/2006_004_001_14681.pdf\nhttps://github.com/binhnguyennus/awesome-scalability\nCommon There isn\u0026rsquo;t one right way to scale a system, as the technique used will depend on the type of constraint you might have. We have a number of different types of scaling we can bring to bear to help with performance, robustness, or perhaps both:\nVertical scaling. In a nutshell, this means getting a bigger machine. Horizontal duplication. Having multiple things capable of doing the same work. Data partitioning. Dividing work based on some attribute of the data, e.g., customer group. Functional decomposition. Separation of work based on the type, e.g., microservice decomposition. Understanding what combination of these scaling techniques is most appropriate will fundamentally come down to the nature of the scaling issue you are facing.\nCombining Models One of the main drivers behind the original Scale Cube was to stop us from thinking narrowly in terms of one type of scaling, and to help us understand that it often makes sense to scale our application along multiple axes, depending on our need.\nIt\u0026rsquo;s worth noting that by scaling along one axis, other axes might be easier to make use of. For example, the functional decomposition of Order enables us to then spin up multiple duplicates of the Order microservice, and also to partition the load on order processing. Without that initial functional decomposition, we\u0026rsquo;d be limited to applying those techniques on the monolith as a whole.\nThe goal when scaling isn\u0026rsquo;t necessarily to scale along all axes, but we should be aware that we have these different mechanisms at our disposal. Given this choice, it\u0026rsquo;s important we understand the pros and cons of each mechanism to work out which ones make the most sense.\nVertical Scaling Benefits. It is fast. On virtualized infrastructure, especially on a public cloud provider, implementing this form of scaling will be fast. Benefits for other types. It\u0026rsquo;s also worth noting that vertical scaling can make it easier to perform other types of scaling. Transparency. Your code or database is unlikely to need any changes to make use of the larger underlying infrastructure, assuming the operating system and chipsets remain the same. Even if changes are needed to your application to make use of the change of hardware, they might be limited to things like increasing the amount of memory available to your runtime through runtime flags.\nHorizontal Duplication With horizontal duplication, you duplicate part of your system to handle more workloads. The exact mechanisms vary—we\u0026rsquo;ll look at implementations shortly—but fundamentally horizontal duplication requires you to have a way of distributing the work across these duplicates.\nAs with vertical scaling, this type of scaling is on the simpler end of the spectrum and is often one of the things I\u0026rsquo;ll try early on. If your monolithic system can\u0026rsquo;t handle the load, spin up multiple copies of it and see if that helps!\nLoad Balancer. Probably the most obvious form of horizontal duplication that comes to mind is making use of a load balancer to distribute requests across multiple copies of your functionality. Load balancer capabilities differ, but you\u0026rsquo;d expect them all to have some mechanism to distribute load across the nodes, and to detect when a node is unavailable and remove it from the load balancer pool. #pattern/lb Queue. Another example of horizontal duplication could be the competing consumer pattern. Making use of read replicas to scale read traffic. In the case of FoodCo, a form of horizontal duplication has been used to reduce the read load on the primary database through the use of read replicas. This has reduced read load on the primary database node, freeing up resources to handle writes, and has worked very effectively, as a lot of the load on the main system was read-heavy. These reads could easily be redirected to these read replicas, and it\u0026rsquo;s common to use a load balancer over multiple read replicas. Data Partitioning Data partitioning requires that we distribute load based on some aspect of data—perhaps distributing load based on the user, for example. More often than not, partitioning will be done by the subsystem you rely on. For example, Cassandra uses partitions to distribute both reads and writes across the nodes in a given \u0026ldquo;ring,\u0026rdquo; and Kafka supports distributing messages across partitioned topics. Source: https://docs.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning https://docs.microsoft.com/en-us/azure/architecture/patterns/sharding\nHorizontal partitioning (sharding) In this strategy, each partition is a separate data store, but all partitions have the same schema. Each partition is known as a shard and holds a specific subset of the data, such as all the orders for a specific set of customers. The most important factor is the choice of a sharding key. It can be difficult to change the key after the system is in operation. The key must ensure that data is partitioned to spread the workload as evenly as possible across the shards.\nsharding https://medium.com/@jeeyoungk/how-sharding-works-b4dec46b3f6 facebook shard manager paper https://kousiknath.medium.com/all-things-sharding-techniques-and-real-life-examples-in-nosql-data-storage-systems-3e8beb98830a https://engineering.fb.com/2020/08/24/production-engineering/scaling-services-with-shard-manager/\nVertical partitioning In this strategy, each partition holds a subset of the fields for items in the data store. The fields are divided according to their pattern of use. For example, frequently accessed fields might be placed in one vertical partition and less frequently accessed fields in another.\nBenefits and Limitations It\u0026rsquo;s worth pointing out that data partitioning has limited utility in terms of improving system robustness. This is why, as outlined earlier, it is common to combine data partitioning with a technique like horizontal duplication to improve the robustness of a given partition.\nFunctional Decomposition With functional decomposition, you extract functionality and allow it to be scaled independently. Extracting functionality from an existing system and creating a new microservice is almost the canonical example of functional decomposition. By itself, functional decomposition isn\u0026rsquo;t going to make our system more robust, but it at least opens up the opportunity for us to build a system that can tolerate a partial failure of functionality, something we explored in more detail in Chapter 12.\nScalability patterns Queue-Based Load leveling pattern Use a queue that acts as a buffer between a task and a service it invokes in order to smooth intermittent heavy loads that can cause the service to fail or the task to time out. This can help to minimize the impact of peaks in demand on availability and responsiveness for both the task and the service.\nA service might experience peaks in demand that cause it to overload and be unable to respond to requests in a timely manner. Flooding a service with a large number of concurrent requests can also result in the service failing if it\u0026rsquo;s unable to handle the contention these requests cause.\nRefactor the solution and introduce a queue between the task and the service. The task and the service run asynchronously.\nThe queue decouples the tasks from the service, and the service can handle the messages at its own pace regardless of the volume of requests from concurrent tasks. Additionally, there\u0026rsquo;s no delay to a task if the service isn\u0026rsquo;t available at the time it posts a message to the queue.\nThis pattern provides the following benefits:\nIt can help to maximize availability because delays arising in services won\u0026rsquo;t have an immediate and direct impact on the application, which can continue to post messages to the queue even when the service isn\u0026rsquo;t available or isn\u0026rsquo;t currently processing messages.\nIt can help to maximize scalability because both the number of queues and the number of services can be varied to meet demand.\nIt can help to control costs because the number of service instances deployed only have to be adequate to meet average load rather than the peak load.\nlink: https://learn.microsoft.com/en-us/azure/architecture/patterns/queue-based-load-leveling\nPriority queue Prioritize requests sent to services so that requests with a higher priority are received and processed more quickly than those with a lower priority. This pattern is useful in applications that offer different service level guarantees to individual clients.\nUsing a priority-queuing mechanism can provide the following advantages:\nIt allows applications to meet business requirements that require the prioritization of availability or performance, such as offering different levels of service to different groups of customers.\nThe multiple message queue approach can help maximize application performance and scalability by partitioning messages based on processing requirements. For example, you can prioritize critical tasks so that they\u0026rsquo;re handled by receivers that run immediately, and less important background tasks can be handled by receivers that are scheduled to run at times that are less busy.\nlink: https://learn.microsoft.com/en-us/azure/architecture/patterns/priority-queue\nCaching You can use cache to reduce load on a target system (which data cache holds). Thus allowing another parts of the system to be scaled effectively. Caching\nCQRS The Command Query Responsibility Segregation (CQRS) pattern refers to an alter‐ nate model for storing and querying information. Rather than our having a single model for how we both manipulate and retrieve data, as is common, responsibilities for reads and writes are instead handled by separate models. These separate read and write models, implemented in code, could be deployed as separate units, giving us the ability to scale reads and writes independently. CQRS is often, though not always, used in conjunction with event sourcing, where—rather than storing the current state of an entity as a single record—we instead project the state of an entity by looking at the history of events related to that entity. Arguably, CQRS is doing something very similar in our application tier to what read replicas can do in the data tier, although due to the large number of different ways CQRS can be implemented, this is a simplification. Personally, although I see value in the CQRS pattern in some situations, it\u0026rsquo;s a com‐ plex pattern to execute well. I\u0026rsquo;ve spoken to very smart people who have hit not insig‐ nificant issues in making CQRS work. As such, if you are considering CQRS as a way to help scale your application, regard it as one of the harder forms of scaling you\u0026rsquo;d need to implement, and perhaps try some of the easier stuff first.\nOne final note on CQRS and event sourcing: from the point of view of a microservice architecture, the decision to use or not use these techniques is an internal implemen‐ tation detail of a microservice. If you\u0026rsquo;ve decided to implement a microservice by split‐ ting responsibility for reads and writes across different processes and models, for example, this should be invisible to consumers of the microservice. If inbound requests need to be redirected to the appropriate model based on the request being made, make this the responsibility of the microservice implementing CQRS. Keeping these implementation details hidden from consumers gives you a lot of flexibility to change your mind later, or to change how you are using these patterns.\nCommunications and Asynchronism We can see communication style as enabler to scalability. You can use asyncronism to parallelization of workload of service.\nQueues Patterns. Idempotency. Asynchronism. Map-Reduce. Space-based Architectures. Stateless services.\nstateless services and idempotent API as a key to scalability\nstateless services and idempotent API as a key to scalability what is about: common things, links and my thoughts on SD*\nStateless services Steve should always get the same results of his request back, independent what server he \u0026ldquo;landed on\u0026rdquo;. That leads to the first golden rule for scalability: every server contains exactly the same codebase and does not store any user-related data, like sessions or profile pictures, on local disc or memory.\nSessions need to be stored in a centralized data store which is accessible to all your application servers. It can be an external database or an external persistent cache, like Redis. An external persistent cache will have better performance than an external database. By external I mean that the data store does not reside on the application servers. Instead, it is somewhere in or near the data center of your application servers.\nPath or Instruction Choose database: SQL or NoSQL for appropriate functionality after first vertical and horizontal duplication. Start Small Define scalability axes chain The architecture that gets you started may not be the architecture that keeps you going when your system has to handle very different volumes of load. As we\u0026rsquo;ve already seen, there are some forms of scaling that can have extremely limited impact on the architecture of your system—vertical scaling and horizontal duplication, for example. At certain points, though, you need to do something pretty radical to change the architecture of your system to support the next level of growth. A redesign may mean splitting apart an existing monolith, as it did for Gilt. Or it might mean picking new data stores that can handle the load better. It could also mean adopting new techniques, such as moving from synchronous request-response to event-based systems, adopting new deployment platforms, changing whole tech‐ nology stacks, or everything in between. There is a danger that people will see the need to rearchitect when certain scaling thresholds are reached as a reason to build for massive scale from the beginning. This can be disastrous. At the start of a new project, we often don\u0026rsquo;t know exactly what we want to build, nor do we know if it will be successful. We need to be able to rapidly experiment and understand what capabilities we need to build. If we tried building for massive scale up front, we\u0026rsquo;d end up front-loading a huge amount of work to pre‐ pare for load that may never come, while diverting effort away from more important activities, like understanding if anyone will actually want to use our product. Eric Ries tells the story of spending six months building a product that no one ever down‐ loaded. He reflected that he could have put up a link on a web page that 404\u0026rsquo;d when people clicked on it to see if there was any demand, spent six months on the beach instead, and learned just as much!\nAutoscaling A news site is a great example of a type of business in which you may want a mix of predictive and reactive scaling. On the last news site I worked on, we saw very clear daily trends, with views climbing from the morning to lunchtime and then starting to decline. This pattern was repeated day in and day out, with traffic generally lower on the weekend. That gave us a fairly clear trend that could drive proactive scaling of resources, whether up or down. On the other hand, a big news story would cause an unexpected spike, requiring more capacity and often at short notice.\nHigh-Level Trade-Offs Performance vs Scalability Another way to look at performance vs scalability:\nIf you have a performance problem, your system is slow for a single user. If you have a scalability problem, your system is fast for a single user but slow under heavy load. And we have another considerations: firstly you have to build performant system, and after it you should brind scalability to your system.\nScalability problem may lead to performance problem. #scalability\nLatency vs Throughput Latency and throughput are the two most important measures of the performance of a system.\nLatency is the time to perform some action or to produce some result. Throughput is the number of such actions or results per unit of time.\nGenerally, you should aim for maximal throughput with acceptable latency.\nAvailability vs Consistency Here should be analysis of CAP theorem\n","date":"2025-02-22","id":59,"permalink":"/system-design/nfr/scalability/","summary":"\u003cp\u003eWhy scalability problems start with organizations and people, not technology, and what to do about it.\u003c/p\u003e\n\u003ch1 id=\"ontology\"\u003eOntology\u003c/h1\u003e\n\u003ch1 id=\"definitions\"\u003eDefinitions\u003c/h1\u003e\n\u003cp\u003eDefinition. What is it that we really mean by scalability? A service is said to be scalable if when we increase the resources in a system, it results in increased performance in a manner \u003cem\u003eproportional\u003c/em\u003e to resources added. Increasing performance in general means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.\u003c/p\u003e","tags":[],"title":"Scalability"},{"content":"Framework IO Framework for SD interviews from interviewing.io consists of three steps:\nRequirements -\u0026gt;\nList of functional requirements and architectural characteristics Data Types, API and Scale -\u0026gt;\nList of Data Types we need to store. Access patterns for these data types. Scale of the data and requests the system needs to serve. Design -\u0026gt;\nData Storage Microservices points: consider mutability ranking\nstep 1. requirements functional Remember\nIdentify the main objects and their relations. What information do these objects hold? Are they mutable? Think about access patterns. \u0026ldquo;Given object X, return all related objects Y.\u0026rdquo; Consider the cross product of all related objects. List all the requirements you\u0026rsquo;ve identified and validate with your interviewer. main objects \u0026amp; relations information \u0026amp; mutability access patterns Think about the possible access patterns for these objects\nAccess patterns are probably the single most influential part of design because they determine how data will be stored.\nLet\u0026rsquo;s think about the cross product of our objects again. This time we want to identify how data will be retrieved from the system.\nThe general shape of an access pattern requirement is: Given [object A], get all related [object B]\nWe\u0026rsquo;re not suggesting you blindly implement all of these, but rather that you consider them as possible access patterns for your clarification questions. For example, should we be able to get all accounts that liked a tweet? Or would the number be enough?\nFor these access patterns, you should also consider ranking.\nAre there any access patterns that require ranking the object? In this example, \u0026ldquo;creating a curated feed of tweets\u0026rdquo; will require further clarification. Strive for simplicity first. Can you return them sorted by chronological time? Identify these access patterns of interest, like the curated feed, and get a feel for what your interviewer is looking for: do they want you to suggest an algorithm for a feed?\ncharacteristics Consistency: performance, availability, consistency, security, ?scalability\nNFRs will strongly influence our design. They define what we should be optimizing for. Bear in mind that you cannot optimize for everything, and you should not overcomplicate your solution. This is a game of trade-offs.\nRule of thumb\nThere is an opportunity to relax one or several specific requirements\nGood candidates can view non-functional requirements mainly as opportunities to relax one specific requirement, such as \u0026ldquo;We don\u0026rsquo;t need to focus on [Insert requirement, such as \u0026ldquo;consistency\u0026rdquo;] as much in this case because [Insert reason, such as \u0026ldquo;it\u0026rsquo;s okay in this scenario of TikTok if some users get access to certain videos later than the rest of our users\u0026rdquo;].\u0026rdquo;\nperformance availability consistency Remember: Non-Functional Requirements\nConsider the three main non-functional requirements: performance, availability, and security. Performance: Which access patterns, if any, require good performance? Availability: What\u0026rsquo;s the cost of downtime for this system? Security: Is there any workflow that requires special security considerations (e.g., code execution)?\nstep 2. Data Types, API and Scale We\u0026rsquo;ve gathered functional and non-functional requirements. At this point we understand what the system is supposed to do as a black box. It\u0026rsquo;s now time to take our first steps toward designing it.\nHowever, you should not begin drawing boxes and discussing implementation right away. There\u0026rsquo;s a bit of pre-work needed before we can start thinking about a concrete design. We need to answer the following three questions:\nWhat data types does the system need to store? What does the API look like? What volume of requests do we need to support? These can be answered pretty quickly from your requirements. In fact, you can probably answer these in just a few minutes. Let\u0026rsquo;s walk through how we might answer each of these questions for our Twitter example:\n2.1 data types What data types does the system need to store?\nThink about the objects the system needs to hold and their data type. There are largely two types of data we might need to store:\nStructured data. Think business objects, like accounts, tweets, likes. Media and blobs. Think images, videos, or any type of large binary data such as TAR or ZIP files. 2.2 api 2.2 What does the API look like?\nRule of thumb\nMore than 90% of the time, users will interact with the system through HTTPS, and as such we encourage you to think about the API in terms of HTTPS requests.\n2.3 scale What volume of requests do we need to support?\nFinally, we should consider the volume of requests that the service needs to serve, as that will influence our design.\nAs a starting point, I recommend that you ask yourself whether this system is read-heavy or write-heavy. Go back to your API and figure out which endpoints are likely to be called more frequently. Do you think our Twitter API would be read-heavy or write-heavy? You guessed it: it\u0026rsquo;s probably read-heavy. Users will be calling getFeed and getTweets far more often than they would call putTweet or retweet.\nNormally, it\u0026rsquo;s enough to think about how people will be using the system and apply some common sense to figure out which endpoints get called the most. In case this is not immediately obvious to you (perhaps you are not familiar with these kinds of systems), it\u0026rsquo;s totally fine to just ask your interviewer. For example: \u0026ldquo;What\u0026rsquo;s the behavior of a typical user using this app?\u0026rdquo; Or be even more direct: \u0026ldquo;What does the distribution of requests look like?\u0026rdquo;\n2.4 back-of-the-envelope math step 3. design The time has come. We\u0026rsquo;ve got all the information we need to start drawing boxes and calling this a \u0026ldquo;system.\u0026rdquo; Yay!\nThere are several reasons that we spent considerable time in steps 1 and 2. Too often people dive straight into design and fail in spectacular ways. It\u0026rsquo;s easy to make that mistake—isn\u0026rsquo;t this interview called \u0026ldquo;system design\u0026rdquo; after all? No one told these candidates that good design is 70%+ requirements and planning.\nIn fact, we can go as far as saying that if you\u0026rsquo;ve executed the last two steps correctly, design should be pretty systematic. This is because system design questions are usually open ended and don\u0026rsquo;t have one single correct answer. Let\u0026rsquo;s use this to our advantage! 💪\nWe know the what (steps 1 and 2), so now we focus on the where and the how. We will start with designing the data storage layer first and then think about the microservices that access this data.\n3.1 data storage blob storage Let\u0026rsquo;s get some of the more obvious components out of the way first. Did you identify any type of media or blobs in step 2.1? If so, these are great candidates to store in blob storage. A blob (Binary Large Object) is basically just binary data. We store and retrieve these as a single item. For example, ZIP files or other binaries.\nSome popular blob stores are Amazon S3 and Azure Blob storage. In general, you don\u0026rsquo;t need to worry too much about the specific brand you\u0026rsquo;d be using. Just tell your interviewer that these images/blobs you identified are good candidates to store in some blob storage, and then draw a \u0026ldquo;blob\u0026rdquo; box.\n[!Rule of thumb] Say the generic name of the component, not the brand name. Unless you are very familiar with a specific brand (like S3), don\u0026rsquo;t say the specific brand. Instead, say \u0026ldquo;some kind of blob storage.\u0026rdquo; Because if you say, \u0026ldquo;we should use S3 here,\u0026rdquo; the next question out of your interviewer\u0026rsquo;s mouth will be, \u0026ldquo;why not Azure blob instead of S3?\u0026rdquo;\nThere\u0026rsquo;s a chance you might want to couple the blob storage with a CDN, but that\u0026rsquo;s something we\u0026rsquo;ll look into in step 3.2. This step is all about identifying how to store content, not how to distribute it.\ndatabase There are a few considerations for this step:\nRelational vs. Non-Relational Entities to store todo learn guide from interviewing.io main source: https://interviewing.io/guides/system-design-interview/part-three\n","date":"2025-02-22","id":60,"permalink":"/system-design/to_sort/sd_algo_interviewing.io/","summary":"\u003ch1 id=\"framework-io\"\u003eFramework IO\u003c/h1\u003e\n\u003cp\u003eFramework for SD interviews from interviewing.io consists of three steps:\u003c/p\u003e\n\u003cp\u003eRequirements -\u0026gt;\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eList of functional requirements and architectural characteristics\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eData Types, API and Scale -\u0026gt;\u003c/p\u003e","tags":[],"title":"Sd Algo Interviewing.Io"},{"content":"SD algorithm This document describes step-by-step framework on how to tackle a system design question\n1. Understand the problem and establish Design Scope What you need to collect and define:\nFunctional Requirements. They may reside in a form of User Stories or Use Cases. Edge Cases and Scenarios that will not be covered Constraints Architectural Characteristics for the whole system or its parts Borders of the system under construction Flag: an agreement with the interviewer on the design. Output (list of artifacts to be obtained): FR (User Stories, Corner Cases), AR+quantitative characteristics\n2. Create a High-Level Design Sketch your main components/parts/subsystems and the connections between them. Justify, refactor and repeat. If possible, go through a few concrete use cases.\n?? Estimations ?? Define Data model (Logical and Physical ERD etc) .\n=\u0026gt; maybe add some APIs for Read/Write scenarios for crucial components/parts then define these API and borders.\nFlag: an agreement with the interviewer on the design. Output: blueprint of a system under construction. Only main components.\n3. Design Deep Dive identify and prioritize components in the architecture dig deeper into 2–3 components and select DB types provide different approaches, their pros and cons, and why would you choose one\nFlag: does your design meet all functional and non-functional requirements? Output: ??\n4. Wrapping up Try to answer these follow-up questions: identify the system bottlenecks and discuss potential improvements it could be useful to give the interviewer a recap of your design. you can talk about error cases (network faults, server failure), operation issues (maybe properties of production-ready systems) how to handle next scale curve\n","date":"2025-02-22","id":61,"permalink":"/system-design/to_sort/sd_algorithm/","summary":"\u003ch1 id=\"sd-algorithm\"\u003eSD algorithm\u003c/h1\u003e\n\u003cp\u003eThis document describes step-by-step framework on how to tackle a system design question\u003c/p\u003e\n\u003ch2 id=\"1-understand-the-problem-and-establish-design-scope\"\u003e1. Understand the problem and establish Design Scope\u003c/h2\u003e\n\u003cp\u003eWhat you need to collect and define:\u003c/p\u003e","tags":[],"title":"Sd Algorithm"},{"content":"To Refactor https://www.youtube.com/watch?v=Gg318hR5JY0 analyze this video from google\nhow candidate reason about scalability and availability? frame an interviewer that you understand that your system would need to be distributed (in some scenarios) design the system to be scalable\ninterview: make reasonable design choice and justify it\nWhat you do is important, but how you communicate is even more important.\nAn interviewer is not expecting exactly correct answers that correspond with a rubric. There is, in fact, no \u0026ldquo;right\u0026rdquo; answer. Instead, they want to see comprehension of the problem at hand. A good interviewee will lead a conversant and comfortable walk through of their assumptions, calculations, trade-offs, and design choices.\nQuestions you may want to keep in mind include:\nHow can we tell that the system is working? Is there a bottleneck in the design? If there are multiple components, what are the APIs and how do they work together? How can we provide great service to users all around the planet? Things you might expect:\nLong gone are the days when everything you design could easily fit onto a single machine. You may be given a large data set to work with. You need to explain how it can be sharded among multiple worker machines. You may be presented with a request which can be answered by any one of a pool of machines and you need to identify the fastest machine and discard the rest. You should also be prepared to discuss system component properties such as latency, throughput, and storage. The interviewer would want to see numeric estimates for these properties, such as how many requests per second the system can handle. Also, be prepared to provide a clear justification of how the design backs up these numbers. You should know industry solution patterns like:\nSharding Data Replication Types Write-ahead Logging Separating Data and Metadata Storage Basic Kinds of Load Distribution Trade-offs and Compromises As a systems designer at Google, you will have to make trade-offs and compromises.\nThe interviewer may ask you to identify systematic shortcomings and describe how the system responds to various failures. You are expected to lay out the trade-offs and compromises you made and explain your reasoning.\nExample: Do you store data on a rotating disk and pay less money for the storage at the cost of increased latency? Or do you put the data on a flash drive where you\u0026rsquo;re able to retrieve it quickly but pay more money?\nGoogle looks for systems designers that can consider multiple solutions, commit to one, and then iterate on it.\nNow that you have the focus areas, here are some overall best practices to keep in mind for your actual interview day.\nExplain: The interviewer wants to understand how you think, so it is important to explain your thought process during the interview. They are not only evaluating your technical ability but also how you solve problems. As an engineer, we like to solve hard problems and jump into the final design; however, this approach is likely to lead you to design the wrong system.\nIn system design interviews, it\u0026rsquo;s crucial to establish a shared understanding with the interviewer before progressing through each phase. Here\u0026rsquo;s a structured approach to achieve this: Try:\nExplore All Alternatives: Before settling on a design choice, consider various options. Discuss these alternatives with the interviewer to evaluate their feasibility and relevance. Confirm Assumptions: Explicitly State Assumptions: Clearly articulate any assumptions you are making about the system\u0026rsquo;s behavior or constraints. Seek Validation: Present these assumptions to the interviewer and ask for confirmation to ensure alignment. For example, you might say, \u0026ldquo;I\u0026rsquo;m assuming the system needs to handle up to 10,000 concurrent users; does that align with your expectations?\u0026rdquo; Obtain Clear Approval Before Proceeding: Summarize Key Points: After discussing requirements and assumptions, recap the main decisions and understandings (?) Seek a Green Light: Ask the interviewer directly if they\u0026rsquo;re comfortable with the discussed approach before moving to the next step. This could be phrased as, \u0026ldquo;Is there anything else we should consider, or are we ready to proceed with this design direction?\u0026rdquo; Algorithm Understand the problem and establish design scope Outline use cases, constraints, and assumptions. Define Feature Expectations\nJust like algorithm design, system design questions will also most likely be weakly defined. So we need gather requirements and scope the problem. You have to get a sense for the scope of the problem before you start exploring the space of possible solutions.\nSo how do you figure out what type of service the interviewer wants you to build? Ask them! A basic prompt leaves room for you to start a conversation with your interviewer about the system you\u0026rsquo;re designing—what type of users does it serve, what type of traffic can it expect, what limits will it have? Demonstrating that you can think critically about the parameters of your service is the first step in any system design interview.\nWhat you need to collect and define:\nFunctional Requirements. May be in form of User Stories or Use Cases. Scenarios that will not be covered and Edge Cases API Design Goals in form of Architectural Characteristics (Non-Functional requirements). It may be such as: number of current and future users and others. Latency and Throughput requirements. Consistency vs Availability [Weak/strong/eventual =\u0026gt; consistency | Failover/replication =\u0026gt; availability]. Question: what parts of my system are absolutely critical? And what parts of my system would be ok to fail? Questions: SLA numbers; volume of the system: number of users, time/flow/peaks: number of current users, peaks data distribution\nUser stories, use cases The very first thing you should do with any system design question is to clarify the system\u0026rsquo;s constraints and to identify what use cases the system needs to satisfy. Spend a few minutes questioning your interviewer and agreeing on the scope of the system. Many of the same rules we discussed while talking about algorithm design apply here as well.\nUsually, part of what the interviewer wants to see is if you can gather the requirements about the problem at hand, and design a solution that covers them well.\nNever assume things that were not explicitly stated.\nFor example, the URL-shortening service could be meant to serve just a few thousand users, but each could be sharing millions of URLs. It could be meant to handle millions of clicks on the shortened URLs, or dozens. The service may have to provide extensive statistics about each shortened URL (which will increase your data size), or statistics may not be a requirement at all.\nYou will also have to think about the use cases that are expected to occur. Your system will be designed based on what it\u0026rsquo;s expected to do. Don\u0026rsquo;t forget to make sure you know all the requirements the interviewer didn\u0026rsquo;t tell you about in the beginning.\nAsk questions to clarify use cases and constraints Do you know the constraints? What kind of inputs does your system need to handle? Discuss assumptions.\nWho is going to use it? How are they going to use it? How many users are there? What does the system do? What are the inputs and outputs of the system? How much data do we expect to handle? How many requests per second do we expect? What is the expected read to write ratio? How fast does the company anticipate to scale up? What are the anticipated scales in 3 months, 6 months, and a year? What is the company\u0026rsquo;s technology stack? What existing services you might leverage to simplify the design? Define API Define what APIs are expected from the system. This would not only establish the exact contract expected from the system but would also ensure if you haven\u0026rsquo;t gotten any requirements wrong. Some examples for our Twitter-like service would be: postTweet(user_id, tweet_text, image_url, user_location, timestamp, …) generateTimeline(user_id, current_time) recordUserTweetLike(user_id, tweet_id, timestamp, …) If you have gathered the requirements and can identify the APIs exposed by the system, you are 50% done.\nDefining Domain Model Defining the Data Model Defining the data model early will clarify how data will flow among different components of the system. Later, it will guide you towards better data partitioning and management. Candidate should be able to identify various entities of the system, how they will interact with each other and different aspect of data management like storage, transfer, encryption, etc. Here are some entities for our Twitter-like service:\nUser: UserID, Name, Email, DoB, CreationData, LastLogin, etc. Tweet: TweetID, Content, TweetLocation, NumberOfLikes, TimeStamp, etc. UserFollows: UserdID1, UserID2 FavoriteTweets: UserID, TweetID, TimeStamp\nOUTPUT: ???\nCreate a High Level Design Which database system should we use? Would NoSQL like Cassandra best fits our needs, or we should use MySQL-like solution. What kind of blob storage should we use to store photos and videos?\nOnce you\u0026rsquo;ve scoped the system you\u0026rsquo;re about to design, you should continue by outlining a high-level abstract design. The goal of this is to outline all the important components that your architecture will need.\nIt is a great idea to collaborate with the interviewer during the process: come up with an initial blueprint for the design. Ask for feedback. Treat your interviewer as a teammate and work together. Many good interviewers love to talk and get involved.\nDraw box diagrams with key components on the whiteboard or paper. This might include clients (mobile/web), APIs, web servers, data stores, cache, CDN, message queue, etc. High Level Design may express the brief functionality of each module in a form of a Usage Scenarios with functional flow. \u0026lt;\u0026ndash; Define this in a form of a Diagram\nGet through main Use Cases.\nYou can tell the interviewer that you would like to do that and draw a simple diagram of your ideas. Sketch your main components and the connections between them. If you do this, very quickly you will be able to get feedback if you are moving in the right direction. Of course, you must be able to justify the high-level design that you just drew.\nDon\u0026rsquo;t get lured to dive deep into some particular aspect of the abstract design. Not yet. Rather, make sure you sketch the important components and the connections between them. Justify your ideas in front of the interviewer and try to address every constraint and use case.\nUsually, this sort of high-level design is a combination of well-known techniques, which people have developed. You have to make sure you are familiar with what\u0026rsquo;s out there and feel comfortable using this knowledge. In this chapter we will assume that you have enough experience to design such a high-level system. Our goal is to focus more on the next steps, where we will talk mainly about scalability and about removing bottlenecks.\nIf possible, go through a few concrete use cases. This will help you frame the high-level design. It is also likely that the use cases would help you discover edge cases you have not yet considered.\nAPIs for Read/Write scenarios for crucial components. Should we include API endpoints and database schema here? This depends on the problem. For large design problems like \u0026ldquo;Design Google search engine\u0026rdquo;, this is a bit of too low level. For a problem like designing the backend for a multi-player poker game, this is a fair game. Communicate with your interviewer. Database schema Basic algorithm High level design for Read/Write scenario Outline a high level design with all important components.\nSketch the main components and connections Justify your ideas ESTIMATIONS [5 min] 6. Throughput (QPS for Read and Write queries) 7. Latency expected from the system (for read and write queries) 8. Read/Write ratio 9. Traffic estimates\nWrite (QPS, Volume of data) Read (QPS, Volume of data) Storage estimates Memory estimates If we are using a cache, what is the kind of data we want to store in the cache How much RAM and how many machines do we need for us to achieve this? Amount of data you want to store in disk/SSD Flag: an agreement with the interviewer on the design.\nOutput:\nDesign deep dive At this step, you and your interviewer should have already achieved the following objectives:\nAgreed on the overall goals and feature scope Sketched out a high-level blueprint for the overall design Obtained feedback from your interviewer on the high-level design Had some initial ideas about areas to focus on in deep dive based on her feedback You shall work with the interviewer to identify and prioritize components in the architecture. It is worth stressing that every interview is different. Sometimes, the interviewer may give off hints that she likes focusing on high-level design. Sometimes, for a senior candidate interview, the discussion could be on the system performance characteristics, likely focusing on the bottlenecks and resource estimations. In most cases, the interviewer may want you to dig into details of some system components. For URL shortener, it is interesting to dive into the hash function design that converts a long URL to a short one. For a chat system, how to reduce latency and how to support online/offline status are two interesting topics.\nTime management is essential as it is easy to get carried away with minute details that do not demonstrate your abilities. You must be armed with signals to show your interviewer. Try not to get into unnecessary details. For example, talking about the EdgeRank algorithm of Facebook feed ranking in detail is not ideal during a system design interview as this takes much precious time and does not prove your ability in designing a scalable system.\nDig deeper into 2–3 components; interviewer\u0026rsquo;s feedback should always guide you towards which parts of the system she wants you to explain further. You should be able to provide different approaches, their pros and cons, and why would you choose one? Remember there is no single answer, the only thing important is to consider tradeoffs between different options while keeping system constraints in mind. e.g.\nSince we\u0026rsquo;ll be storing a huge amount of data, how should we partition our data to distribute it to multiple databases? Should we try to store all the data of a user on the same database? What issues can it cause? How would we handle high-traffic users e.g. celebrities who have millions of followers? Since user\u0026rsquo;s timeline will contain the most recent (and relevant) tweets, should we try to store our data in a way that is optimized to scan latest tweets? How much and at which layer should we introduce cache to speed things up? What components need better load balancing? For example, if you were asked to design a url shortening service, discuss: Generating and storing a hash of the full url, MD5 and Base62, Hash collisions, SQL or NoSQL, Database schema, Translating a hashed url to the full url, Database lookup, API and object-oriented design\nLow Level Design Low-level design fills in some of the gaps to provide extra detail that\u0026rsquo;s necessary before developers can start writing code. It gives more specific guidance for how the parts of the system will work and how they will work together. It refines the definitions of the database, the major classes, and the internal and external interfaces.\nExpectations from the candidates In the LLD Interviews, they will judge you on your knowledge of creating ==modular, flexible, maintainable and reusable software==(this is common architectural characteristics of system under design), by applying [patterns, principles, tools etc].\nUsually LLD talks about the class diagrams with the methods and relations between classes, program specs and other low level details for a given system. It is also known as Object Oriented Design (OOD).\nMachine coding and class diagrams do come under LLD. Most interviewers aren\u0026rsquo;t super interested in the exactness of UML notations. They want to see a general understanding of the class structure and how you design APIs.\nLow Level Design expresses details functional logic of the module.\nHow to solve LLD problems in the Interview 12. Clarify the problem by asking the relevant questions. Gather the complete requirement and start with the basic features. 13. Define the Core Classes ( and Objects ) 14. Establish the relationships between the classes / objects by observing the interactions among the classes / objects. 15. Try to fulfill all the requirements by defining the methods 16. Apply [Object Oriented, Functional etc.] Design Principles and use Design Patterns and other tools to make the system maintainable and reusable. 17. Write well structured clean code (if you are told to implement a function)\nWrap up In this final step, the interviewer might ask you a few follow-up questions or give you the freedom to discuss other additional points. Here are a few directions to follow:\nThe interviewer might want you to identify the system bottlenecks and discuss potential improvements. Never say your design is perfect and nothing can be improved. There is always something to improve upon. This is a great opportunity to show your critical thinking and leave a good final impression. It could be useful to give the interviewer a recap of your design. This is particularly important if you suggested a few solutions. Refreshing your interviewer\u0026rsquo;s memory can be helpful after a long session. Error cases (server failure, network loss, etc.) are interesting to talk about. Operation issues are worth mentioning. How do you monitor metrics and error logs? How to roll out the system? How to handle the next scale curve is also an interesting topic. For example, if your current design supports 1 million users, what changes do you need to make to support 10 million users? Propose other refinements you need if you had more time. Identifying and resolving bottlenecks Most likely your high-level design will have one or more bottlenecks given the constraints of the problem. This is perfectly ok. You are not expected to design a system from the ground up, which immediately handles all the load in the world. It just needs to be scalable, in order for you to be able to improve it using some standard tools and techniques.\nTry to discuss as many bottlenecks as possible and different approaches to mitigate them.\nIs there any single point of failure in our system? What are we doing to mitigate it? Do we\u0026rsquo;ve enough replicas of the data so that if we lose a few servers, we can still serve our users? Similarly, do we\u0026rsquo;ve enough copies of different services running, such that a few failures will not cause total system shutdown? How are we monitoring the performance of our service? Do we get alerts whenever critical components fail or their performance degrades? Principles Things How to deal with ambiguity? What questions should you ask? Is there a some framework?\nWhether this is an algorithmic problem or system design problem, it always makes sense to start with (or at least mention to the interviewer) a simple solution and evolve the solution along the interview.\nCompanies widely adopt system design interviews because the communication and problem solving skills tested in these interviews are similar to those required by a software engineer\u0026rsquo;s daily work.\nThe system design interview simulates real-life problem solving where two co-workers collaborate on an ambiguous problem and come up with a solution that meets their goals. The problem is open-ended, and there is no perfect answer. The final design is less important compared to the work you put in the design process. This allows you to demonstrate your design skill, defend your design choices, and respond to feedback in a constructive manner.\nThe system design interview is an open-ended conversation. You are expected to lead it.\nMany think that system design interview is all about a person\u0026rsquo;s technical design skills. It is much more than that. An effective system design interview gives strong signals about a person\u0026rsquo;s ability to collaborate, to work under pressure, and to resolve ambiguity constructively. The ability to ask good questions is also an essential skill, and many interviewers specifically look for this skill.\nA core aim of a systems design interview is to give the candidate an opportunity to demonstrate their knowledge\nUse your background to your advantage. Your experience and background can vary widely from the next candidate. You bring a set of values and expertise to the table that no one else can. That is what makes you valuable and irreplaceable. Regardless of what field you\u0026rsquo;re in, people care about what you can bring to the table.\nFocus on thought process. What we actually care about is the thought process behind your design choices. This reflects what actually working at Palantir is like. As engineers we have a tremendous amount of freedom. We aren\u0026rsquo;t asked to implement fully-specced features. Instead we take ownership of open-ended problems, and it\u0026rsquo;s our job to come up with the best solution to each. (Delegation). We need people we can trust to do the right thing without a lot of supervision—people who can own large projects and take them consistently in the right direction. Invariably, this means being able to communicate effectively with the people around you. Working on problems with huge scope isn\u0026rsquo;t something you can do in a vacuum.\nRemember that there is no one right answer. A system can be built in different ways. The important thing is to be able to justify your ideas.\nWe\u0026rsquo;ve seen good candidates fail not because they lack the knowledge but because they cannot focus on the right things while discussing a problem.\nFactor: Your ability to articulate your thoughts.\nIterative nature of design: when designing your system, these are the kind of calculations you should be doing over and over in your head.\nIn short, due to the unstructured nature of software design interviews, candidates who are organized with a clear plan to attack the problem have better chances of success.\nAlso, Google divides its system design into different sections: Web Development, Distributed Systems, Machine learning, Mobile Development, Database Design\nTop down + modularization principle\nStaff-Level Can you take a vague goal (\u0026ldquo;design Twitter\u0026rdquo;) and come up with a fully-developed proposal? Are you able to spot ambiguities in the requirements and ask good clarifying questions? Are you able to distinguish between features that really need to go into the MVP versus ones that are really extended/optional features that can be punted?\nDo you proactively look for issues with your design, or do you need prodding from the interviewer?\nAre you able to assess different options and make trade-offs, or are you attached to certain ways of doing things?\nDo you have a good sense of what to prioritize, or do you get lost in the weeds?\nAre you able to produce an actual deliverable within the time frame you are given? Does your design meet all functional and non-functional requirements? In particular, does it scale? While you are not expected to produce an industry-grade design in 45 minutes ー this would be quite unreasonable to ask ー what you produce should be something that can be turned over to a product team for implementation.\nAre you able to achieve the basic requirements quickly so that you have time to extend your design in an interesting way?\nAre you able to use speech, notes, and diagrams to communicate your ideas clearly to someone else? Are you able to take feedback?\nApplications Clarifying questions One of the most useful strategies I personally employ is to ask clarification questions. What are \u0026ldquo;good\u0026rdquo; clarification questions, you ask?\nA good clarification question helps you achieve one, or more, of several things:\nHelps you narrow the scope of what you\u0026rsquo;re supposed to do Helps clarify what the user expectation of the system is Gives you direction about where to proceed Informs you of possible bottlenecks/problem areas In the black box example, you might ask, \u0026ldquo;well, what does the box hold? How many items does it hold? And who is the intended user?\u0026rdquo;\nHandle Murphy\u0026rsquo;s law This is something that most people skip but this is one of the most important things that you must cover which talks about how resilient your system is. In the real world, things break, and when they do, you need to make sure you are in full control of your system.\nTalk about how you monitor the system. What kind of alerting mechanism do you have in place? What are your KPIs (Key Performance Indicators) and how do you track them? What happens when things break, when your service crashes, your DB\u0026rsquo;s master node goes down or even when one of your datacentres goes down? Again, if you haven\u0026rsquo;t done this before, see how I have been doing it, towards the later half of this video.\nNow that you have your high-level design, start thinking about what bottlenecks it has. Perhaps your system needs a load balancer and many machines behind it to handle the user requests. Or maybe the data is so huge that you need to distribute your database on multiple machines. What are some of the downsides that occur from doing that? Is the database too slow and does it need some in-memory caching?\nThese are just examples of questions that you may have to answer in order to make your solution complete. It may be the case that the interviewer wants to direct the discussion in one particular direction. Then, maybe you won\u0026rsquo;t need to address all the bottlenecks but rather talk in more depth about one particular area. In any case, you need to be able to identify the weak spots in a system and be able to resolve them.\nScale the design Identify and address bottlenecks, given the constraints. For example, do you need the following to address scalability issues?\nLoad balancer Horizontal scaling Caching Database sharding Discuss potential solutions and trade-offs. Everything is a trade-off. Address bottlenecks using principles of scalable system design.\nEstimation. Estimation, especially in the form of a back-of-the-envelope calculation, is important because it helps you narrow down the list of possible solutions to only the ones that are feasible. Then you have only a few prototypes or micro-benchmarks to write.\n? Evaluation\nBack-of-the-envelope capacity estimation Envelope_estimations\nFollow-Up Questions How to scale your system to 10^7 users Design the system to apply some arhictetural characteristic: Going distributed Production: monitoring, alerting, logging, deploying, tracing to sort This is my understanding of what interviewers are by-and-large looking for in a candidate for a Staff Software Engineer (or higher) position, at least as it pertains to the systems design interview.\nCan you take a vague goal (\u0026ldquo;design Twitter\u0026rdquo;) and come up with a fully-developed proposal? Are you able to spot ambiguities in the requirements and ask good clarifying questions? Are you able to distinguish between features that really need to go into the MVP versus ones that are really extended/optional features that can be punted?\nDo you proactively look for issues with your design, or do you need prodding from the interviewer?\nAre you able to assess different options and make trade-offs, or are you attached to certain ways of doing things?\nDo you have a good sense of what to prioritize, or do you get lost in the weeds?\nAre you able to produce an actual deliverable within the timeframe you are given? Does your design meet all functional and non-functional requirements? In particular, does it scale? While you are not expected to produce an industry-grade design in 45 minutes ー this would be quite unreasonable to ask ー what you produce should be something that can be turned over to a product team for implementation.\nAre you able to achieve the basic requirements quickly so that you have time to extend your design in an interesting way?\nAre you able to use speech, notes, and diagrams to communicate your ideas clearly to someone else? Are you able to take feedback?\nExamples Example 1 First. Single server setup. To start with something simple, everything is running on a single server. Figure 1-1 shows the illustration of a single server setup where everything is running on one server: web-app, database, cache, etc. To understand this setup, it is helpful to investigate the request flow and traffic source. For this, we need to move state (for instance user session data) out of the web tier. A good practice is to store session data in the persistent storage such as relational database or NoSQL. Each web server in the cluster can access state data from databases. This is called stateless web tier. Several technical challenges must be resolved to achieve multi-data center setup:\nJoin and de-normalization: Once a database has been sharded across multiple servers, it is hard to perform join operations across database shards. A common workaround is to denormalize the database so that queries can be performed in a single table. example. IgotAnOffer interview with exMeta senior topic: \u0026ldquo;Design Google Search\u0026rdquo;. link: https://www.youtube.com/watch?v=CsXrdpjCVFA interesting: interviewee start HLD with introducing an tier-based system. These tiers include: Web Tier, App Tier, Cache Tier, Data Tier, Batch compute platform (crawler), and Access Tier. Components include:\npremature optimization (adding cache, horizontal duplication, introducing queries) add some unused elements and services not usual names for system components (for example, dev panel) to read, view ? https://interviewing.io/blog/never-written-code-but-passed-google-system-design\nModel External Systems \u0026amp; Third-Party Dependencies Role: Any external services your system interacts with.\n🔹 Common Dependencies:\nPayment Providers (Stripe, PayPal) – Handling transactions. Authentication Services (OAuth, SSO, JWT) – User security. Cloud Infrastructure (AWS, GCP, Azure) – Storage, compute, networking. Third-Party APIs (Google Maps, Twilio, OpenAI API) – Enhancing functionality. 🔹 Considerations:\nReliability \u0026amp; SLAs – Can you handle downtime of third-party services? Latency \u0026amp; Performance – Are there rate limits or high response times? Security \u0026amp; Compliance – Are APIs handling sensitive data? Second Level (iteration) Structures First-Principles Thinking (Break It Down) (?) Why It Works:\nPrevents over-engineering and unnecessary complexity. Ensures decisions are based on need, not trend-following. The MVP (Minimum Viable Product) Model 🔹 What It Is:\nStart with the simplest functional solution and iterate based on constraints and scale.\nHow to Apply It:\n1️. Design a basic working system first (e.g., a monolith instead of microservices).\n2️. Add complexity only when justified (e.g., introduce sharding when a single database becomes a bottleneck).\n3️. Iterate based on trade-offs and scalability needs.\nWhy It Works:\n✔ Keeps the discussion structured and iterative.\n✔ Prevents over-engineering too early.\n^^ also maybe we can say like: ok, we have several scenatios for the future. One most probable of them is night-spikes of users. So one of the qualities of one subsystem is to hold this kind of load. We can address it now, but I\u0026rsquo;ll propose that it will be in our list of bottleneck (more common name) analysis and system improvement.\nThe Bottleneck-First Model 🔹 What It Is: Identify and eliminate bottlenecks first, instead of prematurely optimizing.\n🔹 How to Apply It: 1️⃣ Ask where the biggest bottleneck is. \u0026lt;- Maybe here we can find out most critical bottleneck for users? Future x critical bottlenecks? How we can change the design in the future to prevent this bottlenecks? Proactive\nCPU-bound? (Compute-heavy tasks need parallelization) I/O-bound? (Disk reads/writes need caching or distributed databases) Network-bound? (Latency needs CDNs and efficient protocols) 2️⃣ Prioritize optimizations that give the biggest gain. 🔹 Example: Q: \u0026ldquo;How would you handle millions of search queries per second in an e-commerce app?\u0026rdquo; A (Bottleneck-First Approach):\nBottleneck 1: Full-text search in databases → Introduce Elasticsearch or Solr. Bottleneck 2: High read traffic → Use Redis for caching popular searches. Bottleneck 3: High latency due to database queries → Precompute trending results to reduce load. 🔹 Why It Works: ✔ Focuses on real bottlenecks instead of premature optimization. ✔ Makes the solution more efficient incrementally.\nBottleneck-Driven Optimization (Performance-First Approach) 🔹 Goal: Identify and optimize bottlenecks first instead of prematurely optimizing non-critical components.\n🔹 Interview Strategy:\n1️⃣ Start with a baseline design (simple monolith, single DB).\n2️⃣ Identify the first bottleneck (ask interviewer for expected load, latency SLAs).\n3️⃣ Propose targeted optimizations (caching, sharding, replication).\n🔹 How to Use in an Interview:\n💬 \u0026ldquo;Before scaling the system, let\u0026rsquo;s identify where the biggest performance bottleneck is. Are we more concerned with read-heavy or write-heavy workloads?\u0026rdquo;\n🔹 Common Bottlenecks \u0026amp; Fixes:\nBottleneck Optimization Slow Database Queries Indexing, query optimization, read replicas High Read Traffic Caching (Redis, Memcached, CDN) API Overload Rate limiting, API Gateway Latency Issues Load balancing, multi-region deployment Write-heavy Workloads Sharding, partitioning todo apply reflection practices and improvement to sd interview, sd framework, Core Principles of Optimizing Systems Under Design In a system design interview, optimization is not just about improving performance—it\u0026rsquo;s about making informed trade-offs, balancing scalability, cost, reliability, and maintainability, while demonstrating a structured thought process.\n🔹 1. Keep it Simple, then Optimize – Start with a functional design, optimize as scale grows. 🔹 2. Measure Before Optimizing – Profile performance first using real metrics. 🔹 3. Balance Trade-offs – No system is 100% scalable, consistent, and cost-effective at the same time. 🔹 4. Optimize the Right Bottlenecks – Don\u0026rsquo;t waste effort on optimizations with little impact. 🔹 5. Design for Failure – Build resilient systems that degrade gracefully. 🔹 6. Think in Layers – Optimize frontend, backend, database, and network layers systematically. 🔹 7. Automate Scaling – Use auto-scaling and self-healing mechanisms.\nTrade-Off Analysis Tools Trade-Off Triangle The Trade-Off Triangle (CAP Theorem + Scalability vs. Consistency vs. Cost) 🔹 What It Is: Every system design decision trades off between three key constraints:\nScalability (Can the system handle increased load?) Consistency (Does every user see the latest data immediately?) Cost \u0026amp; Complexity (How expensive is it to maintain?) 🔹 How to Apply It:\nScaling reads? Use caching or read replicas. Scaling writes? Consider partitioning or sharding. Need strong consistency? Accept limited availability in network failures (CP system in CAP theorem). 🔹 Example: Q: \u0026ldquo;How would you design a global social media feed?\u0026rdquo; A (Applying Trade-offs): 1️⃣ If low latency is key → Choose eventual consistency (AP system, with caching). 2️⃣ If real-time accuracy is key → Choose strong consistency (CP system, but sacrifice speed). 3️⃣ Hybrid Approach: Prioritize eventual consistency for most users, but strong consistency for VIP users or trending posts.\n🔹 Why It Works: ✔ Shows awareness of real-world constraints. ✔ Justifies design choices with clear reasoning.\ntodo balance proactivity and premature optimization 50 Most Impactful Cognitive Biases in System Design Interviews Cognitive biases can lead candidates to poor system design decisions, suboptimal trade-offs, and ineffective communication during interviews. Here\u0026rsquo;s a structured list of 50 cognitive biases that affect candidates and their decision-making process in system design interviews.\nMost Influential Errors in System Design Interviews System design interviews assess architecture skills, trade-off analysis, scalability, and real-world problem-solving. Candidates often make critical errors that reduce their effectiveness. Below is a structured review of the most influential system design mistakes and how to avoid them.\n1️⃣ Errors in Understanding Requirements 1.1. Skipping Requirement Clarification Jumping straight into designing without asking clarifying questions. Assuming requirements instead of explicitly defining scale, constraints, and business goals. Not verifying expected read vs. write ratios, consistency vs. availability trade-offs, or peak load handling. ✅ How to Avoid:\nAsk who the users are, traffic expectations, and what problem we are solving before starting. Clarify functional (user-facing) and non-functional (performance, security) requirements. 💬 \u0026ldquo;Before we design, let\u0026rsquo;s define the scale: How many daily active users and peak requests per second?\u0026rdquo;\n1.2. Ignoring Business Context Designing purely from a technical perspective, ignoring cost constraints or business goals. Over-optimizing for performance when cost efficiency is more important. ✅ How to Avoid:\nAsk: \u0026ldquo;What are the business priorities? Are we optimizing for speed, cost, or maintainability?\u0026rdquo; Justify design choices based on business impact, not just technical feasibility. 💬 \u0026ldquo;If cost is a concern, we could use a single-region setup with caching instead of a multi-region deployment.\u0026rdquo;\n2️⃣ Errors in Architectural Thinking 2.1. Over-Engineering the Solution Designing for extreme scale when unnecessary. Using Kafka, Kubernetes, and microservices for a small-scale application. Adding too much redundancy and complexity when a simpler approach would suffice. ✅ How to Avoid:\nStart with a Minimal Viable Product (MVP) and optimize later. Prioritize simplicity, modularity, and maintainability. Ask: \u0026ldquo;What\u0026rsquo;s the simplest solution that meets current needs and scales later?\u0026rdquo; 💬 \u0026ldquo;For now, a single SQL database works. If traffic increases, we can introduce sharding.\u0026rdquo;\n2.2. Under-Engineering the Solution Ignoring scalability when the problem clearly requires it. Assuming a single database will handle all future growth. Not accounting for traffic spikes, failover, and data partitioning. ✅ How to Avoid:\nBalance simplicity with future-proofing by considering scale-up vs. scale-out strategies. Introduce sharding, replication, or caching only if justified. 💬 \u0026ldquo;If our user base grows beyond 100M, we can introduce sharding to distribute load.\u0026rdquo;\n2.3. Over-Focusing on One Layer Spending the entire interview on database design, neglecting network, caching, and failure handling. Not addressing load balancing, API gateways, or data flow between components. ✅ How to Avoid:\nUse a structured approach: frontend → backend → database → scaling → failures. Cover end-to-end architecture instead of hyper-focusing on one part. 💬 \u0026ldquo;We\u0026rsquo;ve covered storage, now let\u0026rsquo;s discuss API rate limiting and load balancing.\u0026rdquo;\n2.4. Not Handling Failures \u0026amp; Edge Cases Assuming everything will work perfectly and neglecting failure scenarios. No discussion on retry mechanisms, circuit breakers, or database failover. ✅ How to Avoid:\nAsk: \u0026ldquo;What happens if the primary database crashes?\u0026rdquo; Implement failover strategies, retries, and rate limiting. 💬 \u0026ldquo;If the primary DB goes down, the system will promote a read replica as the new primary.\u0026rdquo;\n3️⃣ Errors in Trade-Off Analysis 3.1. Making Technology Choices Without Justification Picking a specific database, queue system, or framework without explaining why. Always choosing PostgreSQL, Redis, or Kafka without discussing trade-offs. ✅ How to Avoid:\nCompare at least two alternatives for every major decision. Discuss pros, cons, and when each solution is best suited. 💬 \u0026ldquo;We could use Redis for caching, but Memcached would be simpler if we only need key-value lookups.\u0026rdquo;\n3.2. Ignoring Cost vs. Performance Trade-Offs Designing for low latency at the expense of massive infrastructure costs. Using multi-region replication when a single-region setup would suffice. ✅ How to Avoid:\nBalance cost, performance, and maintainability. Ask: \u0026ldquo;What are the cost constraints? Do we need this level of redundancy now?\u0026rdquo; 💬 \u0026ldquo;A global CDN would reduce latency, but it\u0026rsquo;s costly. Should we start with regional caching?\u0026rdquo;\n3.3. Poor Data Storage Choices Using SQL when NoSQL is a better fit (or vice versa). Not considering indexing, partitioning, or data modeling. ✅ How to Avoid:\nChoose storage based on data relationships, query patterns, and consistency needs. Discuss data volume, read vs. write patterns, and future growth. 💬 \u0026ldquo;Since our queries require flexible schema, NoSQL (MongoDB) fits better than a relational DB.\u0026rdquo;\n4️⃣ Errors in Communication \u0026amp; Collaboration 4.1. Not Thinking Out Loud Designing the system silently without explaining thought processes. Making big design jumps without discussing intermediate steps. ✅ How to Avoid:\nNarrate each design decision as you go. Regularly check in with the interviewer. 💬 \u0026ldquo;I\u0026rsquo;ll start with a monolith since it\u0026rsquo;s easier to manage, then discuss how we scale.\u0026rdquo;\n4.2. Ignoring Interviewer Hints Missing subtle cues that your solution is not aligned with the problem. Ignoring the interviewer\u0026rsquo;s questions or redirections. ✅ How to Avoid:\nTreat the interview as a collaborative discussion. If the interviewer asks a question, pause and reassess your approach. 💬 \u0026ldquo;I see your concern. Let\u0026rsquo;s consider an alternative design that better addresses fault tolerance.\u0026rdquo;\n4.3. Poor Time Management Spending too long on one part (e.g., database schema) and running out of time. Leaving no time for optimization, trade-offs, or failure scenarios. ✅ How to Avoid:\nFollow a structured time allocation: 5 mins: Requirements gathering 10-15 mins: High-level architecture 10 mins: Scaling, trade-offs, failure handling 5 mins: Recap and optimizations 💬 \u0026ldquo;We\u0026rsquo;ve covered scaling; let\u0026rsquo;s now discuss failover mechanisms.\u0026rdquo;\n🔹 Summary: Most Influential Errors \u0026amp; Fixes Error Type Common Mistake How to Avoid It Understanding Requirements Skipping clarifying questions Define scale, users, traffic early Over-Engineering Too complex for small systems Start simple, scale when needed Under-Engineering Ignoring scalability needs Plan for future growth logically Ignoring Trade-Offs Choosing tools without justification Compare at least two options Failing to Handle Failures No redundancy or failover plans Add retries, backups, monitoring Poor Time Management Spending too much time on one area Allocate time for each system part Bad Communication Not thinking out loud Narrate decisions, engage interviewer 1️⃣ Design \u0026amp; Architecture Biases Over-Engineering Bias – Designing for extreme scale before confirming actual requirements. Feature Creep Bias – Adding unnecessary complexity by focusing on rare edge cases too early. Familiarity Bias – Favoring technologies or architectures you already know instead of evaluating alternatives. Not-Invented-Here Bias – Preferring to build custom solutions instead of using existing tools or frameworks. Shiny Object Syndrome – Choosing new or trendy technologies over proven, stable solutions. Monolith vs. Microservices Bias – Defaulting to microservices without considering the benefits of monolithic architectures. Cloud-Native Assumption – Assuming every system must be cloud-based, ignoring on-premise or hybrid solutions. Scaling Prematurely Bias – Optimizing for millions of users when the system does not need it. Reinventing the Wheel Bias – Designing new algorithms or protocols when reliable ones already exist. The \u0026ldquo;Best Tool\u0026rdquo; Bias – Believing that a single database, programming language, or framework is the best for all situations. 2️⃣ Decision-Making \u0026amp; Trade-Off Biases Confirmation Bias – Seeking only evidence that supports your initial design choices while ignoring contradictions. Recency Bias – Overweighting the latest trends or experiences instead of considering older, well-established solutions. Sunk Cost Fallacy – Sticking with a bad design choice because you\u0026rsquo;ve already invested time in it. Status Quo Bias – Resisting changes to an existing design, even when improvements are evident. Optimism Bias – Underestimating system failures and assuming everything will work as expected. Pessimism Bias – Assuming worst-case scenarios for every decision, leading to overcomplicated designs. Risk Aversion Bias – Avoiding any risk in design, even when a higher-risk approach would provide better performance. Authority Bias – Blindly following the interviewer\u0026rsquo;s suggestions without critically evaluating them. Groupthink Bias – Defaulting to conventional or widely accepted architectures without questioning their applicability. Default Bias – Accepting the first reasonable solution without exploring alternatives. 3️⃣ Scalability \u0026amp; Performance Biases Big Data Bias – Assuming that all systems require distributed storage and advanced data partitioning. Strong Consistency Bias – Over-prioritizing data consistency at the expense of availability and scalability. Eventual Consistency Bias – Assuming that every large-scale system should prioritize availability over consistency. Zero-Downtime Bias – Designing for zero downtime even when some downtime would be acceptable. Latency Insensitivity Bias – Ignoring real-world latency issues in distributed systems. Throughput Overhead Bias – Assuming higher throughput is always better without considering cost. Redundancy Overkill Bias – Adding too much replication or backups, leading to unnecessary resource consumption. Caching Dependency Bias – Relying too heavily on caching without considering cache invalidation complexities. Over-Sharding Bias – Assuming sharding is necessary even for small-scale applications. Premature Optimization Bias – Optimizing performance before identifying real bottlenecks. 4️⃣ Security \u0026amp; Reliability Biases Perceived Security Bias – Assuming a system is secure just because best practices were followed. Compliance Overhead Bias – Overcomplicating security measures when simple protections would suffice. Single Point of Failure Neglect – Forgetting to address failure points in system design. Overconfidence in Failover Mechanisms – Assuming that redundancy alone guarantees high availability. Observability Blindness – Ignoring the need for logging, monitoring, and real-time alerts. False Sense of API Security – Assuming that API authentication alone prevents abuse. Ignoring User Privacy Bias – Not considering data privacy regulations like GDPR and HIPAA. Firewall and Perimeter Bias – Relying solely on network security measures instead of a zero-trust approach. Ignoring Insider Threat Bias – Assuming security risks come only from external threats. Trusting Third-Party Services Blindly – Failing to account for the risks of API rate limits, outages, or security vulnerabilities in third-party integrations. 5️⃣ Communication \u0026amp; Collaboration Biases Jargon Overuse Bias – Using overly technical language without confirming if the interviewer understands it. Excessive Detail Bias – Spending too much time on low-level implementation details instead of high-level architecture. Lack of Clarification Bias – Assuming system requirements without asking enough questions. Rigid Thinking Bias – Refusing to adapt your design after receiving feedback. Presentation Bias – Prioritizing how an idea sounds rather than how well it works. Overconfidence Bias – Overstating your expertise or making definitive statements without verifying facts. Underconfidence Bias – Hesitating to make firm design decisions out of fear of being wrong. Linear Thinking Bias – Failing to think in terms of parallelism, concurrency, or event-driven architectures. Ignoring Business Requirements Bias – Focusing purely on technical design while neglecting business needs. Time Mismanagement Bias – Spending too much time on one part of the system and running out of time for critical sections. A strong candidate actively challenges their biases and remains flexible in their approach. Naming an Approach That Accounts for Users and Their Needs When designing a system while considering users and their needs, different approaches can be used, depending on the context. Here are some common naming conventions that reflect this mindset:\n1️⃣ User-Centered Design (UCD) Definition:\nFocuses on users\u0026rsquo; behaviors, needs, and goals when designing a system. Ensures the system provides a good user experience (UX) while meeting functional requirements. Best For:\n✔ Frontend-heavy applications (e.g., e-commerce, social media).\n✔ APIs or platforms that prioritize developer experience (DX).\n2️⃣ Human-Centered System Design Definition:\nA broader approach that focuses on how people interact with technology and ensures the system adapts to human needs rather than forcing users to adapt. Best For:\n✔ Accessibility-focused applications.\n✔ AI/ML systems that involve human decision-making (e.g., recommendation systems).\n3️⃣ Requirement-Driven Design Definition:\nFocuses on gathering and analyzing explicit and implicit user requirements before defining the architecture. Best For:\n✔ Enterprise applications with complex workflows.\n✔ Business-to-business (B2B) software.\n4️⃣ Context-Aware System Design Definition:\nTakes into account the environment, usage patterns, and external constraints affecting users. Examples include geo-aware services (Google Maps), IoT systems, or AI-driven applications. Best For:\n✔ Location-based applications.\n✔ Systems that adapt to user context dynamically.\n5️⃣ Jobs-To-Be-Done (JTBD) Approach Definition:\nFocuses on what the user wants to accomplish, rather than just the features they need. Emphasizes user goals over system capabilities. Best For:\n✔ Consumer-facing applications where users have clear end goals.\n✔ Product development frameworks that prioritize outcomes over features.\n6️⃣ Demand-Driven Architecture Definition:\nDesigns the system around actual demand from users, ensuring that scalability, availability, and performance align with real-world usage. Best For:\n✔ Large-scale systems that need to handle variable traffic patterns (e.g., cloud services, e-commerce).\n✔ SaaS platforms that evolve based on user demand.\n7️⃣ Customer-First Engineering Definition:\nA software engineering philosophy that prioritizes customer impact, feedback loops, and iterative improvements. Best For:\n✔ Startups that iterate quickly based on customer feedback.\n✔ Companies using Agile methodologies and A/B testing to refine systems.\nFinal Thoughts: Choosing the Right Naming Approach The best term depends on your system design goals:\nIf prioritizing UX \u0026amp; usability: → User-Centered Design (UCD), Human-Centered System Design If focusing on business impact: → Jobs-To-Be-Done (JTBD), Requirement-Driven Design If scaling based on usage patterns: → Demand-Driven Architecture, Context-Aware System Design If iterating based on customer feedback: → Customer-First Engineering Each of these approaches ensures the system is built with real user needs in mind, balancing functionality, performance, and user experience.\n","date":"2025-02-22","id":62,"permalink":"/system-design/to_sort/sd_framework_old/","summary":"\u003ch1 id=\"to-refactor\"\u003eTo Refactor\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/watch?v=Gg318hR5JY0\"\u003ehttps://www.youtube.com/watch?v=Gg318hR5JY0\u003c/a\u003e\nanalyze this video from google\u003c/p\u003e\n\u003cp\u003ehow candidate reason about scalability and availability?\nframe an interviewer that you understand that your system would need to be distributed (in some scenarios)\ndesign the system to be scalable\u003c/p\u003e","tags":[],"title":"Sd Framework Old"},{"content":"tiny url offline key generation. We can have a standalone Key Generation Service (KGS) that generates random six letter strings beforehand and stores them in a database (let\u0026rsquo;s call it key-DB). Whenever we want to shorten a URL, we will just take one of the already-generated keys and use it. This approach will make things quite simple and fast. Not only are we not encoding the URL, but we won\u0026rsquo;t have to worry about duplications or collisions. [trade-offs] Concurrency problems. Can concurrency cause problems? As soon as a key is used, it should be marked in the database to ensure it doesn\u0026rsquo;t get used again. If there are multiple servers reading keys concurrently, we might get a scenario where two or more servers try to read the same key from the database. How can we solve this concurrency problem? Servers can use KGS to read/mark keys in the database. KGS can use two tables to store keys: one for keys that are not used yet, and one for all the used keys. As soon as KGS gives keys to one of the servers, it can move them to the used keys table.\nKGS can always keep some keys in memory so that it can quickly provide them whenever a server needs them.\nFor simplicity, as soon as KGS loads some keys in memory, it can move them to the used keys table. This ensures each server gets unique keys. If KGS dies before assigning all the loaded keys to some server, we will be wasting those keys–which is acceptable, given the huge number of keys we have. KGS also has to make sure not to give the same key to multiple servers. For that, it must synchronize (or get a lock on) the data structure holding the keys before removing keys from it and giving them to a server\nscheme with commiting?\ndropbox Internally, files can be stored in small parts or chunks (say 4MB); this can provide a lot of benefits i.e. all failed operations shall only be retried for smaller parts of a file. If a user fails to upload a file, then only the failing chunk will be retried.\nWe can reduce the amount of data exchange by transferring updated chunks only. By removing duplicate chunks, we can save storage space and bandwidth usage. Keeping a local copy of the metadata (file name, size, etc.) with the client can save us a lot of round trips to the server. For small changes, clients can intelligently upload the diffs instead of the whole chunk.\nHow can clients efficiently listen to changes happening with other clients? One solution could be that the clients periodically check with the server if there are any changes. The problem with this approach is that we will have a delay in reflecting changes locally as clients will be checking for changes periodically compared to a server notifying whenever there is some change. If the client frequently checks the server for changes, it will not only be wasting bandwidth, as the server has to return an empty response most of the time, but will also be keeping the server busy. Pulling information in this manner is not scalable. A solution to the above problem could be to use HTTP long polling. With long polling the client requests information from the server with the expectation that the server may not respond immediately. If the server has no new data for the client when the poll is received, instead of sending an empty response, the server holds the request open and waits for response information to become available. Once it does have new information, the server immediately sends an HTTP/S response to the client, completing the open HTTP/S Request. Upon receipt of the server response, the client can immediately issue another server request for future updates.\ndifferences The Synchronization Service should be designed in such a way that it transmits less data between clients and the Cloud Storage to achieve a better response time. To meet this design goal, the Synchronization Service can employ a differencing algorithm to reduce the amount of the data that needs to be synchronized. Instead of transmitting entire files from clients to the server or vice versa, we can just transmit the difference between two versions of a file. Therefore, only the part of the file that has been changed is transmitted. This also decreases bandwidth consumption and cloud data storage for the end user. As described above, we will be dividing our files into 4MB chunks and will be transferring modified chunks only. Server and clients can calculate a hash (e.g., SHA-256) to see whether to update the local copy of a 54chunk or not. On the server, if we already have a chunk with a similar hash (even from another user), we don\u0026rsquo;t need to create another copy, we can use the same chunk. This is discussed in detail later under Data Deduplication.\nTo be able to provide an efficient and scalable synchronization protocol we can consider using a communication middleware between clients and the Synchronization Service. The messaging middleware should provide scalable message queuing and change notifications to support a high number of clients using pull or push strategies. This way, multiple Synchronization Service instances can receive requests from a global request Queue, and the communication middleware will be able to balance its load.\ndata deduplication We can implement deduplication in two ways in our system: a. Post-process deduplication With post-process deduplication, new chunks are first stored on the storage device and later some process analyzes the data looking for duplication. The benefit is that clients will not need to wait for the hash calculation or lookup to complete before storing the data, thereby ensuring that there is no degradation in storage performance. Drawbacks of this approach are 1) We will unnecessarily be storing duplicate data, though for a short time, 2) Duplicate data will be transferred consuming bandwidth. b. In-line deduplication Alternatively, deduplication hash calculations can be done in real-time as the clients are entering data on their device. If our system identifies a chunk that it has already stored, only a reference to the existing chunk will be added in the metadata, rather than a full copy of the chunk. This approach will give us optimal network and storage usage.\nfacebook messenger At a high-level, we will need a chat server that will be the central piece, orchestrating all the communications between users. When a user wants to send a message to another user, they will connect to the chat server and send the message to the server; the server then passes that message to the other user and also stores it in the database.\nMessages Handling How would we efficiently send/receive messages? To send messages, a user needs to connect to the server and post messages for the other users. To get a message from the server, the user has two options:\nPull model: Users can periodically ask the server if there are any new messages for them. Push model: Users can keep a connection open with the server and can depend upon the server to notify them whenever there are new messages. If we go with our first approach, then the server needs to keep track of messages that are still waiting to be delivered, and as soon as the receiving user connects to the server to ask for any new message, the server can return all the pending messages. To minimize latency for the user, they have to check the server quite frequently, and most of the time they will be getting an empty response if there are no pending message. This will waste a lot of resources and does not look like an efficient solution. If we go with our second approach, where all the active users keep a connection open with the server, then as soon as the server receives a message it can immediately pass the message to the intended user. This way, the server does not need to keep track of the pending messages, and we will have minimum latency, as the messages are delivered instantly on the opened connection.\nHow will clients maintain an open connection with the server? We can use HTTP Long Polling or WebSockets. In long polling, clients can request information from the server with the expectation that the server may not respond immediately. If the server has no new data for the client when the poll is received, instead of sending an empty response, the server holds the request open and waits for responseinformation to become available. Once it does have new information, the server immediately sends the response to the client, completing the open request. Upon receipt of the server response, the client can immediately issue another server request for future updates. This gives a lot of improvements in latencies, throughputs, and performance. The long polling request can timeout or can receive a disconnect from the server, in that case, the client has to open a new request.\nHow can the server keep track of all the opened connection* to redirect messages to the users efficiently?\nThe server can maintain a hash table, where \u0026ldquo;key\u0026rdquo; would be the UserID and \u0026ldquo;value\u0026rdquo; would be the connection object. So whenever the server receives a message for a user, it looks up that user in the hash table to find the connection object and sends the message on the open request. What will happen when the server receives a message for a user who has gone offline? If the receiver has disconnected, the server can notify the sender about the delivery failure. If it is a temporary disconnect, e.g., the receiver\u0026rsquo;s long-poll request just timed out, then we should expect a reconnect from the user. In that case, we can ask the sender to retry sending the message. This retry could be embedded in the client\u0026rsquo;s logic so that users don\u0026rsquo;t have to retype the message. The server can also\nManaging user\u0026rsquo;s status We need to keep track of user\u0026rsquo;s online/offline status and notify all the relevant users whenever a status change happens. Since we are maintaining a connection object on the server for all active users, we can easily figure out the user\u0026rsquo;s current status from this. With 500M active users at any time, if we have to broadcast each status change to all the relevant active users, it will consume a lot of resources. We can do the following optimization around this:\nWhenever a client starts the app, it can pull the current status of all users in their friends\u0026rsquo; list. Whenever a user sends a message to another user that has gone offline, we can send a failure to the sender and update the status on the client. Whenever a user comes online, the server can always broadcast that status with a delay of a few seconds to see if the user does not go offline immediately. Client\u0026rsquo;s can pull the status from the server about those users that are being shown on the user\u0026rsquo;s viewport. This should not be a frequent operation, as the server is broadcasting the online status of users and we can live with the stale offline status of users for a while. Whenever the client starts a new chat with another user, we can pull the status at that time. twitter sharding Sharding based on UserID: We can try storing all the data of a user on one server. While storing, we can pass the UserID to our hash function that will map the user to a database server where we will store all of the user\u0026rsquo;s tweets, favorites, follows, etc. While querying for tweets/follows/favorites of a user, we can ask our hash function where can we find the data of a user and then read it from there. This approach has a couple of issues:\nWhat if a user becomes hot? There could be a lot of queries on the server holding the user. This high load will affect the performance of our service. Over time some users can end up storing a lot of tweets or having a lot of follows compared to others. Maintaining a uniform distribution of growing user data is quite difficult. To recover from these situations either we have to repartition/redistribute our data or use consistent hashing. We can further improve our performance by introducing cache to store hot tweets in front of the database servers.\ncaching\nyoutube Consistency can take a hit (in the interest of availability); if a user doesn\u0026rsquo;t see a video for a while, it should be fine.\nHow should we efficiently manage read traffic? We should segregate our read traffic from write traffic. Since we will have multiple copies of each video, we can distribute our read traffic on different servers. For metadata, we can have master- slave configurations where writes will go to master first and then gets applied at all the slaves. Such configurations can cause some staleness in data, e.g., when a new video is added, its metadata would be inserted in the master first and before it gets applied at the slave our slaves would not be able to see it; and therefore it will be returning stale results to the user. This staleness might be acceptable in our system as it would be very short-lived and the user would be able to see the new videos after a few milliseconds.\nWe have many options to shard our data. Let\u0026rsquo;s go through different strategies of sharding this data one by one:\nSharding based on UserID: We can try storing all the data for a particular user on one server. While storing, we can pass the UserID to our hash function which will map the user to a database server where we will store all the metadata for that user\u0026rsquo;s videos. While querying for videos of a user, we can ask our hash function to find the server holding the user\u0026rsquo;s data and then read it from there. To search videos by titles we will have to query all servers and each server will return a set of videos. A centralized server will then aggregate and rank these results before returning them to the user. This approach has a couple of issues:\nWhat if a user becomes popular? There could be a lot of queries on the server holding that user; this could create a performance bottleneck. This will also affect the overall performance of our service. Over time, some users can end up storing a lot of videos compared to others. Maintaining a uniform distribution of growing user data is quite tricky. To recover from these situations either we have to repartition/redistribute our data or used consistent hashing to balance the load between servers. rate-limiter Our system can get huge benefits from caching recent active users. Application servers can quickly check if the cache has the desired record before hitting backend servers. Our rate limiter can significantly benefit from the Write-back cache by updating all counters and timestamps in cache only. The write to the permanent storage can be done at fixed intervals. This way we can ensure minimum latency added to the user\u0026rsquo;s requests by the rate limiter. The reads can always hit the cache first; which will be extremely useful once the user has hit their maximum limit and the rate limiter will only be reading data without any updates.\ntwitter search To deal with hot tweets we can introduce a cache in front of our database. We can use Memcached, which can store all such hot tweets in memory. Application servers, before hitting the backend database, can quickly check if the cache has that tweet. Based on clients\u0026rsquo; usage patterns, we can adjust how many cache servers we need.\nthings sd language: trade-off, compromise; estimation, estimate, assumption; guess, valuate api gw it it not obvious if caching is a win. We estimated earlier that the dataset for the business table with 200M businesses is in the terabyte range. The dataset size is on the borderline where sharding might have make sense. For this table the update rate is low, and it\u0026rsquo;s read-heavy we should be able to get away without sharding if we put cache in front of it. This cache would take most of the read load of the frequency accessed businesses.\nIf read performance is the bottlenech we can add read replicas to help.\npayments https://www.youtube.com/watch?v=olfaBgJrUBI https://www.youtube.com/watch?v=zsD4R_aQctw https://www.youtube.com/watch?v=shipSEFMzHs https://www.youtube.com/watch?v=g8XqFuDkga0 https://newsletter.pragmaticengineer.com/p/designing-a-payment-system https://hackernoon.com/system-design-interview-designing-payment-systems-follow-up-questions-and-probable-issues https://hiringfor.tech/2020/05/11/system-design-practice-designing-a-payment-system.html https://devoxsoftware.com/blog/the-2022-manual-to-payment-system-architecture/ https://www.cockroachlabs.com/blog/cockroachdb-payments-system-architecture/\nfile storage why four-nines? not five nines? alternatives -\u0026gt; high availability in common as a AC (nfr) how to reason about read/write heavy property? in terms of latency: \u0026ldquo;very low latency\u0026rdquo;?\nbatch vs stream. cdc.\ntbd: the ac gived a hint: 20,000 rps. They gived this hint intentionally. Why? What do they want? there are 4 stages in meta: scoping, designing for scale, communication\nfinding nearby friends +tc clarified definition of \u0026rsquo;nearby' +tc clarified number of friends to show +tc shared an assumption that is out of scope for this discussion -he ask a question \u0026ldquo;how to interact with feature\u0026rdquo; -\u0026gt; instead you should get several options +tc asked average number of interaction from a user to this feature +tc clarified how much delay is acceptable in terms of real-time location of the user\nproximity servers Static Locations my notes: This service\u0026rsquo;s fundamental operation is searching. User should be able to search all nearby friends within a specified radius for any particular location. phrase: Given a user\u0026rsquo;s location, return top X points of interest near the user\n! static locations pipeline: design iteration (Naive Approach) -\u0026gt;\nfirst iteration lat, lon\nsecond iteration geohashing algorithms and proximity servers, geohash4, geohash5, and geohash6 \u0026hellip; find neighbours and query select .. like 'asdfn%' index on geohash\nthird iteration problems: LIKE can be slow; every request still makes a DB query which can be a bottleneck; diffucult to scale\nsolution: introduce 4, 5, 6 prefixes(and index these columns) and denormalizing solution 2: add business cache and 3 leveled caches (return ids of bussinesses with this geohash)\nwrite flow: introduce CDC in Kafka - cdc from mysql to kafka and then to \u0026lsquo;business post processing\u0026rsquo; service to write changes to caches\nlanguage: first pass\nhttps://www.youtube.com/watch?v=UCaVJsyq8ac\ndynamic locations Other problem: global counter. https://www.youtube.com/watch?v=V1HlNh4IhUo https://systemdesign.one/distributed-counter-system-design/\nac analyzing signals:\nclarified strategy of burger assignment -\u0026gt; E6 kind of signal reasoning about single region vs cross region is good mentioned bloom filter at an early stage -\u0026gt; low-level detail -\u0026gt; he want to use Bloom filter instead of designing a proper solution you should involve interviewer more. don\u0026rsquo;t do decision on behalf of customers. So you can identify trade-offs, then describe pros and cons and then ask a mention of ac. \u0026ldquo;what do you think it\u0026rsquo;s important for our problem?\u0026rdquo; tc slightly release on of the requirements: counter could be less than 6M -\u0026gt; good signal for E6 (why?) tc didn\u0026rsquo;t handle the critical part of the system which is the race conditions and how the things would be handled 23.12.23\n\u0026ndash; communication: tc iterated on the fact that marketing compain is the key feature of the design tc stated their assumption that each user has one account tc clarified if they should focus on tracking the order infrastructure\nuser experience when post promo code\nphrase. tc also identified the trade-off that they can have the API as sync and async. TC also identified the pros and cons in terms of technical simplicity and user experience.\ntechnical. tc talked about the rate-limiting to avoid the abusing the system.\nsync/async red flag: tc could have involved the interviewer while making the decision about sync vs async.\nmost common trade-offs: api, sql/nosql, caching, scaling, core problem, push/pull, sync/async\nnot all numbers are important. qps is always important. storage numbers usually important. interview: get several solutions, and explicitly say about trade-offs make several deep-dive moments (db choises, db sizes, connection with)\nac(nfr) -\u0026gt; what is the implications? for example, why latency p95 200 ms? why 3.2GB/day explain for an ac it\u0026rsquo;s meaning example. 40x10^7 -\u0026gt; horizontal scaling? connect you discussion with numbers and acs(nfr)\nget feedback on milestones write down your discussion\nefficiency play:\nyou can explain api, schema etc in a flow \u0026ldquo;I\u0026rsquo;m going to explain thing end to end. You know, I\u0026rsquo;m going to explain API while I\u0026rsquo;m talking about the flow.\u0026rdquo;\ncore puzzle what is the core puzzle. for example, for live comments this is a push vs pull trade-off examples: push/pull in live commenting, save/process photos on instagram surfing, algorithm connecting rider and driver\nspikes\npush/pull and dt, frequency, numbers considerations. (example - I cannot use pull - dt \u0026lt; 100 ms)\nfan-out\ncoordination communication\nranking algorithms? feed construction CUCKOO vs BLOOM filter\neventual consistency patterns consistensy model https://www.scylladb.com/glossary/consistency-models/\n21.12.23\nIt\u0026rsquo;s a good practice to include a brief point about cache invalidation during system design interviews.\n","date":"2025-02-22","id":63,"permalink":"/system-design/projects/sd_reviews/","summary":"\u003ch1 id=\"tiny-url\"\u003etiny url\u003c/h1\u003e\n\u003cp\u003eoffline key generation.\nWe can have a standalone Key Generation Service (KGS) that generates random six letter strings \u003cem\u003ebeforehand\u003c/em\u003e and \u003cem\u003estores\u003c/em\u003e them in a database (let\u0026rsquo;s call it key-DB). Whenever we want to shorten a URL, we will just take one of the already-generated keys and use it.\nThis approach will make things quite simple and fast. Not only are we not encoding the URL, but we won\u0026rsquo;t have to worry about duplications or collisions. [trade-offs]\nConcurrency problems.\nCan concurrency cause problems? As soon as a key is used, it should be marked\nin the database to ensure it doesn\u0026rsquo;t get used again. If there are multiple servers\nreading keys concurrently, we might get a scenario where two or more servers try to\nread the same key from the database. How can we solve this concurrency problem?\nServers can use KGS to read/mark keys in the database. KGS can use two tables to\nstore keys: one for keys that are not used yet, and one for all the used keys. As soon\nas KGS gives keys to one of the servers, it can move them to the used keys table.\u003c/p\u003e","tags":[],"title":"Sd Reviews"},{"content":"Various Think Big - Start Small\nFake it until you can make it\nThe ivory-tower architect most enjoys an end-state vision of ringing crystal perfection, but the pragmatic architect constantly thinks about the dynamics of change.\nA new architect will focus on the boxes; an experienced one is more interested in the arrows.\nThus, the set of architectural structures is neither fixed nor limited. What is architectural depends on what is useful to reason about in your context for your system. This means that architecture specically and intentionally omits certain information about elements that is not useful for reasoning about the system.\nThere is a reasoning behind the design of Kafka and Samza, which allow complex applications to be built by composing a small number of simple primitives – replicated logs and stream operators. #architecture\nPicking the right architecture = Picking the right battles + Managing trade-offs\nRobustness principle. Be conservative in what you do, be liberal in what you accept from others. #engineering\nThis also makes testing useful as a design tool #engineering\nFundamentals of SA \u0026amp; SA Hard Parts Because virtually every problem presents novel challenges, the real job of an architect lies in their ability to objectively determine and assess the set of trade-offs on either side of a consequential decision to resolve it as well as possible.\nOther Meilir Page-Jones made the astute observation that coupling in architecture may be split into static and dynamic coupling. Static coupling refers to the way architectural parts (classes, components, services, and so on) are wired together: dependencies, coupling degree, connection points, and so on. Dynamic coupling refers to how architecture parts call one another: what kind of communication, what information is passed, strictness of contracts, and so on.\nATAM process\nArchitectural modularity: increased scalability is only one benefit of architectural modularity. Another important benefit is agility, the ability to respond quickly to change.\nModularity Drivers Architects shouldn\u0026rsquo;t break a system into smaller parts unless clear business drivers exist. The primary business drivers for breaking applications into smaller parts include speed-to-market (sometimes called time-to-market) and achieving a level of competitive advantage in the marketplace.\nElasticity relies on services having a very small mean time to startup (MTTS), which is achieved architecturally by having very small, fine-grained services.\nLaws Why is more important than how Everything in software architecture is a trade-off.\nArchitectural characteristics Architects may collaborate on defining the domain or business requirements, but one key responsibility entails defining, discovering, and otherwise analyzing all the things the software must do that isn\u0026rsquo;t directly related to the domain functionality: architectural characteristics (AC).\nWhen designing an application, the requirements specify what the application should do; architecture characteristics specify operational and design criteria for success, concerning how to implement the requirements and why certain choices were made. For example, a common important architecture characteristic specifies a certain level of performance for the application, which often doesn\u0026rsquo;t appear in a requirements document. Even more pertinent: no requirements document states \u0026ldquo;prevent technical debt,\u0026rdquo; but it is a common design consideration for archi‐ tects and developers.\nMain goal: Thus, a critical job for architects lies in choosing the fewest architecture characteristics rather than the most possible.\nTrade-Offs and Least Worst Architecture Applications can only support a few of the architecture characteristics we\u0026rsquo;ve listed for a variety of reasons. First, each of the supported characteristics requires design effort and perhaps structural support. Second, the bigger problem lies with the fact that each architecture characteristic often has an impact on others. For example, if an architect wants to improve security, it will almost certainly negatively impact performance. More often, the decisions come down to trade-offs between several competing concerns.\nOur goal is to investigate how to do trade-off analysis in distributed architectures; to do that, we must pull the moving pieces apart so that we can discuss them in isolation to understand them fully before putting them back together.\nDistributed architectures like microservices are difficult, especially if architects cannot untangle all the forces at play. What we need is an approach or framework that helps us figure out the hard problems in our architecture\nProblem with decoupling: Architects design fine-grained microservices to achieve decoupling, but then orchestration, transactionality, and asynchronicity become huge problems. Until now, architects lacked the correct perspective and terminology to allow a careful analysis that could determine the best (or least worst) set of trade-offs on a case-by-case basis.\nProcesses of SD Trade-offs Analysis To analyze trade-offs, an architect must first determine what forces need to trade off with each other.\nArchitectural Characteristics Identifying the driving architectural characteristics is one of the first steps in creating an architecture or determining the validity of an existing architecture.\nUnderstanding the key domain goals and domain situation allows an architect to translate those domain concerns to \u0026ldquo;-ilities,\u0026rdquo; which then forms the basis for correct and justifiable architecture decisions.\nOne tip when collaborating with domain stakeholders to define the driving architecture characteristics is to work hard to keep the final list as short as possible. A common anti-pattern in architecture entails trying to design a generic architecture, one that supports all the architecture characteristics. Each architecture characteristic the architecture supports complicates the overall system design; supporting too many architecture characteristics leads to greater and greater complexity before the architect and developers have even started addressing the problem domain, the original motivation for writing the software. Don\u0026rsquo;t obsess over the number of charateristics, but rather the motivation to keep design simple.\nA better approach is to have the domain stakeholders select the top three most important characteristics from the final list (in any order).\nComponent Design Components form the fundamental modular building block in architecture, making them a critical consideration for architects. In fact, one of the primary decisions an architect must make concerns the top-level partitioning of components in the architecture.\nArchitects, often in collaboration with other roles such as developers, business analysts, and subject matter experts, create an initial component design based on general knowledge of the system and how they choose to decompose it, based on technical or domain partitioning. The team goal is an initial design that partitions the problem space into coarse chunks that take into account differing architecture characteristics.\nApproaches:\nactors/actions approach. event storming workflow approach Choose architecture style Choosing an architecture style represents the culmination of analysis and thought about tradeoffs for architecture characteristics, domain considerations, strategic goals, and a host of other things.\nAnalysing architecture risks Analyzing architecture risk is one of the key activities of architecture.\n4+1 architecture model To successfully design, analyze, and evolve software, developers must consider all the coupling points that could break. Other Distributed Considerations In addition to the eight fallacies of distributed computing previously described, there are other issues and challenges facing distributed architecture that aren\u0026rsquo;t present in monolithic architectures:\ndistributed logging distributed transactions contract maintenance and versioning Styles Data architecture concerns There is separation between separation between operational versus analytical data.\nUnderstanding Distributed Systems Building reliable abstractions on top of unreliable ones is a common pattern we will encounter again in the rest of the book. For example, this protocol could be TCP. TCP\u0026rsquo;s reliability and stability come at the price of lower bandwidth and higher latencies than the underlying network can deliver\nFailure detection Pings and heartbeats are generally used for processes that interact with each other frequently, in situations where an action needs to be taken as soon as one of them is no longer reachable. In other circumstances, detecting failures just at communication time is good enough.\nTime But in a distributed system, there is no shared global clock that all processes agree on that can be used to order operations. And, to make matters worse, processes can run concurrently.\nThis happened-before relationship creates a causal bond between the two operations, since the one that happens first can have side-effects that affect the operation that comes after it. We can use this intuition to build a different type of clock that isn\u0026rsquo;t tied to the physical concept of time but rather captures the causal relationship between operations: a logical clock.\nA vector clock is a logical clock that guarantees that if a logical timestamp is less than another, then the former must have happened-before the latter. A vector clock is implemented with an array of counters, one for each process in the system. And, as with Lamport clocks, each process has its local copy.\nLeader election There are times when a single process in the system needs to have special powers, like accessing a shared resource or assigning work to others. To grant a process these powers, the system needs to elect a leader among a set of candidate processes, which remains in charge until it relinquishes its role or becomes otherwise unavailable. When that happens, the remaining processes can elect a new leader among themselves.\nA leader election algorithm needs to guarantee that there is at most one leader at any given time and that an election eventually completes even in the presence of failures. These two properties are also referred to as safety and liveness, respectively, and they are general properties of distributed algorithms. Informally, safety guarantees that nothing bad happens and liveness that something good eventually does happen.\nAlthough having a leader can simplify the design of a system as it eliminates concurrency, it can also become a scalability bottleneck if the number of operations performed by it increases to the point where it can no longer keep up. Also, a leader is a single point of failure with a large blast radius; if the election process stops working or the leader isn\u0026rsquo;t working as expected, it can bring down the entire system with it. We can mitigate some of these downsides by introducing partitions and assigning a different leader per partition, but that comes with additional complexity. This is the solution many distributed data stores use since they need to use partitioning anyway to store data that doesn\u0026rsquo;t fit in a single node.\nFlow control Flow control is a backoff mechanism that TCP implements to prevent the sender from overwhelming the receiver. The receiver stores incoming TCP segments waiting to be processed by the application into a receive buffer. Assuming it\u0026rsquo;s respecting the protocol, the sender avoids sending more data than can fit in the receiver\u0026rsquo;s buffer.\nDDIA But the basic idea is still the same: each layer hides the complexity of the layers below it by providing a clean data model (27)\nOther databases at that time forced application developers to think a lot about the internal representation of the data in the database. The goal of the relational model was to hide that implementation detail behind a cleaner interface.\n(NoSQL) They embrace schemaless data, run on clusters, and have the ability to trade off traditional consistency for other useful properties. (Martin Fowler cite)\nThe JSON representation has better locality than the multi-table schema (32). It\u0026rsquo;s worth pointing out that the idea of grouping related data together for locality is not limited to the document model (41).\nEvolution of a data model: moreover, even if the initial version of an application fits well in a join-free document model, data has a tendency of becoming more interconnected as features are added to applications.\nThe ease of changes to be made to an application\u0026rsquo;s data model. \u0026lt;- data models comparison\n?? schema changes and migrations ?? joins in Document DBs Part II \u0026hellip; In this part of the book, we focus on shared-nothing architectures—not because they are necessarily the best choice for every use case, but rather because they require the most caution from you, the application developer. If your data is distributed across multiple nodes, you need to be aware of the constraints and trade-offs that occur in such a distributed system—the database cannot magically hide these from you.\nWith an understanding of those concepts, we can discuss the difficult trade-offs that you need to make in a distributed system.\nHowever, you do need to know how your software reacts to network problems and ensure that the system can recover from them.\nWe will now continue along the same lines, and seek abstractions that can allow an application to ignore some of the problems with distributed systems. For example, one of the most important abstractions for distributed systems is consensus: that is, getting all of the nodes to agree on something.\nHopre - Architect As our world is becoming more complex and difficult to understand, telling stories is one of the best ways to engage and teach.\nAn architect shouldn\u0026rsquo;t ignore production issues, which provide valuable feedback into possible architectural weaknesses.\nMaking decisions is important, but I also believe in Martin Fowler\u0026rsquo;s conclusion that \u0026ldquo;one of an architect\u0026rsquo;s most important tasks is to eliminate irreversibility in software designs, avoiding the \u0026ldquo;big\u0026rdquo; decisions that cannot be easily reversed.\nWhen asked to characterize the seniority of an architect, I resort to a simple framework that I believe applies to most high-end professions\nAs I converse with colleagues about what distinguishes a great architect, we often identify rational and disciplined decision-making as a key factor in translating skill into impact.\nDesigning fine-grained microservices Key Concepts of Microservices Independent Deployability. Independent deployability is the idea that we can make a change to a microservice, deploy it, and release that change to our users, without having to deploy any other microservices. More important, it\u0026rsquo;s not just the fact that we can do this; it\u0026rsquo;s that this is actually how you manage deployments in your system. It\u0026rsquo;s a discipline you adopt as your default release approach. Modeled Around a Business Domain. Rolling out a feature that requires changes to more than one microservice is expensive. You need to coordinate the work across each service (and potentially across sep‐ arate teams) and carefully manage the order in which the new versions of these services are deployed. That takes a lot more work than making the same change inside a single service (or inside a monolith, for that matter). It therefore follows that we want to find ways to make cross-service changes as infrequent as possible. Owning Their Own State. If a microservice wants to access data held by another microservice, it should go and ask that second microservice for the data. This gives the microservices the ability to decide what is shared and what is hidden, which allows us to clearly separate functionality that can change freely (our internal implementation) from the functionality that we want to change infrequently (the external contract that the consumers use). Microservices embrace the concept of information hiding. Information hiding means hiding as much information as possible inside a component and exposing as little as possible via external interfaces. This allows for clear separation between what can change easily and what is more difficult to change. Implementation that is hidden from external parties can be changed freely as long as the networked interfaces the microservice exposes don\u0026rsquo;t change in a backward-incompatible fashion. C CI Integrate early, and integrate often. Avoid the use of long-lived branches for feature development, and consider trunk-based devel‐ opment instead. If you really have to use branches, keep them short!\n","date":"2025-02-22","id":64,"permalink":"/system-design/to_sort/sd_to_sort/","summary":"\u003ch1 id=\"various\"\u003eVarious\u003c/h1\u003e\n\u003cp\u003eThink Big - Start Small\u003c/p\u003e\n\u003cp\u003eFake it until you can make it\u003c/p\u003e\n\u003cp\u003eThe ivory-tower architect most enjoys an end-state vision of ringing crystal perfection, but the pragmatic architect constantly thinks about the \u003cstrong\u003edynamics of change\u003c/strong\u003e.\u003c/p\u003e","tags":[],"title":"Sd To Sort"},{"content":"","date":"2025-02-22","id":65,"permalink":"/system-design/topics/security/","summary":"","tags":[],"title":"Security"},{"content":"","date":"2025-02-22","id":66,"permalink":"/projects/common/speech_improver/","summary":"","tags":[],"title":"Speech Improver"},{"content":"","date":"2025-02-22","id":67,"permalink":"/system-design/projects/spotify/","summary":"","tags":[],"title":"Spotify"},{"content":"Dictionary today\u0026rsquo;s mission-critical application event streaming platform data-driven organizations\nKafka as a Central Nervous System Kafka gives you a really different way of thinking about data Kafka is necessary but not sufficient for building modern data pipelines Kafka + Schema ~ Rest + Json\nSources https://gist.github.com/aviflax/7f453a41a06a200a2f5d https://medium.com/microservicegeeks/best-books-on-apache-kafka-and-event-stream-processing-c0d267e352bd\nEvents etc investigate topic: events, event-driven architecture, event streams real time data streams with Kafka stateless service and caches Data Mesh and Data as a Product. contract, schema, api.\ndata pipelines done right data pipeline technologies modern data pipelines data flow streaming pipelines\n5 principles of modern data flow:\nStreaming - reality is real time: organization really wants to what is going on right now. Decentralized Declarative (ksqlDB SQL syntax) Developer-Oriented (ksqlDB pipelines as code, can be checked into git) Governed \u0026amp; Observable You should pick and white down links to info source when writing an article topic: Log abstraction and its possibilities in data engineering architecture Event-driven architecture vs. event streaming Both event-driven architecture and event streaming center around events. Events are records of something that occurred, such as a mouse click, keystroke or loading a program. The difference in the platforms is in how the events are received.\nEvent Driven Architecture (EDA) Event common definition definition from DDD\nStreams The big advantage of storing raw event data is that you have maximum flexibility for analysis. For example, you can trace the sequence of pages that one person visited over the course of their session. You can\u0026rsquo;t do that if you\u0026rsquo;ve squashed all the events into counters. That sort of analysis is really important for some offline processing tasks such as training a recommender system (e.g., \u0026ldquo;people who bought X also bought Y\u0026rdquo;). For such use cases, it\u0026rsquo;s best to simply keep all the raw events so that you can later feed them all into your shiny new machine-learning system.\nOn the other hand, the aggregated data is the form in which it\u0026rsquo;s ideal to read data from the database. If a customer is looking at the contents of their shopping cart, they are not interested in the entire history of modifications that led to the current state: they only want to know what\u0026rsquo;s in the cart right now. An analytics application normally doesn\u0026rsquo;t need to show the user the full list of all page views - only the aggregated summary in the form of a chart. Thus, when you\u0026rsquo;re reading, you can get the best performance if the history of changes has already been squashed together into a single object representing the current state.\nIn general, the form of data that\u0026rsquo;s best optimized for writing is not the same as the form that is best optimized for reading. It can thus make sense to separate the way you write to your system from the way you read from it (this idea is sometimes known as command-query responsibility segregation, or CQRS).\nImmutable Facts and the Source of Truth From the Twitter and Facebook examples we can see a certain pat tern: the input events, corresponding to the buttons in the user interface, are quite simple. They are immutable facts, we can simply store them all, and we can treat them as the source of truth.\nYou can derive everything that you can see on a website - that is, everything that you read from the database - from those raw events. There is a process that derives those aggregates from the raw events, and which updates the caches when new events come in, and that process is entirely deterministic. You could, if necessary, re-run it from scratch: if you feed in the entire history of everything that ever happened on the site, you can reconstruct every cache entry to be exactly as it was before. The database you read from is just a cached view of the event log.\nThe beautiful thing about this separation between source of truth and caches is that in your caches, you can denormalize data to your heart\u0026rsquo;s content. In regular databases, it is often considered best practice to normalize data, because if something changes, you then only need to change it one place. Normalization makes writes fast and simple, but it means you must do more work (joins) at read time. To speed up reads, you can denormalize data; that is, duplicate information in various places so that it can be read faster.\nDenormalization Also, denormalization is just another form of duplicating data, similar to caching—if some value is too expensive to recompute on reads, you can store that value somewhere, but now you need to also keep it up-to-date when the underlying data changes. Materialized aggregates, such as those in the analytics example in Chapter 1, are again a form of redundant data.\nI\u0026rsquo;m not saying that this duplication of data is bad—far from it. Caching, indexing, and other forms of redundant data are often essential for achieving good performance on reads. However, keeping the data synchronized between all these various different representations and storage systems becomes a real challenge.\nStream processing A key characteristic of stream processing is that the events are processed as soon as (or almost as soon as) they are available. This is to minimize the latency between the original event\u0026rsquo;s entrance into the streaming system and the end result from processing the event. In most cases, the latency varies from a few milliseconds to seconds, which can be considered real-time or near real-time; hence, stream processing is also called real-time processing.\nThis is definition from Grokking - What is real-time here? We can do batching with near-real timings as well.\nWe would like streaming systems to process the events and generate results as soon as possible after the events are collected. This is desirable because it allows the results to be available with minimal delays and the proper reactions to be performed in time. Their real-time nature makes streaming systems very useful in many scenarios, such as the laboratory and the website, where low-latency results are desired.\nThe multi-stage architecture in batch and stream processing systems\nStreaming systems If we remove the latency with the request/response model, I bet we can handle the traffic and keep an accurate real-time count of vehicles. One benefit is that streaming systems will handle this type of data flow better than the request/response model.\nPatterns Example Framework Your job or components don\u0026rsquo;t run by themselves. They are driven by a streaming engine. Let\u0026rsquo;s take a look under the hood and inspect how your job is executed by the Streamwork engine. There are three moving parts (at the current state), and we are going to look into them one by one: source executor, operator executor, and job starter.\nDuring the job execution, you\u0026rsquo;ve hopefully noticed the events automatically move from the sensor reader object to the vehicle counter object without you needing to implement any additional logic. Fancy, right?\nterms: the life of a data element simple streaming engine\nElements\nJob, also known as a Pipeline or a Topology, is an implementation of a streaming system. A job is composed of components (sources and operators) and streams connecting the components.\nhttps://developer.confluent.io/patterns/ - event streaming patterns from Confluent microservices patterns in java ddd by vlad\nfunction fancyAlert(arg) { if(arg) { $.facebox({div:\u0026#39;#foo\u0026#39;}) } } ","date":"2025-02-22","id":68,"permalink":"/system-design/topics/stream_processing/","summary":"\u003ch1 id=\"dictionary\"\u003eDictionary\u003c/h1\u003e\n\u003cp\u003etoday\u0026rsquo;s mission-critical application\nevent streaming platform\ndata-driven organizations\u003c/p\u003e\n\u003cp\u003eKafka as a Central Nervous System\nKafka gives you a really different way of thinking about data\nKafka is necessary but not sufficient for building modern data pipelines\nKafka + Schema ~ Rest + Json\u003c/p\u003e","tags":[],"title":"Stream Processing"},{"content":"Sources and Links https://trueaccord.atlassian.net/wiki/spaces/DS/pages/815105668/Data+Streaming+overview\nhttps://drive.google.com/file/d/1wyP1aUBAwhLLonENwIJjVXxXWyUfJ5nN/view https://drive.google.com/file/d/1QeIte_uFbKYlPr4bIVuBVQCx1vWSb0l_/edit https://trueaccord.atlassian.net/wiki/spaces/DS/pages/815105668/Data+Streaming+overview\nhttps://trueaccord.atlassian.net/wiki/spaces/DS/pages/815727051/Glossary\nold. Streaming Platform overview link\nlocal repos? streaming/platform\u0026hellip;\nSources: \u0026ldquo;Data Streaming\u0026rdquo; space slack channel\nhttps://trueaccord.atlassian.net/wiki/x/_QRmbg https://trueaccord.atlassian.net/wiki/spaces/DS/settings/home\nhttps://systemdesignschool.io/fundamentals/stream-processing https://aws.amazon.com/what-is/batch-processing/ https://www.youtube.com/watch?v=1xgBQTF24mU\nreally interesting analysis https://www.reddit.com/r/ExperiencedDevs/comments/183f70f/tradeoffs_between_batch_processing_and_streaming/\nreviews: https://www.kai-waehner.de/blog/2023/12/21/the-data-streaming-landscape-2024/\nkai has also various use cases review https://www.kai-waehner.de/blog/2024/11/28/data-streaming-in-healthcare-and-pharma-use-cases-cardinal-health/ https://www.kai-waehner.de/blog/2022/04/04/real-time-analytics-machine-learning-with-apache-kafka-in-the-healthcare-industry/ https://www.kai-waehner.de/blog/2022/03/28/apache-kafka-data-streaming-healthcare-industry/\nkappa vs lambda https://www.kai-waehner.de/blog/2021/09/23/real-time-kappa-architecture-mainstream-replacing-batch-lambda/ https://www.youtube.com/watch?v=j7D29eyysDw kappa is a single real-time pipeline?\nvideos: kafka summit conference\nproduct RedPanda documentation https://www.redpanda.com/blog/streaming-data-examples-best-practices-tools https://www.redpanda.com/blog/data-streaming-architecture https://www.redpanda.com/blog/batch-vs-streaming-data-processing https://www.redpanda.com/blog/kafka-stream-processors\nand many tutorials here https://www.redpanda.com/blog/monetize-real-time-ads-gaming https://www.redpanda.com/blog/build-inventory-management-flink-mongodb-redpanda\nBlogs Uber Engineering Blog. Articles regarding streaming and events.\nBooks \u0026ldquo;Grokking Streaming\u0026rdquo; Hueske \u0026ldquo;Stream Processing with Apache Flink: Fundamentals, Implementation, and \u0026quot; examples?\non kafka:\nFor Kafka Streams \u0026ldquo;Kafka Streams in Action\u0026rdquo; 2nd edition\nto sort Stream processing approach frequently has been comparing with batch processing. So we can use it and find some definitive features and other things from this comparing. To do what? (next -\u0026gt;) I can see that this comparison exists on two levels: 1. stream vs batch on communication level; 2. Kappa vs Lambda architecture;\nData Streaming Use Cases by Business Value\ntopic: streaming ETL?\nalso there is a topic of some foundations: foundations-streaming library and tasks from DeGoe. You can search for other fundamental things also.\nCommunity \u0026amp; Events https://www.redpanda.com/streamfest #todo register\nregister fro https://events.confluent.io/5thingsyouneedtoknow2025\nActions write your own vision of current and future state of the streaming platform ask other teams and people \u0026hellip; communicate to them\ncollaborate\ncommunication\ntopics:\nfuture problems Future Problems prevention\npractices\nFlow? Exercise One: \u0026ldquo;Vision Before Reference\u0026rdquo; Without any prior research, challenge yourself to define \u0026ldquo;streaming\u0026rdquo;, \u0026ldquo;streaming platform\u0026rdquo;, and other key concepts: their characteristics, approaches, and typical components — purely based on your current knowledge and intuition. Work entirely from your current knowledge. Create an \u0026lsquo;Ontology Map\u0026rsquo; (reflection: tried to systemize approach, #todo define) #todo rewrite this\nThen compare these to industry-standard definitions.\nWhat\u0026rsquo;s the benefit? By contrasting your ideas with established definitions, you can:\nBuild on your foundational knowledge. Advance your learning strategy Iteration One Let\u0026rsquo;s begin with the concept of a \u0026ldquo;Stream.\u0026rdquo; The first defining characteristic that comes to mind is its endlessness—a stream flows without a defined endpoint. Another key feature is its continuity, where elements appear in a steady sequence without significant interruptions.\n(try to draw it on the paper). Producer emits messages to store. These elements are messages or events depending on the level of abstraction..\nContrasting (comparing to existing)\nAs a senior backend developer, when we consider the concept of a \u0026ldquo;Stream,\u0026rdquo; the first defining feature is its unbounded or infinite nature—it represents a continuous flow of data without a predetermined endpoint. Additionally, a stream is characterized by its uninterrupted continuity, where elements are processed in a steady, sequential manner without significant pauses or delays. This seamless flow is essential for real-time data processing, allowing for efficient handling of data as it arrives.\nOk, we contrasted these. But what knowledge have we gotten? \u0026hellip;\nIdentify several use cases and corresponding user requirements. Analyze the limitations and shortcomings of existing approaches in these scenarios. Use the identified user requirements as a foundation to develop and justify the adoption of the streaming method.\nUse Specific and Concrete Example\nRecommendation Engine Example (comparing approaches) Other thing that came up: why one should use it? Which problems can it solve? Trade offs? So you can consider \u0026lsquo;streaming\u0026rsquo; approach as a tool and approach to solve some problems.\nnear real-time insights\nReflection This type of exercise can be broadly classified under reflective learning or self-assessment exercises. The primary goal is to encourage learners to engage with their prior knowledge, articulate their understanding, and then refine it by comparing it with authoritative sources.\n(other definition) Constructivist Learning Activity Definition: An exercise rooted in constructivist learning theory, where individuals construct their understanding of a topic before comparing it to shared or formalized knowledge.\nConstructivism is the theory that says learners construct knowledge rather than just passively take in information. As people experience the world and reflect upon those experiences, they build their own representations and incorporate new information into their pre-existing knowledge (schemas)\nreflection2: really use it helped me to understand that this kind of exercise \u0026lsquo;create first, then evaluate and contrast to established standards\u0026rsquo; is already exist as a part of Constructivism Learning\nThe Importance of Specific Examples When attempting to compare streaming to other approaches, I found myself stuck with vague ideas and struggling to identify anything concrete. Even when I tried to create a diagram, it ended up being overly abstract, with generic producers, messages, and events—I couldn\u0026rsquo;t even name an intermediate buffer.\nTo address this, I decided to focus on constructing and using a specific, concrete example. This approach provided the clarity needed for meaningful comparison and better understanding.\nthese are example of\nCase-Based Learning (CBL): A method where insights are derived by analyzing and understanding specific cases or examples. Experiential Learning: Gaining knowledge through direct experience and reflection on specific situations. ^^ kind of bottom-up approach\nDynamic Pricing Use Case https://www.kai-waehner.de/blog/2024/11/14/a-new-era-in-dynamic-pricing-real-time-data-streaming-with-apache-kafka-and-flink/ https://www.confluent.io/events/kafka-summit-apac-2021/kafka-tiered-storage/\nRedPanda Analytics Use Case In this track you will learn how to create a web analytics platform using Redpanda and Clickhouse.\nWelcome back. In this lesson, we\u0026rsquo;ll discuss the reference architecture for building a cutting-edge analytics product. We\u0026rsquo;ll leverage modern technologies to create a powerful, scalable, and privacy-focused solution that can adapt to various deployment needs.\nHono At the heart of our data collection process lies Hono, an ultrafast web framework for edge computing. Hono allows us to create lightweight, high-performance edge services that can handle incoming analytics data with minimal latency.\nBy deploying our data collection endpoint at the edge, we ensure that user interactions are captured swiftly and efficiently, regardless of their geographic location. Hono will serve the JavaScript snippet that collects user data and also handle the incoming analytics events, acting as the first point of contact in our data pipeline.\nRedpanda\nOnce the data is received by our Hono-powered edge service, we\u0026rsquo;ll immediately forward it to Redpanda. Redpanda serves as a high-performance buffer between our data collection layer and our analytics database. This buffering is crucial for handling traffic spikes and ensuring data durability.\nRedpanda\u0026rsquo;s ability to handle millions of events per second with low latency makes it an ideal choice for real-time analytics pipelines. Also, its Kafka compatibility means we can leverage a rich ecosystem of tools and connectors while benefiting from Redpanda\u0026rsquo;s improved performance and simpler operational model.\nClickhouse From Redpanda, our data will stream into ClickHouse, a column-oriented database that\u0026rsquo;s designed for real-time analytics on large datasets. ClickHouse\u0026rsquo;s architecture allows for blazing-fast query execution, making it possible to analyze billions of rows in milliseconds.\nThis capability is essential for providing real-time insights and interactive dashboards. ClickHouse also offers excellent data compression, reducing storage costs while maintaining query performance. By combining Redpanda and ClickHouse, we can handle massive amounts of data while providing near-instantaneous query results.\nGrafana To bring our analytics to life, we\u0026rsquo;ll use Grafana, an open-source platform for monitoring and observability. Grafana will connect to ClickHouse and provide a rich, interactive interface for creating dashboards, alerts, and visualizations. Its flexibility allows us to create custom panels and graphs that cater to specific analytics needs, from high-level overviews to detailed drill-downs.\nGrafana\u0026rsquo;s ability to combine data from multiple sources also means we can integrate our analytics with other metrics and logs, providing a comprehensive view of our system\u0026rsquo;s performance and user behavior.\nBenefits One of the key strengths of this architecture is its flexibility in deployment options. Each component - from Hono to Grafana - can be self-hosted or used as a managed service, allowing you to choose the approach that best fits your needs and resources.\nThis flexibility extends to the data collection layer, where companies can implement custom tracking solutions or integrate with existing analytics libraries. By providing this freedom, we ensure that our analytics platform can adapt to various privacy requirements, regulatory needs, and scaling demands. If you aim to run every component in-house, you can do that. If you want to minimize overhead for certain pieces (e.g. Redpanda or Clickhouse), you can swap in their cloud offerings.\nEspecially noteworthy is Redpanda\u0026rsquo;s Bring Your Own Cloud (BYOC) service, which allows you to deploy Redpanda in-house within your own cloud environment. This approach ensures that your data remains secure and under your control while benefiting from professional management services provided directly by Redpanda\u0026rsquo;s expert team. This model offers the best of both worlds: the security and compliance of an on-premises solution with the convenience and expertise of a managed service.\nSummary This reference architecture combines the speed of edge computing, the reliability of event streaming, the power of columnar databases, and the flexibility of modern visualization tools to create a comprehensive analytics solution.\nAs we progress through this course, you\u0026rsquo;ll gain hands-on experience with each of these technologies, learning how to harness their individual strengths and integrate them into a cohesive, powerful analytics platform.\nWhether you\u0026rsquo;re building a privacy-first solution or a large-scale enterprise system, the skills you\u0026rsquo;ll acquire will enable you to create fast, scalable, and insightful analytics products.\nWith this architectural blueprint in place, let\u0026rsquo;s move on to the prerequisites.\nWe\u0026rsquo;re trying to connect to the service, but it seems like it\u0026rsquo;s not quite ready yet\u0026hellip;\nDepending on the service you are trying to reach this may take a few minutes.\nThis page will try to reload once the service is available,\nor you can try to refresh the page yourself.\nsource ~/.bash_profile git clone https://github.com/redpanda-data-university/rp-use-cases-web-analytics.git -b instruqt cd rp-use-cases-web-analytics\rCreate the ingestion endpoints Since the marketing website (which we intend to capture analytics from) is referencing an endpoint called /js in our Hono application, let\u0026rsquo;s implement that.\nThis confirms that our data collection endpoint is wired up to our website, but it\u0026rsquo;s not doing much yet. Let\u0026rsquo;s modify the code to collect some information about the page visit.\nCollecting data Now, we want to collect the page that the user is visiting (/pricing, /about, etc).\nEnrich the payload\nIn addition to tracking which page the user visited (/pricing, /blog, etc), there\u0026rsquo;s more information that would be useful to collect. For example:\nThe user\u0026rsquo;s browser / client information IP address Approximate geolocation While we needed to collect the page URL client-side (using the JS snippet we returned from the js/ endpoint), we can collect the rest of this information from the server side.\nImprove the formatting\nThis payload is starting to look pretty good. But thinking ahead, the highly nested structure of the client field could complicate the way we query and process the data later on. So let\u0026rsquo;s add the following helper function to flatten the structure.\nFinalizing the payload\nBefore we starting publishing this data to Redpanda and Clickhouse, lets add two more fields to the payload: ip and country. This is where our implementation gets a little Cloudflare-specific. While the main Hono application can be deployed to many different edge runtimes, the methods for accessing certain data points may differ according to the runtime.\nThe following methods for getting the IP address (c.req.header(\u0026quot;CF-Connecting-IP\u0026quot;)) and the user\u0026rsquo;s country (c.req.raw?.cf?.country) in the code snippet below can be adapted accordingly if you decide to deploy your application to one of the non-Cloudflare edge runtimes.\nWhat\u0026rsquo;s Next? You have now completed one of the most important tasks for building an analytics system: collecting the raw data. In the next two chapters, you learn how to store this data in Redpanda and Clickhouse, and implement more advanced features, as well. See you in the next chapter!\nChapter 2: Intro In this lesson, you\u0026rsquo;ll create the Redpanda topic for storing user analytics, and then start writing data to it.\nSetting Up Redpanda\nWe have included a local Redpanda cluster in the docker-compose.yml file. To interact with this cluster, we\u0026rsquo;ll use the rpk CLI, which is installed alongside the broker.\nTo make the CLI available, run the following command to set an alias that points to the rpk executable.\nWith the website_visits topic created, we\u0026rsquo;re almost ready to write the data to Redpanda. However, since Redpanda gives us a couple of different options for producing data, we need to decide which method to use:\nThe HTTP proxy allows us to produce requests by submitting HTTP requests to Redpanda\u0026rsquo;s REST API Kafka producer libraries write data using a different wire format Since Redpanda includes the HTTP proxy inside its binary, and Javascript has native support for making HTTP requests, we\u0026rsquo;ll pursue that option in this tutorial. Another factor influencing our decision is that npm modules don\u0026rsquo;t always work as expected in edge runtimes, so we\u0026rsquo;ll pursue the path of least surprise by simply issuing HTTP requests instead of using one of the Kafka Node modules.\nIt\u0026rsquo;s time to start producing data to Redpanda.\nProducing to Redpanda The Redpanda documentation includes many examples of how to interact with Redpanda\u0026rsquo;s HTTP Proxy API. For example, to produce data to a Redpanda topic, you\u0026rsquo;ll need to issue a POST request to the topics/{topic_name} endpoint.\nalias rpk=\u0026#34;docker exec -ti redpanda-1 rpk\u0026#34; rpk topic consume website_visits\rNow that the page visit data is flowing through Redpanda, we can do a couple of things with it: Write the data to a data warehouse to visualize and query the data Start building event-driven systems to utilize this data for various business use cases We\u0026rsquo;ll start with the first option and show you how to ingest this data into Clickhouse. Proceed to the next lesson to continue the tutorial.\nIngesting the data into Clickhouse\nIn this lesson, you\u0026rsquo;ll pipe the analytics data from Redpanda into a real-time data warehouse called Clickhouse.\nSetting Up Clickhouse\nClickhouse is a popular analytics database that is great for low-latency queries on large analytic datasets. It\u0026rsquo;s open-source and easy to get started with.\nWe\u0026rsquo;ve included a Clickhouse deployment in the docker-compose.yaml file, so it should have automatically started after you ran docker-compose up -d earlier. To verify that it\u0026rsquo;s running, run the following command from the Terminal:\nthen create database\nCREATE DATABASE analytics;\rNext, create a Clickhouse table that uses the Kafka table engine to read data from Redpanda into Clickhouse. The columns in the DDL statement below correspond to the field names in the payload we constructed in Chapter 1.\nCREATE OR REPLACE TABLE analytics.web_kafka ( url String, ip String, country String, ua String, browser_name String, browser_version String, browser_major String, engine_name String, engine_version String, os_name String, os_version String, device_vendor String, device_model String, timestamp DateTime ) ENGINE = Kafka SETTINGS kafka_broker_list = \u0026#39;redpanda-1:9092\u0026#39;, kafka_topic_list = \u0026#39;website_visits\u0026#39;, kafka_group_name = \u0026#39;rp\u0026#39;, kafka_format = \u0026#39;JSONEachRow\u0026#39;, kafka_num_consumers = 1;\rA materialized view is a special type of view that stores the results of a query physically in the database, unlike a regular view which does not store any data and only represents a query. Materialized views in ClickHouse are used to improve query performance by precomputing and storing the results of complex queries, which can then be queried directly instead of recomputing the results each time the query is run.\nFinally, set up a materialized view to query the data. Run the following command to DDL statement to create thie view:\nCREATE MATERIALIZED VIEW analytics.web ENGINE = Memory AS SELECT * FROM analytics.web_kafka SETTINGS stream_like_engine_allow_direct_select = 1;\rWith the table and materialized view setup, the data can now flow from Redpanda into Clickhouse. Open the Marketing Website again and click around a few pages. Then go back to the Clickhouse UI, and run the following query:\nSELECT * FROM analytics.web LIMIT 10\rYou should see multiple rows of data.\nYou can also run aggregate queries like the following:\nSELECT COUNT() as count, country FROM analytics.web GROUP BY country\rNote If you experience any issues creating these tables, you can always check the Clickhouse system errors with the following query: SELECT * FROM system.errors;\nOnce you\u0026rsquo;ve confirmed that the data is flowing into Clickhouse, proceed to the next section to learn how to visualize the data in Grafana. We\u0026rsquo;ll cover this topic in the next lesson.\nYou\u0026rsquo;ve made a lot of progress in the last few lessons! Your Hono application is now collecting web analytics from the example marketing website, and this data is flowing through Redpanda and Clickhouse.\nHowever, in order to extract even more value from our analytics data, we need a way to visualize it. Fortunately, Clickhouse integrates with many different data visualization tools to enable this functionality.\nFor this tutorial, we\u0026rsquo;ll be choosing a well-established open-source option for visualizing our data: Grafana.\ngrafana -\u0026gt; connections-\u0026gt; \u0026ldquo;ClickHouse\u0026rdquo; -\u0026gt; install\nNext, click Dashboards in the left navigation, then click + Create dashboard, and finally, click Add visualization.\nSelect your new Clickhouse connection, and then from the SQL Editor tab, type in the following query:\nSELECT timestamp AS time, country, count() AS requests FROM analytics.web GROUP BY country, timestamp ORDER BY timestamp\rAnd that\u0026rsquo;s all there is to start visualizing your data. From here, you have limitless possibilities thanks to Grafana\u0026rsquo;s wide range of chart / visualization types.\nWe won\u0026rsquo;t go through the process of building an entire analytics dashboard, but the process should be clear now. As an exercise for the reader, try adding a few more visualizations in Grafana by writing some additional queries. Some examples of visualizations you can add include:\nIn this lesson, you\u0026rsquo;ll make some slight updates to the data collection script to improve the response times and modularize the code for producing data to Redpanda. This will pave the path for storing session recordings in Redpanda and Clickhouse.\nAsync ingestion One of the benefits of running our analytics service on the edge is that our code for collecting information is deployed close to the end user. This allows us to initiate data collection very quickly after a user visits our website.\nHowever, depending on where your Redpanda cluster is deployed, the cluster itself may not be geographically close to the user. Consequently, producing data to Redpanda may incur a few hundred milliseconds of additional latency.\nThe current implementation of our track/ handler won\u0026rsquo;t return a response until after the data is produced to Redpanda. This is okay, but session recordings can generate a lot of data. If we can minimize the number of open requests that clients are waiting on at any given point in time, we can mitigate certain types of client-side performance issues.\nLet\u0026rsquo;s address this performance concern before we implement session recordings.\nPreparing Redpanda\nLet\u0026rsquo;s make a slight modification to our code so that we can return a response to the client right after receiving the analytic events. We\u0026rsquo;ll then produce the events to Redpanda in a non-blocking way to improve our service\u0026rsquo;s response time and address the issues mentioned above.\nTo accomplish this, we can use a built-in function called waitUntil. This function takes a promise that will continue executing even after a response is returned to the client. It\u0026rsquo;s basic usage looks like this:\nBy using waitUntil, our service will acknowledge tracking requests much faster. Also, by creating a dedicated function for producing data to Redpanda, we can now avoid some boilerplate when producing data from new endpoints (e.g. the upcoming session recording endpoints).\nSession Recording Many third-party vendors offer session recording as one of their most advanced features. Unfortunately, this feature also raises some data privacy concerns.\nThe idea is that whenever a user visits your website, everything they do, from mouse movements, clicks, to keystrokes, is recorded so that it can be replayed in the future. The recordings can be used for a variety of reasons:\nImproving customer support Fraud and security investigations Fine-grained product research and more To enable these use cases, the session recorder needs to watch every user movement and reconstruct the entire layout, style, and content of your webpage so that it can replay user events in a visually meaningful way.\nThis leads to session recorders like rrweb having multiple event types. Some events will be large as they need to capture metadata about your website (i.e. structure and style information to reconstruct your website at replay time). Other events will be small, since they only contain x/y coordinates of mouse movements.\nWhen processed and stored correctly, this data will allow you to watch a video of your user interacting with your website.\nIf you\u0026rsquo;re hesitant to ship such detailed information about your users\u0026rsquo; behaviors and also your platform to a third party, or if you just want a better understanding of how this works, we\u0026rsquo;re going to show you how to implement this advanced feature yourself using rrweb, Redpanda, and Clickhouse.\nSession Recording To get started, you\u0026rsquo;ll need to create a new Redpanda topic for storing the session recordings.\nAs mentioned in the previous section, certain metadata messages that occur at the beginning of each recording may be on the larger side. These payloads can exceed Redpanda\u0026rsquo;s default max message size of 1MB.\ncreate topics with session recordings.\nSession Recording: Clickhouse setup\nNext, you\u0026rsquo;ll need to create a new table in Clickhouse for storing the session recordings. Run the following DDL statement from your local Clickhouse UI to create the table:\nCREATE OR REPLACE TABLE analytics.recordings_kafka ( id String, page_title String, recording String, timestamp DateTime64(3) ) ENGINE = Kafka SETTINGS kafka_broker_list = \u0026#39;redpanda-1:9092\u0026#39;, kafka_topic_list = \u0026#39;session_recordings\u0026#39;, kafka_group_name = \u0026#39;rp\u0026#39;, kafka_format = \u0026#39;JSONEachRow\u0026#39;, kafka_num_consumers = 1;\rThe analytics.recordings_kafka table uses the Kafka engine to read session recordings from a Redpanda topic called session_recordings. The fields names correspond to the following values:\nFinally, create a materialized view that we can use to query the session recording data.\nCREATE MATERIALIZED VIEW analytics.recordings ENGINE = Memory AS SELECT id, page_title, recording, timestamp FROM analytics.recordings_kafka SETTINGS stream_like_engine_allow_direct_select = 1;\rSession Recording: Hono Updates\nTo implement the session recording, we\u0026rsquo;re going to use an open source library called rrweb. You can read about that library in detail in the Github repo, but to summarize the implementation, we need to:\nInject the rrweb library into the webpage Initialize start recording when the page is loaded Post all recording events to our Hono service Save these events to Redpanda and Clickhouse Build a page to watch / replay the recording events We could implement all of this in our existing /js endpoint, but to improve readability for this tutorial, we will create a new endpoint called record.js.\nWhether or not you ultimately decide to move some of your analytic workloads in-house, we hope this course provided some useful experience with Hono, Redpanda, and Clickhouse. The skills you built here can be applied to a wide range of use cases, and familiarity with the core components (a data collector, a streaming layer, and a real-time analytics database) will ensure a smooth journey for whatever you decide to build next.\n","date":"2025-02-22","id":69,"permalink":"/system-design/topics/streaming/","summary":"\u003ch1 id=\"sources-and-links\"\u003eSources and Links\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://trueaccord.atlassian.net/wiki/spaces/DS/pages/815105668/Data\u0026#43;Streaming\u0026#43;overview\"\u003ehttps://trueaccord.atlassian.net/wiki/spaces/DS/pages/815105668/Data+Streaming+overview\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://drive.google.com/file/d/1wyP1aUBAwhLLonENwIJjVXxXWyUfJ5nN/view\"\u003ehttps://drive.google.com/file/d/1wyP1aUBAwhLLonENwIJjVXxXWyUfJ5nN/view\u003c/a\u003e\n\u003ca href=\"https://drive.google.com/file/d/1QeIte_uFbKYlPr4bIVuBVQCx1vWSb0l_/edit\"\u003ehttps://drive.google.com/file/d/1QeIte_uFbKYlPr4bIVuBVQCx1vWSb0l_/edit\u003c/a\u003e\n\u003ca href=\"https://trueaccord.atlassian.net/wiki/spaces/DS/pages/815105668/Data\u0026#43;Streaming\u0026#43;overview\"\u003ehttps://trueaccord.atlassian.net/wiki/spaces/DS/pages/815105668/Data+Streaming+overview\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://trueaccord.atlassian.net/wiki/spaces/DS/pages/815727051/Glossary\"\u003ehttps://trueaccord.atlassian.net/wiki/spaces/DS/pages/815727051/Glossary\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eold. Streaming Platform overview \u003ca href=\"https://trueaccord.atlassian.net/wiki/x/jAHvNg\"\u003elink\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003elocal repos?\nstreaming/platform\u0026hellip;\u003c/p\u003e\n\u003cp\u003eSources:\n\u0026ldquo;Data Streaming\u0026rdquo; space\nslack channel\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://trueaccord.atlassian.net/wiki/x/_QRmbg\"\u003ehttps://trueaccord.atlassian.net/wiki/x/_QRmbg\u003c/a\u003e\n\u003ca href=\"https://trueaccord.atlassian.net/wiki/spaces/DS/settings/home\"\u003ehttps://trueaccord.atlassian.net/wiki/spaces/DS/settings/home\u003c/a\u003e\u003c/p\u003e","tags":[],"title":"Streaming"},{"content":"","date":"2025-02-22","id":70,"permalink":"/system-design/","summary":"","tags":[],"title":"System Design"},{"content":"Common Guide There are numerous sources available for learning System Design (SD). To help avoid \u0026ldquo;tutorial hell,\u0026rdquo; this selection focuses on core tutorials, videos and courses that provide a structured learning path.\nCourses \u0026ldquo;System Design for Interviews and Beyond\u0026rdquo; course and youtube videos with examples. \u0026ldquo;In this video-based course, we will take you on a journey to learn the fundamental concepts of system design. \u0026hellip; In addition to the knowledge and understanding of system design concepts, this course will teach you the thought process behind system design. We will discuss the questions you should ask yourself when designing a system and how to evaluate available options. \u0026hellip; \u0026quot;\nAlgoexpert 2 courses (fundamentals and examples) Zerotomastery \u0026ldquo;# Master the Coding Interview: System Design + Architecture (Part 1)\u0026rdquo;\nHellointerview examples + youtube channel + (maybe) premium https://interviewing.io/guides/system-design-interview/part-four#chapter-one\n(?) algoexpert \u0026ldquo;# Systems Design Interview Tips\u0026rdquo; https://github.com/Sairyss/system-design-patterns interesting cirriculim with examples\ndesigngurus ? https://www.designgurus.io/path/system-design-interview-playbook\nbytebytego ?\ntryexponent courses ?\nSD Framework steps, checklists\nhttps://interviewing.io/guides/system-design-interview/part-three\nCommon / Preparation Guides interviewing.io guides https://blog.pragmaticengineer.com/preparing-for-the-systems-design-and-coding-interviews/ https://github.com/Sairyss/system-design-patterns\ninterview process guides interview guide + interviewing.io https://www.tryexponent.com/blog/system-design-interview-guide\nwatch, analyze and reflect mock interviews Mock interviews\nmaybe see channels with useful videos Useful videos\nOther codemia.io (?)\nreflect and analyze papers Papers\nblogs on system design; technical blogs; conferences\nPlan Start by developing a toolbox (Version 1) to address the time-consuming nature of selecting tools and patterns, as identifying the right tools and patterns can be a lengthy process. Apply it to practical challenges while iterating and improving it along the way.\nAction Plan Structure 1.Develop a Starter Toolbox (Version 1): - Create a collection of foundational tools, frameworks, and design patterns to streamline the decision-making process. - Focus on essential concepts like scalability, reliability, and modularity to build a versatile starting point.\n2.Apply the Toolbox to Real-World Challenges: - Use your toolbox to design solutions for practical scenarios, such as building scalable APIs, designing a database schema, or implementing caching layers. - Select diverse problems to cover a range of system design principles.\n3.Iterate and Refine: - Continuously evaluate your toolbox based on the challenges you solve. - Identify gaps or inefficiencies and incorporate new tools, techniques, or patterns as you gain experience.\n4.Practice Collaborative Design: - Simulate interview-style system design discussions with peers or mentors to enhance your ability to explain and adapt your toolbox in real-time.\n5.Document Lessons Learned: - Keep a log of the systems you design, the tools you use, and the trade-offs you encounter. - Use these reflections to improve your problem-solving and prepare for interviews.\nLearning (SD Interview) Advice\nUse Learning principles and practices!\nAt a high level, preparing for system design interviews is really about assembling the right pieces: you\u0026rsquo;ll need to know some core concepts, key technologies, and common patterns. On this base, you\u0026rsquo;ll establish a strategy or delivery framework for executing the interview. And finally, you\u0026rsquo;ll need to practice to ensure you\u0026rsquo;re comfortable the day of your actual interview.\nHelloInterview structured learning process into these blocks: Core Concepts =\u0026gt; { Key Technologies, Patterns } =\u0026gt; Delivery Framework =\u0026gt; Common Problems Practicing\nPractice Practice Practice Once you have the foundation in place, it\u0026rsquo;s time to practice. Passively consuming content is good, but you\u0026rsquo;ll retain 10x more information by actually doing.\nChoose a question: Select a question from the list of common questions below. Read the requirements: Understand the requirements of the system you\u0026rsquo;ll need to design. Try to answer on your own: Either practice with our Guided Practices (below) or on a virtual whiteboard like Excalidraw. Read the answer key: Only after you have tried to answer the question, read the answer key to see how your answer compares. Put your knowledge to the test: Once you\u0026rsquo;ve done a few questions and are feeling comfortable, put your knowledge to the test by scheduling a mock interview with an interviewer from your target company. Interviewing.io Candidates often get overwhelmed with system design. We don\u0026rsquo;t blame them. There are literally hundreds of topics you can study when preparing for an interview. But does that mean that you should drop everything and go study all of them? Absolutely not. It\u0026rsquo;s vital to master the basic principles first.\nProfessional experience with distributed systems isn\u0026rsquo;t needed to pass system design interviews. And even if you do have that experience, keep in mind that many talented distributed systems engineers still struggle with the system design interview format. How you perform in an interview is not a measure of your worth as a software engineer \u0026ndash; it is a measure of your ability to do system design interviews. The two are related but not equal; being a good programmer has a surprisingly small role in passing interviews.\nmock_interviews interviewing.io videos from youtube https://www.youtube.com/watch?v=mQgKAK7y11s\ntryexponent mock videos https://www.youtube.com/watch?v=iyLqwyFL0Zc\nhello interview https://www.youtube.com/watch?v=tgSe27eoBG0 really cool! https://www.hellointerview.com/\nhttp://youtube.com/watch?v=PI0yGBT9LHo\n^^ analyze them:\nfind out errors, try it yourself first, how would you resolve this errors? technical depth? alternative solutions? communication aspect, leadership skills, proactivity, interview structure. Advices It\u0026rsquo;s vital to master the Basic Principles first.\nThe best candidates trying by doing LOTS of mock interviews, with peers\nUse enigneering principles and practices. ? [link]\nInterview Process Interview\nWhat to Learn Elements\nAreas (from algoexpert):\nfoundational SD knowledge key characteristics of systems actual components of the system like LB, caches, proxies, leader election actual tuck; real-life tools, existing products to use in SD to build your system system design patterns (refactor) Topics to research and grok: building blocks (systems approach?) nfu reasoning trade-offs:\ncommong trade-off reasoning make a list of these in one place and refine them after you get more and more practice async patterns, communication, protocols, distributed coordination, transactions and sagas.. common technical problems data and databases api construction fundamental things: vector clocks (hellointerview) classification\n(interviewing.io) 12 fundamental (technical) system design concepts: a. APIs b. Databases (SQL vs NoSQL) c. Scaling d. CAP Theorem e. Web authentication and basic security f. Load balancers g. Caching h. Message queues i. Indexing j. Failover k. Replication l. Consistent hashing\nProjects \u0026amp; Examples first in themes file, also we have a big amount of mock interviews algoexpert videos hellointerview smarchok etc +this one https://interviewing.io/guides/system-design-interview/part-four from distributed-systems (link) (? some examples)\nCommunity discord servers! forums meet ups\nhttps://launchpass.com/pminterview\nyou can discuss some project on leetcode, for example\nFundamentals \u0026amp; Distributed Systems Distributed-systems\nMicroservices Microservices\nSources https://www.hellointerview.com/learn/system-design/problem-breakdowns/leetcode\nexample how to organize your SD public Result: https://github.com/Sairyss/system-design-patterns -\u0026gt; todo add projects here (public learning)\nOther https://mlengineer.io/facebook-system-design-interview-4-must-watched-videos-212e07d4fbc2 Scaling Instagram Infrastructure - https://www.youtube.com/watch?v=hnpzNAPiC0E\u0026t=669s Scaling Facebook Live Videos to a Billion Users - https://www.youtube.com/watch?v=IO4teCbHvZw\u0026t=1692s Building Real Time Infrastructure at Facebook - Facebook - SRECon2017 - https://www.youtube.com/watch?v=ODkEWsO5I30 USENIX ATC \u0026lsquo;13 - TAO: Facebook\u0026rsquo;s Distributed Data Store for the Social Graph - https://www.youtube.com/watch?v=sNIvHttFjdI\nhttps://muratbuffalo.blogspot.com/2023/10/hints-for-distributed-systems-design.html\nuseful_videos common overview https://www.youtube.com/watch?v=F2FmTdLtb_4\nchannels with useful videos: https://www.youtube.com/@hello_interview staff engineer https://www.youtube.com/@irtizahafiz https://www.youtube.com/@interviewpen ?? https://www.youtube.com/@ByteByteGo\nPapers https://www.confluent.io/blog/kafka-streams-tables-part-1-event-streaming/ https://levelup.gitconnected.com/system-design-interview-all-or-none-ordered-peer-to-peer-broadcast-45b33fb2f6be\nsla, slo and other: https://sre.google/sre-book/service-level-objectives/\npapers: • Amazon - Dynamo paper • Google - Map-reduce paper • Google - GFS paper • Facebook - TAO paper • Jeff Dean\u0026rsquo;s talk at Stanford: /watch?v=modXC5IWTJI • Building Billion user Load Balancer at Facebook: /watch?v=bxhYNfFeVF4 • Netflix Guide to Microservices: /watch?v=CZ3wIuvmHeM • Amazon DynamoDB deep dive: /watch?v=HaEPXoXVf2k\n","date":"2025-02-22","id":71,"permalink":"/system-design/about/","summary":"\u003ch1 id=\"common-guide\"\u003eCommon Guide\u003c/h1\u003e\n\u003cp\u003eThere are numerous sources available for learning System Design (SD). To help avoid \u0026ldquo;tutorial hell,\u0026rdquo; this selection focuses on core tutorials, videos and courses that provide a structured learning path.\u003c/p\u003e","tags":["system-design","architecture","advice"],"title":"System Design"},{"content":"Callouts information\nhere we come other line\nuse it\nother\nCaution\nIf you\u0026rsquo;re\none second callout_pattern = re.compile()\rbe awsre\nof these things\ncode block example\ndef convert_callout(match): \u0026#34;\u0026#34;\u0026#34;Convert an Obsidian callout to a Hugo Doks callout format.\u0026#34;\u0026#34;\u0026#34; callout_type = match.group(2).lower() # Extract callout type (e.g., \u0026#34;info\u0026#34;) title = match.group(3) # Extract callout title content = match.group(4) # Extract callout content # Remove leading \u0026#39;\u0026gt;\u0026#39; from multi-line content content_lines = [line.lstrip(\u0026#34;\u0026gt; \u0026#34;).strip() for line in content.split(\u0026#34;\\n\u0026#34;) if line.startswith(\u0026#34;\u0026gt;\u0026#34;)] content = \u0026#34;\\n\u0026#34;.join(content_lines) # Get Doks callout properties or fallback to \u0026#34;note\u0026#34; doks_data = doks_callout_mapping.get(callout_type, {\u0026#34;context\u0026#34;: \u0026#34;note\u0026#34;, \u0026#34;icon\u0026#34;: \u0026#34;outline/info-circle\u0026#34;})\rTest link to learning Learning\nElements\nlink to heading in current document Courses\nlink to heading in other document CDN Service\nCourses courses text\n","date":"2025-02-22","id":72,"permalink":"/system-design/elements/test_page/","summary":"\u003ch2 id=\"callouts\"\u003eCallouts\u003c/h2\u003e\n\u003cdiv class=\"callout callout-note d-flex flex-row mt-4 mb-4 pt-4 pe-4 pb-2 ps-3\"\u003e\r\n  \u003csvg\n  xmlns=\"http://www.w3.org/2000/svg\"\n  width=\"24\"\n  height=\"24\"\n  viewBox=\"0 0 24 24\"\n  fill=\"none\"\n  stroke=\"currentColor\"\n  stroke-width=\"2\"\n  stroke-linecap=\"round\"\n  stroke-linejoin=\"round\"\n \n class=\"outline/info-circle svg-inline callout-icon me-2 mb-3\" id=\"svg-info-circle\" role=\"img\"\u003e\n  \u003cpath stroke=\"none\" d=\"M0 0h24v24H0z\" fill=\"none\"/\u003e\n  \u003cpath d=\"M3 12a9 9 0 1 0 18 0a9 9 0 0 0 -18 0\" /\u003e\n  \u003cpath d=\"M12 9h.01\" /\u003e\n  \u003cpath d=\"M11 12h1v4h1\" /\u003e\n\u003c/svg\u003e\r\n  \u003cdiv class=\"callout-content\"\u003e\r\n    \u003cdiv class=\"callout-title\"\u003e\r\n        \u003cp\u003einformation\u003c/p\u003e","tags":[],"title":"Test Page"},{"content":"Let\u0026rsquo;s say you are not familiar with TikTok. What do you do next? Probably ask your interviewer, right? Even if you know the platform well, it\u0026rsquo;s wise to start by clarifying rather than assuming. The flow might go something like this:\nInterviewer: Design TikTok.\nCandidate: I\u0026rsquo;m not very familiar with the platform. Would you be able to give me a high-level overview of what we are looking for?\nInterviewer: Sure. TikTok is a mobile app for video sharing between users. Basically, you can upload a video to TikTok, and you can view a feed of videos. You can follow other users and perform basic actions over videos, such as \u0026ldquo;like,\u0026rdquo; \u0026ldquo;favorite,\u0026rdquo; and \u0026ldquo;comment.\u0026rdquo;\nNote If this is still not clear to you, or you think your interviewer is still withholding functional requirements from you, by all means go ahead and ask more questions. In this example, it seems like the interviewer has given us a pretty good overview, so we can move into our steps for functional requirements.\n1. Identify the main objects and their relations It appears there will be primarily two objects of interest: accounts and videos. What are their relations?\nAccount -\u0026gt; Video\nCan post\nCan like\nCan comment\nAccount -\u0026gt; Account\nCan follow\n2. What information do these objects hold? Check with your interviewer, but we can have some basic information like:\nAccount: username, description Video: description Are they mutable? Good question for your interviewer.\nCandidate: Can videos be changed after uploading them?\nInterviewer: No, let\u0026rsquo;s assume that once a video is uploaded it will stay immutable.\n3. Think about the access patterns Remember, we are looking for statements of the form \u0026ldquo;Given object X, return all related objects Y.\u0026rdquo; Check with your interviewer to learn what different access patterns are needed here. Example:\nGiven a user, get all videos they\u0026rsquo;ve posted. Given a user, get their feed (videos posted by people they follow). Given a video, get likes/comments. Then add the writes:\nPost a video. Follow an account. Like/comment on a video. Non-Functional Requirements Discuss with your interviewer, but you probably want to optimize for performance and availability because of reasons that are similar to our Twitter example.\n","date":"2025-02-22","id":73,"permalink":"/system-design/projects/tik_tok/","summary":"\u003cp\u003eLet\u0026rsquo;s say you are not familiar with TikTok. What do you do next? Probably ask your interviewer, right? Even if you know the platform well, it\u0026rsquo;s wise to start by clarifying rather than assuming. The flow might go something like this:\u003c/p\u003e","tags":[],"title":"Tik Tok"},{"content":"","date":"2025-02-22","id":74,"permalink":"/system-design/to_sort/","summary":"","tags":[],"title":"To Sort"},{"content":"","date":"2025-02-22","id":75,"permalink":"/system-design/topics/","summary":"","tags":[],"title":"Topics"},{"content":"relational-vs-nosql Don\u0026rsquo;t oversell a solution. Every solution has positive and negative aspects and needs to be approached with a sense of realism. If you\u0026rsquo;re being unrealistic, you probably won\u0026rsquo;t change your mind (even when it benefits you to change your mind!). For example, sometimes the interviewer will give you an out by asking some follow-up questions, giving you a chance to see your own mistake and change your mind. But if you\u0026rsquo;re too fixated on being right, you\u0026rsquo;ll miss the opportunity.\nTherefore, we\u0026rsquo;re giving you two very powerful tools: (1) A rule of thumb to pick Relational (SQL) vs. Non-Relational (NoSQL), and (2) A list of trade-offs that you should mention to your interviewer after stating your decision.\nAt the risk of oversimplifying the decision, we can assert with confidence that if you don\u0026rsquo;t fall into any of the above cases, you are probably fine picking either SQL or NoSQL. However, many interesting system design questions require strong consistency, unstructured data, or both. Actually, using both is also somewhat common, and something we\u0026rsquo;ll touch on.\n(Tell your interviewer)\nIf you picked relational: \u0026ldquo;Although I think a relational database better fits this requirement, we should also be mindful of the downsides. For example, our database will have a more rigid structure and schema, so it might be harder for us to incorporate changes. We\u0026rsquo;ll also need to scale up vertically, meaning that as we get more load we\u0026rsquo;ll upscale existing servers rather than dividing the work over more servers.\u0026rdquo;\nIf you picked non-relational: \u0026ldquo;Although I think a non-relational database better fits this requirement, we should also be mindful of the downsides. We\u0026rsquo;ll be able to scale horizontally at the cost of not having ACID guarantees. I\u0026rsquo;m assuming there will be no need for strong consistency in the future.\u0026rdquo;\nSo which one is a better fit for our Twitter example? Let\u0026rsquo;s run our requirements through these questions:\nDo we need strong consistency? We probably don\u0026rsquo;t. It\u0026rsquo;s fine if after publishing a tweet, some users can see it before others. Same for likes and followers. We don\u0026rsquo;t need to treat these as atomic, consistent operations. Eventual consistency works fine for our requirements.\nDo we have large volumes of unstructured data? Not necessarily. Our entities, tweets and accounts, will have some well-defined static fields that are unlikely to change in meaningful ways.\nGiven the answer to these two questions is \u0026ldquo;no,\u0026rdquo; this is yet another example where we are good with either choice. It comes down to how we justify it. In fact, Twitter started using MySQL and then moved to NoSQL seeking better scalability and availability.\nIf you ask us, we\u0026rsquo;d probably go for NoSQL and justify it as: (1) It doesn\u0026rsquo;t look like we need strong consistency, and (2) NoSQL will scale horizontally and likely have better availability.\nExamples 1. Design a banking system This is a textbook example of strong consistency. Transactions in a banking system need ACID guarantees. As such, we are probably better off picking a relational database that can give us this strong consistency.\n2. Design a system to help doctors diagnose potential illnesses given symptoms Let\u0026rsquo;s say this is mainly a querying system. Doctors enter a list of symptoms and get back potential illnesses and treatments. The data we will be storing is unstructured in nature, and it will likely be an ever increasing database as we add more illnesses, symptoms, and diagnoses. In this example, it might be wise to pick a non-relational database where we can store large volumes of unstructured data, scale horizontally, and be fine with just eventual consistency.\n3. Design Amazon Amazon is a good example of a system where we might want to use both of these. We\u0026rsquo;d want to have consistency for product transactions, while being flexible about the data in our product catalog. It wouldn\u0026rsquo;t be crazy to suggest using a relational database to keep track of purchases and stock, while using a non-relational database for the product catalog.\n","date":"2025-02-22","id":76,"permalink":"/engineering/data/trade-off-db/","summary":"\u003ch1 id=\"relational-vs-nosql\"\u003erelational-vs-nosql\u003c/h1\u003e\n\u003cp\u003eDon\u0026rsquo;t oversell a solution. Every solution has positive and negative aspects and needs to be approached with a sense of realism. If you\u0026rsquo;re being unrealistic, you probably won\u0026rsquo;t change your mind (even when it benefits you to change your mind!). For example, sometimes the interviewer will give you an out by asking some follow-up questions, giving you a chance to see your own mistake and change your mind. But if you\u0026rsquo;re too fixated on being right, you\u0026rsquo;ll miss the opportunity.\u003c/p\u003e","tags":[],"title":"Trade Off Db"},{"content":"A Systematic Trade-Off Analysis Framework (from chatgpt) Here\u0026rsquo;s a more detailed, systemic approach to constructing and using a Trade-Off Analysis Framework:\n1. Define Objectives and Goals Purpose Identification: Articulate the overarching purpose of the analysis (e.g., selecting a technology stack, optimizing a process, allocating resources). Hierarchy of Objectives: Identify primary objectives (e.g., reduce operational costs) and secondary objectives (e.g., improve scalability or usability). Consider constraints (e.g., budget limits, deadlines) and align them with organizational or project priorities. SMART Goals: Ensure objectives are Specific, Measurable, Achievable, Relevant, and Time-bound. 2. Identify Decision Criteria Comprehensive Listing: List all factors that will influence the decision. Examples: Technical: Performance, scalability, reliability. Financial: Cost, ROI, TCO (Total Cost of Ownership). Operational: Implementation time, ease of integration, risk. Criteria Prioritization: Use methods like pairwise comparison or Analytic Hierarchy Process (AHP) to rank criteria. Assign weights to each criterion based on importance. For instance: Performance: 40% Cost: 30% Scalability: 20% Risk: 10% 3. List Alternatives Alternative Identification: Outline all potential options or solutions. Ensure alternatives are: Mutually Exclusive: No overlap between options. Collectively Exhaustive: Covers all feasible choices. Expand Creativity: Consider out-of-the-box options or hybrid solutions to enrich the set of alternatives. 4. Quantify Attributes Data Collection: Collect quantitative and qualitative data for each alternative against the identified criteria. Example metrics: Costs in dollars, time in hours, defect rates, scalability scores. Scoring and Standardization: Normalize data to ensure comparability (e.g., scale all criteria to 0–10). Use methods like: Weighted Scoring Models: Multiply scores by criteria weights and sum them up. Utility Functions: Define mathematical functions to calculate the utility of each alternative. 5. Assess Trade-Offs Conflict Identification: Identify and document conflicts (e.g., better performance often comes with higher costs). Visualization: Use tools like: Trade-Off Curves: Visualize how changes in one criterion affect others. Pareto Fronts: Highlight alternatives that are Pareto-efficient (no other option is better on all criteria simultaneously). Sensitivity Analysis: Test how changes in weights or assumptions affect the ranking of alternatives. Example: \u0026ldquo;If cost weight increases by 10%, does the preferred alternative change?\u0026rdquo; 6. Make a Decision Alternative Selection: Choose the alternative that offers the best balance of trade-offs, considering: Weighted scores or utility values. Qualitative judgments, if applicable (e.g., alignment with long-term strategy). Documentation and Justification: Clearly document: Why the chosen alternative was selected. The rationale behind the trade-offs. Supporting data and analysis. 7. Iterate and Validate Iterative Review: Revisit the framework as: New data becomes available. Decision priorities or constraints change. Implementation Testing: Test the selected solution in a controlled or pilot environment to validate assumptions. Feedback Loop: Gather feedback from stakeholders and compare outcomes against the initial objectives. Adjust and refine the framework for future use. Tools and Techniques for Implementation Decision-Making Tools: Analytic Hierarchy Process (AHP) Multi-Criteria Decision Analysis (MCDA) Visualization Tools: Pareto charts Heat maps for criteria vs. alternatives Software Platforms: Tools like Excel, R, or Python for quantitative analysis. Specialized tools like MATLAB or Tableau for complex visualizations. This systematic framework ensures that trade-offs are evaluated comprehensively, and decisions are made transparently and effectively. Let me know if you\u0026rsquo;d like more details on implementing any specific component!\n","date":"2025-02-22","id":77,"permalink":"/system-design/topics/trade-offs/","summary":"\u003ch1 id=\"a-systematic-trade-off-analysis-framework\"\u003eA Systematic Trade-Off Analysis Framework\u003c/h1\u003e\n\u003cp\u003e(from chatgpt)\nHere\u0026rsquo;s a more detailed, systemic approach to constructing and using a Trade-Off Analysis Framework:\u003c/p\u003e","tags":[],"title":"Trade Offs"},{"content":"Definition How do you figure out whether you need transactions? In order to answer that question, we first need to understand exactly what safety guarantees transactions can provide, and what costs are associated with them. Although transactions seem straightforward at first glance, there are actually many subtle but important details that come into play.\nThe truth is not that simple: like every other technical design choice, transactions have advantages and limitations. In order to understand those trade-offs, let\u0026rsquo;s go into the details of the guarantees that transactions can provide—both in normal operation and in various extreme (but realistic) circumstances.\nACID Atomicity Rather, ACID atomicity describes what happens if a client wants to make several writes, but a fault occurs after some of the writes have been processed—for example, a process crashes, a network connection is interrupted, a disk becomes full, or some integrity constraint is violated. If the writes are grouped together into an atomic transaction, and the transaction cannot be completed (committed) due to a fault, then the transaction is aborted and the database must discard or undo any writes it has made so far in that transaction.\nThe ability to abort a transaction on error and have all writes from that transaction discarded is the defining feature of ACID atomicity. Perhaps abortability would have been a better term than atomicity, but we will stick with atomicity since that\u0026rsquo;s the usual word.\nConsistency Isolation Durability isolation levels: read commited and serializable\nImplementing read committed Read committed is a very popular isolation level. It is the default setting in Oracle 11g, PostgreSQL, SQL Server 2012, MemSQL, and many other databases [8]. Most commonly, databases prevent dirty writes by using row-level locks: when a transaction wants to modify a particular object (row or document), it must first acquire a lock on that object. It must then hold that lock until the transaction is com‐ mitted or aborted. Only one transaction can hold the lock for any given object; if another transaction wants to write to the same object, it must wait until the first transaction is committed or aborted before it can acquire the lock and continue. This locking is done automatically by databases in read committed mode (or stronger iso‐ lation levels). How do we prevent dirty reads? One option would be to use the same lock, and to require any transaction that wants to read an object to briefly acquire the lock and then release it again immediately after reading. This would ensure that a read couldn\u0026rsquo;t happen while an object has a dirty, uncommitted value (because during that time the lock would be held by the transaction that has made the write).\nHowever, the approach of requiring read locks does not work well in practice, because one long-running write transaction can force many read-only transactions to wait until the long-running transaction has completed. This harms the response time of read-only transactions and is bad for operability: a slowdown in one part of an application can have a knock-on effect in a completely different part of the applica‐ tion, due to waiting for locks.\nFor that reason, most databasesvi prevent dirty reads using the approach illustrated in Figure 7-4: for every object that is written, the database remembers both the old com‐ mitted value and the new value set by the transaction that currently holds the write lock. While the transaction is ongoing, any other transactions that read the object are simply given the old value. Only when the new value is committed do transactions switch over to reading the new value.\nSnapshot Isolation and Repeatable Read If you look superficially at read committed isolation, you could be forgiven for think‐ ing that it does everything that a transaction needs to do: it allows aborts (required for atomicity), it prevents reading the incomplete results of transactions, and it pre‐ vents concurrent writes from getting intermingled. Indeed, those are useful features, and much stronger guarantees than you can get from a system that has no transac‐ tions.\nimplementing\nLike read committed isolation, implementations of snapshot isolation typically use write locks to prevent dirty writes (see \u0026ldquo;Implementing read committed\u0026rdquo; on page 236), which means that a transaction that makes a write can block the progress of another transaction that writes to the same object. However, reads do not require any locks. From a performance point of view, a key principle of snapshot isolation is readers never block writers, and writers never block readers. This allows a database to handle long-running read queries on a consistent snapshot at the same time as processing writes normally, without any lock contention between the two. To implement snapshot isolation, databases use a generalization of the mechanism we saw for preventing dirty reads in Figure 7-4. The database must potentially keep several different committed versions of an object, because various in-progress trans‐ actions may need to see the state of the database at different points in time. Because it maintains several versions of an object side by side, this technique is known as multi- version concurrency control (MVCC).\nhttps://www.postgresql.org/docs/7.2/mvcc.html\n","date":"2025-02-22","id":78,"permalink":"/system-design/topics/transactions/","summary":"\u003ch1 id=\"definition\"\u003eDefinition\u003c/h1\u003e\n\u003cp\u003eHow do you figure out whether you need transactions? In order to answer that question, we first need to understand exactly \u003cem\u003ewhat safety guarantees transactions can provide\u003c/em\u003e, and what costs are associated with them. Although transactions seem straightforward at first glance, there are actually many subtle but important details that come into play.\u003c/p\u003e","tags":[],"title":"Transactions"},{"content":"https://en.wikipedia.org/wiki/Understanding https://plato.stanford.edu/entries/understanding/\nUnderstanding is a cognitive process related to an abstract or physical object, such as a person, situation, or message whereby one is able to use concepts to model that object (to do what ?). Understanding is a relation between the knower and an object of understanding. Understanding implies abilities and dispositions with respect to an object of knowledge that are sufficient to support intelligent behavior\n","date":"2025-02-22","id":79,"permalink":"/projects/foundations/understanding/","summary":"\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Understanding\"\u003ehttps://en.wikipedia.org/wiki/Understanding\u003c/a\u003e\n\u003ca href=\"https://plato.stanford.edu/entries/understanding/\"\u003ehttps://plato.stanford.edu/entries/understanding/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eUnderstanding is a cognitive process related to an abstract or physical object, such as a person, situation, or message whereby one is able to use concepts to \u003cem\u003emodel that object\u003c/em\u003e (to do what ?). Understanding is a \u003cem\u003erelation\u003c/em\u003e between the knower and an object of understanding. Understanding implies abilities and dispositions with respect to an object of knowledge that are sufficient to support intelligent behavior\u003c/p\u003e","tags":[],"title":"Understanding"},{"content":"","date":"2025-02-22","id":80,"permalink":"/system-design/projects/whatsapp/","summary":"","tags":[],"title":"Whatsapp"},{"content":"","date":"2025-02-21","id":81,"permalink":"/blog/example/","summary":"","tags":[],"title":"Example"},{"content":"","date":"2025-02-21","id":82,"permalink":"/english/","summary":"","tags":[],"title":"English"},{"content":"","date":"2025-02-21","id":83,"permalink":"/topics/","summary":"","tags":[],"title":"Topics"},{"content":"","date":"2025-02-22","id":84,"permalink":"/tags/advice/","summary":"","tags":[],"title":"Advice"},{"content":"","date":"2025-02-22","id":85,"permalink":"/tags/architecture/","summary":"","tags":[],"title":"Architecture"},{"content":"","date":"2025-02-22","id":86,"permalink":"/tags/system-design/","summary":"","tags":[],"title":"System-Design"},{"content":"","date":"2025-02-22","id":87,"permalink":"/tags/","summary":"","tags":[],"title":"Tags"},{"content":"","date":"2023-09-07","id":88,"permalink":"/privacy/","summary":"","tags":[],"title":"Privacy Policy"},{"content":"I’m putting together this knowledge base on system design, backend engineering, and software architecture with a clear aim: it’s not just about building systems that scale, run smoothly, and don’t crash—it’s also about making them solve real problems and deliver value, with no fluff attached.\nI’m not here to drone on about technical details for the sake of it. I want to dig into trade-off analysis, structured decision-making, and seeing the whole picture. Instead of just showing how things work, I want to focus on why certain choices are better than others and how to use them to get things done that actually matter.\n","date":"2023-09-07","id":89,"permalink":"/","summary":"\u003cp\u003eI’m putting together this knowledge base on system design, backend engineering, and software architecture with a clear aim: it’s not just about building systems that scale, run smoothly, and don’t crash—it’s also about making them solve real problems and deliver value, with no fluff attached.\u003c/p\u003e","tags":[],"title":"System Design and Architecture"},{"content":"","date":"0001-01-01","id":90,"permalink":"/categories/","summary":"","tags":[],"title":"Categories"},{"content":"","date":"0001-01-01","id":91,"permalink":"/contributors/","summary":"","tags":[],"title":"Contributors"}]